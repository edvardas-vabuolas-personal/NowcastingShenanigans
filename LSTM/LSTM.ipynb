{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from nowcast_lstm.LSTM import LSTM\n",
    "from nowcast_lstm.model_selection import select_model, variable_selection\n",
    "import torch\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_end_date = False):\n",
    "    \n",
    "    # Load and rename data\n",
    "    data = pd.read_excel('../230315 Nowcasting Dataset.xlsx', sheet_name='Nowcasting Dataset', parse_dates=['Date'])\n",
    "    data = data.rename(columns={\"GDP_QNA_RG\": \"GDP\"})\n",
    "\n",
    "    # Drop unnecessary GDP variables\n",
    "    data = data.drop(\n",
    "        [\"GDP_QNA_PCT\", \"GDP_QNA_LVL_LD\", \"GDP_QNA_LVL\"], axis=1)\n",
    "\n",
    "    # Fill in missing NA values using linear interpolation\n",
    "    data[\"GDP\"] = data[\"GDP\"].interpolate()\n",
    "    data[\"LIBOR_3mth\"] = data[\"LIBOR_3mth\"].interpolate()\n",
    "    \n",
    "    # Select sub-sample, see INTERVALS\n",
    "    if dataset_end_date:\n",
    "        data = data[\n",
    "            (data['Date'] <= pd.to_datetime(dataset_end_date))\n",
    "            ]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intervals():\n",
    "    INTERVALS = {\n",
    "        2010: {\n",
    "            \"dataset_end_date\": \"2010-12-01\",\n",
    "            \"train_end_date\": \"2005-12-01\",\n",
    "            \"test_start_date\": \"2006-01-01\",\n",
    "            \"initial_window\": 200,\n",
    "            \"break_points\": [217]\n",
    "        },\n",
    "        2019: {\n",
    "            \"dataset_end_date\": \"2019-12-01\",\n",
    "            \"train_end_date\": \"2015-12-01\",\n",
    "            \"test_start_date\": \"2016-01-01\",\n",
    "            \"initial_window\": 200,\n",
    "            \"break_points\": [217]\n",
    "        },\n",
    "        2022: {\n",
    "            \"dataset_end_date\": False,\n",
    "            \"train_end_date\": \"2015-12-01\",\n",
    "            \"test_start_date\": \"2016-01-01\",\n",
    "            \"initial_window\": 310,\n",
    "            \"break_points\": [217, 361]\n",
    "        },\n",
    "    }\n",
    "    return INTERVALS\n",
    "\n",
    "ALL_VARIABLES = False\n",
    "OUR_OWN_HYPERPARAMETERS = True\n",
    "STR_BREAKS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_selection_results(dummy_variables = False):\n",
    "    \"\"\"\n",
    "    selection_results.csv is output from nowcast_lstm/select_model()\n",
    "    The function trained Â±1000 models with varying parameters\n",
    "    and variables; the process took approx. 4 hrs. The output\n",
    "    of the function is 8 models that performed best.\n",
    "    \n",
    "    However, we find that LSTM performs better with variables\n",
    "    selected by Elastic Net / Ridge. Lasso omits most of them.\n",
    "    The list of variables from EN / Ridge is under 'varImp_results'.\n",
    "    \n",
    "    Finally, this function also allows running LSTM with all variables.\n",
    "    To do this, just change ALL_VARIABLES from False to True.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select best performing model\n",
    "    selections = pd.read_csv('selection_results.csv')\n",
    "    best_selection = selections[\n",
    "        selections.performance == selections.performance.max()]\n",
    "\n",
    "    # Extract best hyperparameters\n",
    "    if not OUR_OWN_HYPERPARAMETERS:\n",
    "        best_hyperparameters = eval(best_selection['hyperparameters'].values[0])\n",
    "    else:\n",
    "        best_hyperparameters = {\n",
    "            \"n_timesteps\": 2,\n",
    "            \"train_episodes\": 500,\n",
    "            \"batch_size\": 64,\n",
    "            \"decay\": 0.98,\n",
    "            \"n_hidden\": 20,\n",
    "            \"n_layers\": 2,\n",
    "            \"dropout\": 0,\n",
    "            \"n_models\": 10\n",
    "        }\n",
    "    \n",
    "    # Extract best variables\n",
    "    best_variables = eval(best_selection['variables'].values[0])\n",
    "    \n",
    "    if ALL_VARIABLES:\n",
    "        all_columns = load_data().columns.values.tolist()\n",
    "        if dummy_variables:\n",
    "            all_columns.extend(dummy_variables)\n",
    "        return (best_hyperparameters, all_columns)\n",
    "    \n",
    "    # Date and GDP are required in the dataset.\n",
    "    best_variables.extend(['Date', 'GDP'])\n",
    "    \n",
    "    # Elastic Net / Ridge suggested variables\n",
    "    varImp_results = [\n",
    "        'CPI_ALL', \n",
    "        'RPI_GOOD',\n",
    "        'TOT_WEEK_HRS',\n",
    "        'EMP',\n",
    "        'M2',\n",
    "        'RETAIL_TRADE_INDEX'\n",
    "        ]\n",
    "\n",
    "    best_variables.extend(varImp_results)\n",
    "    \n",
    "    if STR_BREAKS and dummy_variables:    \n",
    "        best_variables.extend(dummy_variables)\n",
    "    \n",
    "    return (best_hyperparameters, best_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_data(data, lags, dummy_variables: list):\n",
    "    ## The purpose of this is to lag explanatory variables\n",
    "    # i.e. Lag = 2 would be\n",
    "    \n",
    "    # Dependent variable at t+1   Explanatory vars at t and t-1\n",
    "    # GDP                         L1GDP   L2GDP   ...\n",
    "    # 0.9                         NA      NA      ...\n",
    "    # 0.4                         0.9     NA      ...\n",
    "    # 0.2                         0.4     0.9     ...\n",
    "    # 0.5                         0.2     0.4     ...\n",
    "    # ...                         ...     ...     ...\n",
    "    \n",
    "    if lags < 1:\n",
    "        return data\n",
    "    \n",
    "    original_data = data\n",
    "    \n",
    "    # Keep date and t+1 GDP\n",
    "    data = original_data[['Date', 'GDP']]\n",
    "    \n",
    "    # Add t, t-1, ..., t-lag explanatory variables to 'data' variable\n",
    "    for lag in range(1, lags+1):\n",
    "        \n",
    "        # Load selection results from nowcast_lstm.select_model()\n",
    "        _, best_variables = load_selection_results(dummy_variables)\n",
    "   \n",
    "        # We don't want to lag 'Date', so we temporarily remove it\n",
    "        best_variables.pop(best_variables.index('Date'))\n",
    "        \n",
    "        # Reset lagged_data for future lag iterations\n",
    "        lagged_data = original_data[best_variables]\n",
    "        \n",
    "        # Shift explanatory variables by <lag>\n",
    "        lagged_data = original_data[best_variables].shift(lag)\n",
    "        \n",
    "        # Rename columns \n",
    "        for col_name in lagged_data.columns:\n",
    "            if col_name != 'Date':\n",
    "                lagged_data = lagged_data.rename(columns={col_name: f'L{lag}{col_name}'})\n",
    "        \n",
    "        # Add 'Date' column back to lagged data for merge\n",
    "        lagged_data['Date'] = original_data['Date']\n",
    "        \n",
    "        # Add t, t-1, ..., t-lag explanatory variables to 'data' variable\n",
    "        data = pd.merge(data, lagged_data, on='Date')\n",
    "    \n",
    "    # First and last rows now contain NA values (because we lagged variables)\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n",
      "step :  0 loss :  0.460578590631485\n",
      "step :  1 loss :  0.27860143780708313\n",
      "step :  2 loss :  0.20083078742027283\n",
      "step :  3 loss :  0.23598957061767578\n",
      "step :  4 loss :  0.18507328629493713\n",
      "step :  5 loss :  0.19248689711093903\n",
      "step :  6 loss :  0.19058726727962494\n",
      "step :  7 loss :  0.16001971065998077\n",
      "step :  8 loss :  0.1447630077600479\n",
      "step :  9 loss :  0.13168324530124664\n",
      "step :  10 loss :  0.1408414989709854\n",
      "step :  11 loss :  0.14921943843364716\n",
      "step :  12 loss :  0.13182145357131958\n",
      "step :  13 loss :  0.13480211794376373\n",
      "step :  14 loss :  0.14817292988300323\n",
      "step :  15 loss :  0.13784421980381012\n",
      "step :  16 loss :  0.1215885728597641\n",
      "step :  17 loss :  0.11416423320770264\n",
      "step :  18 loss :  0.11390405893325806\n",
      "step :  19 loss :  0.11125227808952332\n",
      "step :  20 loss :  0.108932264149189\n",
      "step :  21 loss :  0.10797641426324844\n",
      "step :  22 loss :  0.10679000616073608\n",
      "step :  23 loss :  0.10655854642391205\n",
      "step :  24 loss :  0.10693492740392685\n",
      "step :  25 loss :  0.10391098260879517\n",
      "step :  26 loss :  0.10127103328704834\n",
      "step :  27 loss :  0.09507668763399124\n",
      "step :  28 loss :  0.08738871663808823\n",
      "step :  29 loss :  0.08538686484098434\n",
      "step :  30 loss :  0.07896912097930908\n",
      "step :  31 loss :  0.07414310425519943\n",
      "step :  32 loss :  0.07100719213485718\n",
      "step :  33 loss :  0.06355270743370056\n",
      "step :  34 loss :  0.06222409009933472\n",
      "step :  35 loss :  0.06112879514694214\n",
      "step :  36 loss :  0.06354739516973495\n",
      "step :  37 loss :  0.06267572939395905\n",
      "step :  38 loss :  0.08674230426549911\n",
      "step :  39 loss :  0.10896270722150803\n",
      "step :  40 loss :  0.08677666634321213\n",
      "step :  41 loss :  0.06068071722984314\n",
      "step :  42 loss :  0.0606180839240551\n",
      "step :  43 loss :  0.07152342051267624\n",
      "step :  44 loss :  0.06339497864246368\n",
      "step :  45 loss :  0.05770096927881241\n",
      "step :  46 loss :  0.06185206025838852\n",
      "step :  47 loss :  0.05925681069493294\n",
      "step :  48 loss :  0.0572163388133049\n",
      "step :  49 loss :  0.05699070543050766\n",
      "step :  50 loss :  0.05764072388410568\n",
      "step :  51 loss :  0.057707931846380234\n",
      "step :  52 loss :  0.056109752506017685\n",
      "step :  53 loss :  0.0549749918282032\n",
      "step :  54 loss :  0.055093228816986084\n",
      "step :  55 loss :  0.05601951479911804\n",
      "step :  56 loss :  0.054039668291807175\n",
      "step :  57 loss :  0.053616151213645935\n",
      "step :  58 loss :  0.05512094497680664\n",
      "step :  59 loss :  0.05292526260018349\n",
      "step :  60 loss :  0.052748821675777435\n",
      "step :  61 loss :  0.053655341267585754\n",
      "step :  62 loss :  0.05242932587862015\n",
      "step :  63 loss :  0.056170959025621414\n",
      "step :  64 loss :  0.053620100021362305\n",
      "step :  65 loss :  0.05802048370242119\n",
      "step :  66 loss :  0.053160473704338074\n",
      "step :  67 loss :  0.05349103733897209\n",
      "step :  68 loss :  0.05104636400938034\n",
      "step :  69 loss :  0.050854891538619995\n",
      "step :  70 loss :  0.05175043269991875\n",
      "step :  71 loss :  0.051116764545440674\n",
      "step :  72 loss :  0.05177374556660652\n",
      "step :  73 loss :  0.050210535526275635\n",
      "step :  74 loss :  0.05003795027732849\n",
      "step :  75 loss :  0.052787117660045624\n",
      "step :  76 loss :  0.05039841681718826\n",
      "step :  77 loss :  0.05089794099330902\n",
      "step :  78 loss :  0.04931413754820824\n",
      "step :  79 loss :  0.049692776054143906\n",
      "step :  80 loss :  0.049323178827762604\n",
      "step :  81 loss :  0.04877728596329689\n",
      "step :  82 loss :  0.0506645105779171\n",
      "step :  83 loss :  0.04940883815288544\n",
      "step :  84 loss :  0.05214591324329376\n",
      "step :  85 loss :  0.04857915639877319\n",
      "step :  86 loss :  0.05005338788032532\n",
      "step :  87 loss :  0.04780159145593643\n",
      "step :  88 loss :  0.048403605818748474\n",
      "step :  89 loss :  0.047827884554862976\n",
      "step :  90 loss :  0.047651953995227814\n",
      "step :  91 loss :  0.047147851437330246\n",
      "step :  92 loss :  0.04742817208170891\n",
      "step :  93 loss :  0.04672493413090706\n",
      "step :  94 loss :  0.04695679619908333\n",
      "step :  95 loss :  0.046547338366508484\n",
      "step :  96 loss :  0.046541087329387665\n",
      "step :  97 loss :  0.04613092914223671\n",
      "step :  98 loss :  0.04601779207587242\n",
      "step :  99 loss :  0.04653593525290489\n",
      "step :  100 loss :  0.045672692358493805\n",
      "step :  101 loss :  0.04526715725660324\n",
      "step :  102 loss :  0.04643195495009422\n",
      "step :  103 loss :  0.045213304460048676\n",
      "step :  104 loss :  0.0483548678457737\n",
      "step :  105 loss :  0.0455007441341877\n",
      "step :  106 loss :  0.04785981774330139\n",
      "step :  107 loss :  0.04485388845205307\n",
      "step :  108 loss :  0.046603549271821976\n",
      "step :  109 loss :  0.04413355514407158\n",
      "step :  110 loss :  0.04418625682592392\n",
      "step :  111 loss :  0.045317381620407104\n",
      "step :  112 loss :  0.04394420608878136\n",
      "step :  113 loss :  0.04587840288877487\n",
      "step :  114 loss :  0.043589524924755096\n",
      "step :  115 loss :  0.044619884341955185\n",
      "step :  116 loss :  0.04321080446243286\n",
      "step :  117 loss :  0.043125219643116\n",
      "step :  118 loss :  0.04439225420355797\n",
      "step :  119 loss :  0.04308689013123512\n",
      "step :  120 loss :  0.044179536402225494\n",
      "step :  121 loss :  0.043049413710832596\n",
      "step :  122 loss :  0.04400372505187988\n",
      "step :  123 loss :  0.04313456639647484\n",
      "step :  124 loss :  0.04404046759009361\n",
      "step :  125 loss :  0.04300782456994057\n",
      "step :  126 loss :  0.04376021400094032\n",
      "step :  127 loss :  0.04258832335472107\n",
      "step :  128 loss :  0.04286656528711319\n",
      "step :  129 loss :  0.042242150753736496\n",
      "step :  130 loss :  0.042327869683504105\n",
      "step :  131 loss :  0.042348749935626984\n",
      "step :  132 loss :  0.042308006435632706\n",
      "step :  133 loss :  0.042309969663619995\n",
      "step :  134 loss :  0.042207613587379456\n",
      "step :  135 loss :  0.042228829115629196\n",
      "step :  136 loss :  0.04209241643548012\n",
      "step :  137 loss :  0.04197628051042557\n",
      "step :  138 loss :  0.04213619977235794\n",
      "step :  139 loss :  0.04183618724346161\n",
      "step :  140 loss :  0.042157504707574844\n",
      "step :  141 loss :  0.041681285947561264\n",
      "step :  142 loss :  0.04243249073624611\n",
      "step :  143 loss :  0.0415429063141346\n",
      "step :  144 loss :  0.04218330606818199\n",
      "step :  145 loss :  0.04143702611327171\n",
      "step :  146 loss :  0.04190664738416672\n",
      "step :  147 loss :  0.041368093341588974\n",
      "step :  148 loss :  0.041798364371061325\n",
      "step :  149 loss :  0.041313789784908295\n",
      "step :  150 loss :  0.04190034419298172\n",
      "step :  151 loss :  0.041219744831323624\n",
      "step :  152 loss :  0.041735656559467316\n",
      "step :  153 loss :  0.041195470839738846\n",
      "step :  154 loss :  0.04169017821550369\n",
      "step :  155 loss :  0.04124089330434799\n",
      "step :  156 loss :  0.04119597002863884\n",
      "step :  157 loss :  0.041559234261512756\n",
      "step :  158 loss :  0.04118048772215843\n",
      "step :  159 loss :  0.041857700794935226\n",
      "step :  160 loss :  0.041170231997966766\n",
      "step :  161 loss :  0.041312459856271744\n",
      "step :  162 loss :  0.04138655960559845\n",
      "step :  163 loss :  0.04115184396505356\n",
      "step :  164 loss :  0.041465599089860916\n",
      "step :  165 loss :  0.04121021181344986\n",
      "step :  166 loss :  0.0411556251347065\n",
      "step :  167 loss :  0.04131096974015236\n",
      "step :  168 loss :  0.04112549498677254\n",
      "step :  169 loss :  0.041124820709228516\n",
      "step :  170 loss :  0.04116440564393997\n",
      "step :  171 loss :  0.041057899594306946\n",
      "step :  172 loss :  0.041106004267930984\n",
      "step :  173 loss :  0.04105578735470772\n",
      "step :  174 loss :  0.04105899855494499\n",
      "step :  175 loss :  0.04104026034474373\n",
      "step :  176 loss :  0.04106510430574417\n",
      "step :  177 loss :  0.04101341962814331\n",
      "step :  178 loss :  0.041006214916706085\n",
      "step :  179 loss :  0.041013047099113464\n",
      "step :  180 loss :  0.04094388708472252\n",
      "step :  181 loss :  0.04095616936683655\n",
      "step :  182 loss :  0.04094620794057846\n",
      "step :  183 loss :  0.0408870168030262\n",
      "step :  184 loss :  0.04090198874473572\n",
      "step :  185 loss :  0.040956657379865646\n",
      "step :  186 loss :  0.04083115607500076\n",
      "step :  187 loss :  0.04087359458208084\n",
      "step :  188 loss :  0.04088791832327843\n",
      "step :  189 loss :  0.04083925113081932\n",
      "step :  190 loss :  0.0408112071454525\n",
      "step :  191 loss :  0.040818165987730026\n",
      "step :  192 loss :  0.040824465453624725\n",
      "step :  193 loss :  0.04079199954867363\n",
      "step :  194 loss :  0.04076910391449928\n",
      "step :  195 loss :  0.04078830033540726\n",
      "step :  196 loss :  0.0407651886343956\n",
      "step :  197 loss :  0.04072388634085655\n",
      "step :  198 loss :  0.040713123977184296\n",
      "step :  199 loss :  0.040729984641075134\n",
      "step :  200 loss :  0.040709689259529114\n",
      "step :  201 loss :  0.040693748742341995\n",
      "step :  202 loss :  0.04066424444317818\n",
      "step :  203 loss :  0.040665894746780396\n",
      "step :  204 loss :  0.04069900140166283\n",
      "step :  205 loss :  0.04067239910364151\n",
      "step :  206 loss :  0.04064474627375603\n",
      "step :  207 loss :  0.04066150635480881\n",
      "step :  208 loss :  0.04065942391753197\n",
      "step :  209 loss :  0.04062085971236229\n",
      "step :  210 loss :  0.040610428899526596\n",
      "step :  211 loss :  0.04063691571354866\n",
      "step :  212 loss :  0.0406360886991024\n",
      "step :  213 loss :  0.04058438166975975\n",
      "step :  214 loss :  0.040579698979854584\n",
      "step :  215 loss :  0.0406169593334198\n",
      "step :  216 loss :  0.04058092087507248\n",
      "step :  217 loss :  0.040555037558078766\n",
      "step :  218 loss :  0.04056103527545929\n",
      "step :  219 loss :  0.040555231273174286\n",
      "step :  220 loss :  0.04056196287274361\n",
      "step :  221 loss :  0.04056563600897789\n",
      "step :  222 loss :  0.040551960468292236\n",
      "step :  223 loss :  0.040534909814596176\n",
      "step :  224 loss :  0.04053279757499695\n",
      "step :  225 loss :  0.04051589220762253\n",
      "step :  226 loss :  0.04050245136022568\n",
      "step :  227 loss :  0.04052409157156944\n",
      "step :  228 loss :  0.04052691534161568\n",
      "step :  229 loss :  0.04049143195152283\n",
      "step :  230 loss :  0.04048983380198479\n",
      "step :  231 loss :  0.04050183296203613\n",
      "step :  232 loss :  0.04047498479485512\n",
      "step :  233 loss :  0.04048216715455055\n",
      "step :  234 loss :  0.04050131514668465\n",
      "step :  235 loss :  0.04048167169094086\n",
      "step :  236 loss :  0.040469057857990265\n",
      "step :  237 loss :  0.040482521057128906\n",
      "step :  238 loss :  0.040469083935022354\n",
      "step :  239 loss :  0.040449414402246475\n",
      "step :  240 loss :  0.04044855758547783\n",
      "step :  241 loss :  0.040465015918016434\n",
      "step :  242 loss :  0.04046573117375374\n",
      "step :  243 loss :  0.0404408723115921\n",
      "step :  244 loss :  0.04042894393205643\n",
      "step :  245 loss :  0.040448494255542755\n",
      "step :  246 loss :  0.0404522530734539\n",
      "step :  247 loss :  0.04042845219373703\n",
      "step :  248 loss :  0.04040711373090744\n",
      "step :  249 loss :  0.040426842868328094\n",
      "step :  250 loss :  0.04044000059366226\n",
      "step :  251 loss :  0.04042771831154823\n",
      "step :  252 loss :  0.04040530323982239\n",
      "step :  253 loss :  0.04039711877703667\n",
      "step :  254 loss :  0.04040896147489548\n",
      "step :  255 loss :  0.04041633382439613\n",
      "step :  256 loss :  0.04039930924773216\n",
      "step :  257 loss :  0.04038310423493385\n",
      "step :  258 loss :  0.04038461670279503\n",
      "step :  259 loss :  0.04038740694522858\n",
      "step :  260 loss :  0.04038434475660324\n",
      "step :  261 loss :  0.040376774966716766\n",
      "step :  262 loss :  0.04037078097462654\n",
      "step :  263 loss :  0.04037941247224808\n",
      "step :  264 loss :  0.04037434607744217\n",
      "step :  265 loss :  0.040363628417253494\n",
      "step :  266 loss :  0.0403747521340847\n",
      "step :  267 loss :  0.04037819430232048\n",
      "step :  268 loss :  0.04036777466535568\n",
      "step :  269 loss :  0.040359482169151306\n",
      "step :  270 loss :  0.040364738553762436\n",
      "step :  271 loss :  0.04036760330200195\n",
      "step :  272 loss :  0.04036670923233032\n",
      "step :  273 loss :  0.04035324230790138\n",
      "step :  274 loss :  0.04034758731722832\n",
      "step :  275 loss :  0.040358297526836395\n",
      "step :  276 loss :  0.04036221280694008\n",
      "step :  277 loss :  0.040355026721954346\n",
      "step :  278 loss :  0.0403437577188015\n",
      "step :  279 loss :  0.04034090414643288\n",
      "step :  280 loss :  0.0403461791574955\n",
      "step :  281 loss :  0.04034554585814476\n",
      "step :  282 loss :  0.04034297913312912\n",
      "step :  283 loss :  0.04034452140331268\n",
      "step :  284 loss :  0.04034295305609703\n",
      "step :  285 loss :  0.040334947407245636\n",
      "step :  286 loss :  0.04033420607447624\n",
      "step :  287 loss :  0.04033428430557251\n",
      "step :  288 loss :  0.040329545736312866\n",
      "step :  289 loss :  0.040329709649086\n",
      "step :  290 loss :  0.04033533111214638\n",
      "step :  291 loss :  0.040330253541469574\n",
      "step :  292 loss :  0.04032314568758011\n",
      "step :  293 loss :  0.040324121713638306\n",
      "step :  294 loss :  0.040331631898880005\n",
      "step :  295 loss :  0.04032886400818825\n",
      "step :  296 loss :  0.040323156863451004\n",
      "step :  297 loss :  0.040318794548511505\n",
      "step :  298 loss :  0.04032229632139206\n",
      "step :  299 loss :  0.04032677784562111\n",
      "step :  300 loss :  0.04032408073544502\n",
      "step :  301 loss :  0.04031713306903839\n",
      "step :  302 loss :  0.04032029211521149\n",
      "step :  303 loss :  0.04032355919480324\n",
      "step :  304 loss :  0.04032041132450104\n",
      "step :  305 loss :  0.04031355679035187\n",
      "step :  306 loss :  0.04031488671898842\n",
      "step :  307 loss :  0.04031456634402275\n",
      "step :  308 loss :  0.040314238518476486\n",
      "step :  309 loss :  0.04031229764223099\n",
      "step :  310 loss :  0.0403108187019825\n",
      "step :  311 loss :  0.040309250354766846\n",
      "step :  312 loss :  0.0403105765581131\n",
      "step :  313 loss :  0.04031001031398773\n",
      "step :  314 loss :  0.040306273847818375\n",
      "step :  315 loss :  0.04030703008174896\n",
      "step :  316 loss :  0.04030508175492287\n",
      "step :  317 loss :  0.04030443727970123\n",
      "step :  318 loss :  0.04030477628111839\n",
      "step :  319 loss :  0.04030264914035797\n",
      "step :  320 loss :  0.04030301049351692\n",
      "step :  321 loss :  0.0403035543859005\n",
      "step :  322 loss :  0.040301721543073654\n",
      "step :  323 loss :  0.040300026535987854\n",
      "step :  324 loss :  0.04030321165919304\n",
      "step :  325 loss :  0.04030469432473183\n",
      "step :  326 loss :  0.040300846099853516\n",
      "step :  327 loss :  0.04029671475291252\n",
      "step :  328 loss :  0.04029719531536102\n",
      "step :  329 loss :  0.0402996651828289\n",
      "step :  330 loss :  0.04029756784439087\n",
      "step :  331 loss :  0.040295567363500595\n",
      "step :  332 loss :  0.04029638320207596\n",
      "step :  333 loss :  0.040295086801052094\n",
      "step :  334 loss :  0.04029478505253792\n",
      "step :  335 loss :  0.04029422998428345\n",
      "step :  336 loss :  0.04029761999845505\n",
      "step :  337 loss :  0.040297165513038635\n",
      "step :  338 loss :  0.040294382721185684\n",
      "step :  339 loss :  0.04029355198144913\n",
      "step :  340 loss :  0.04029407352209091\n",
      "step :  341 loss :  0.04029354453086853\n",
      "step :  342 loss :  0.04029140621423721\n",
      "step :  343 loss :  0.0402907058596611\n",
      "step :  344 loss :  0.04029079154133797\n",
      "step :  345 loss :  0.0402902252972126\n",
      "step :  346 loss :  0.04029141366481781\n",
      "step :  347 loss :  0.04028931260108948\n",
      "step :  348 loss :  0.0402892604470253\n",
      "step :  349 loss :  0.040289256721735\n",
      "step :  350 loss :  0.040288910269737244\n",
      "step :  351 loss :  0.040288813412189484\n",
      "step :  352 loss :  0.04028802737593651\n",
      "step :  353 loss :  0.040287844836711884\n",
      "step :  354 loss :  0.04028720036149025\n",
      "step :  355 loss :  0.04028673097491264\n",
      "step :  356 loss :  0.0402868315577507\n",
      "step :  357 loss :  0.040286287665367126\n",
      "step :  358 loss :  0.040285587310791016\n",
      "step :  359 loss :  0.04028528183698654\n",
      "step :  360 loss :  0.040286146104335785\n",
      "step :  361 loss :  0.04028472676873207\n",
      "step :  362 loss :  0.0402846522629261\n",
      "step :  363 loss :  0.04028630256652832\n",
      "step :  364 loss :  0.04028542712330818\n",
      "step :  365 loss :  0.04028385132551193\n",
      "step :  366 loss :  0.0402839295566082\n",
      "step :  367 loss :  0.04028472304344177\n",
      "step :  368 loss :  0.040284737944602966\n",
      "step :  369 loss :  0.040282800793647766\n",
      "step :  370 loss :  0.040282461792230606\n",
      "step :  371 loss :  0.04028400033712387\n",
      "step :  372 loss :  0.04028373584151268\n",
      "step :  373 loss :  0.04028229042887688\n",
      "step :  374 loss :  0.04028185456991196\n",
      "step :  375 loss :  0.040282927453517914\n",
      "step :  376 loss :  0.04028266295790672\n",
      "step :  377 loss :  0.040282025933265686\n",
      "step :  378 loss :  0.04028155282139778\n",
      "step :  379 loss :  0.040280766785144806\n",
      "step :  380 loss :  0.040280308574438095\n",
      "step :  381 loss :  0.040280360728502274\n",
      "step :  382 loss :  0.040280409157276154\n",
      "step :  383 loss :  0.04028052091598511\n",
      "step :  384 loss :  0.04028022661805153\n",
      "step :  385 loss :  0.04028012976050377\n",
      "step :  386 loss :  0.040279462933540344\n",
      "step :  387 loss :  0.040279291570186615\n",
      "step :  388 loss :  0.04027935117483139\n",
      "step :  389 loss :  0.04027891531586647\n",
      "step :  390 loss :  0.04027931019663811\n",
      "step :  391 loss :  0.0402793288230896\n",
      "step :  392 loss :  0.04027850180864334\n",
      "step :  393 loss :  0.040279194712638855\n",
      "step :  394 loss :  0.040278974920511246\n",
      "step :  395 loss :  0.04027809202671051\n",
      "step :  396 loss :  0.04027828946709633\n",
      "step :  397 loss :  0.04027782753109932\n",
      "step :  398 loss :  0.040277671068906784\n",
      "step :  399 loss :  0.040277767926454544\n",
      "step :  400 loss :  0.04027758538722992\n",
      "step :  401 loss :  0.040277618914842606\n",
      "step :  402 loss :  0.04027748480439186\n",
      "step :  403 loss :  0.04027736559510231\n",
      "step :  404 loss :  0.04027765244245529\n",
      "step :  405 loss :  0.04027761518955231\n",
      "step :  406 loss :  0.04027692601084709\n",
      "step :  407 loss :  0.04027673602104187\n",
      "step :  408 loss :  0.04027657210826874\n",
      "step :  409 loss :  0.04027621075510979\n",
      "step :  410 loss :  0.04027620330452919\n",
      "step :  411 loss :  0.04027635231614113\n",
      "step :  412 loss :  0.040276333689689636\n",
      "step :  413 loss :  0.040276382118463516\n",
      "step :  414 loss :  0.04027598351240158\n",
      "step :  415 loss :  0.040275804698467255\n",
      "step :  416 loss :  0.04027577489614487\n",
      "step :  417 loss :  0.04027553275227547\n",
      "step :  418 loss :  0.04027567058801651\n",
      "step :  419 loss :  0.04027584195137024\n",
      "step :  420 loss :  0.040275510400533676\n",
      "step :  421 loss :  0.0402752086520195\n",
      "step :  422 loss :  0.040275655686855316\n",
      "step :  423 loss :  0.040275853127241135\n",
      "step :  424 loss :  0.040275149047374725\n",
      "step :  425 loss :  0.04027450829744339\n",
      "step :  426 loss :  0.04027455672621727\n",
      "step :  427 loss :  0.04027508199214935\n",
      "step :  428 loss :  0.04027530550956726\n",
      "step :  429 loss :  0.04027494043111801\n",
      "step :  430 loss :  0.040274426341056824\n",
      "step :  431 loss :  0.0402744896709919\n",
      "step :  432 loss :  0.040275052189826965\n",
      "step :  433 loss :  0.040274981409311295\n",
      "step :  434 loss :  0.0402744859457016\n",
      "step :  435 loss :  0.040274180471897125\n",
      "step :  436 loss :  0.040274444967508316\n",
      "step :  437 loss :  0.04027470573782921\n",
      "step :  438 loss :  0.04027458280324936\n",
      "step :  439 loss :  0.04027419909834862\n",
      "step :  440 loss :  0.040273893624544144\n",
      "step :  441 loss :  0.04027404636144638\n",
      "step :  442 loss :  0.040274228900671005\n",
      "step :  443 loss :  0.04027416929602623\n",
      "step :  444 loss :  0.040273893624544144\n",
      "step :  445 loss :  0.04027366265654564\n",
      "step :  446 loss :  0.04027385637164116\n",
      "step :  447 loss :  0.040273942053318024\n",
      "step :  448 loss :  0.04027361422777176\n",
      "step :  449 loss :  0.04027358442544937\n",
      "step :  450 loss :  0.040273625403642654\n",
      "step :  451 loss :  0.040273427963256836\n",
      "step :  452 loss :  0.04027341306209564\n",
      "step :  453 loss :  0.04027348756790161\n",
      "step :  454 loss :  0.04027334228157997\n",
      "step :  455 loss :  0.04027319326996803\n",
      "step :  456 loss :  0.04027346521615982\n",
      "step :  457 loss :  0.040273625403642654\n",
      "step :  458 loss :  0.040273286402225494\n",
      "step :  459 loss :  0.040272992104291916\n",
      "step :  460 loss :  0.0402730293571949\n",
      "step :  461 loss :  0.04027334228157997\n",
      "step :  462 loss :  0.04027329757809639\n",
      "step :  463 loss :  0.04027313366532326\n",
      "step :  464 loss :  0.040273137390613556\n",
      "step :  465 loss :  0.040273137390613556\n",
      "step :  466 loss :  0.04027305170893669\n",
      "step :  467 loss :  0.040272995829582214\n",
      "step :  468 loss :  0.040272895246744156\n",
      "step :  469 loss :  0.0402727946639061\n",
      "step :  470 loss :  0.0402727909386158\n",
      "step :  471 loss :  0.04027266055345535\n",
      "step :  472 loss :  0.04027267545461655\n",
      "step :  473 loss :  0.04027289152145386\n",
      "step :  474 loss :  0.04027286544442177\n",
      "step :  475 loss :  0.04027262330055237\n",
      "step :  476 loss :  0.04027250036597252\n",
      "step :  477 loss :  0.04027257114648819\n",
      "step :  478 loss :  0.04027271270751953\n",
      "step :  479 loss :  0.040272586047649384\n",
      "step :  480 loss :  0.040272437036037445\n",
      "step :  481 loss :  0.04027261212468147\n",
      "step :  482 loss :  0.040272701531648636\n",
      "step :  483 loss :  0.04027250409126282\n",
      "step :  484 loss :  0.04027228429913521\n",
      "step :  485 loss :  0.0402722992002964\n",
      "step :  486 loss :  0.040272440761327744\n",
      "step :  487 loss :  0.04027244821190834\n",
      "step :  488 loss :  0.04027233645319939\n",
      "step :  489 loss :  0.04027217999100685\n",
      "step :  490 loss :  0.04027218371629715\n",
      "step :  491 loss :  0.040272295475006104\n",
      "step :  492 loss :  0.040272314101457596\n",
      "step :  493 loss :  0.04027223587036133\n",
      "step :  494 loss :  0.04027217999100685\n",
      "step :  495 loss :  0.04027222469449043\n",
      "step :  496 loss :  0.04027225077152252\n",
      "step :  497 loss :  0.040272146463394165\n",
      "step :  498 loss :  0.04027213901281357\n",
      "step :  499 loss :  0.04027210921049118\n",
      "Training model 2\n",
      "step :  0 loss :  0.6272076964378357\n",
      "step :  1 loss :  0.4257656931877136\n",
      "step :  2 loss :  0.19516339898109436\n",
      "step :  3 loss :  0.281852126121521\n",
      "step :  4 loss :  0.1872149109840393\n",
      "step :  5 loss :  0.18260733783245087\n",
      "step :  6 loss :  0.2001439929008484\n",
      "step :  7 loss :  0.17856425046920776\n",
      "step :  8 loss :  0.14887632429599762\n",
      "step :  9 loss :  0.1500929743051529\n",
      "step :  10 loss :  0.1307983547449112\n",
      "step :  11 loss :  0.13704261183738708\n",
      "step :  12 loss :  0.13633303344249725\n",
      "step :  13 loss :  0.12173766642808914\n",
      "step :  14 loss :  0.12165310233831406\n",
      "step :  15 loss :  0.1330948919057846\n",
      "step :  16 loss :  0.14054635167121887\n",
      "step :  17 loss :  0.130948007106781\n",
      "step :  18 loss :  0.1192593052983284\n",
      "step :  19 loss :  0.11618782579898834\n",
      "step :  20 loss :  0.11754234880208969\n",
      "step :  21 loss :  0.11392907053232193\n",
      "step :  22 loss :  0.10832302272319794\n",
      "step :  23 loss :  0.10697175562381744\n",
      "step :  24 loss :  0.1087936982512474\n",
      "step :  25 loss :  0.105911985039711\n",
      "step :  26 loss :  0.10193030536174774\n",
      "step :  27 loss :  0.10239717364311218\n",
      "step :  28 loss :  0.10378842800855637\n",
      "step :  29 loss :  0.09963437169790268\n",
      "step :  30 loss :  0.0956183448433876\n",
      "step :  31 loss :  0.09427949786186218\n",
      "step :  32 loss :  0.09089762717485428\n",
      "step :  33 loss :  0.08531750738620758\n",
      "step :  34 loss :  0.08228275924921036\n",
      "step :  35 loss :  0.08053800463676453\n",
      "step :  36 loss :  0.07696007937192917\n",
      "step :  37 loss :  0.07348815351724625\n",
      "step :  38 loss :  0.06607759743928909\n",
      "step :  39 loss :  0.06425214558839798\n",
      "step :  40 loss :  0.05575770139694214\n",
      "step :  41 loss :  0.05539756640791893\n",
      "step :  42 loss :  0.05209166184067726\n",
      "step :  43 loss :  0.05296803265810013\n",
      "step :  44 loss :  0.050883591175079346\n",
      "step :  45 loss :  0.048687729984521866\n",
      "step :  46 loss :  0.04956373944878578\n",
      "step :  47 loss :  0.05533817410469055\n",
      "step :  48 loss :  0.050178367644548416\n",
      "step :  49 loss :  0.04748383164405823\n",
      "step :  50 loss :  0.04827547445893288\n",
      "step :  51 loss :  0.04794124513864517\n",
      "step :  52 loss :  0.04654669389128685\n",
      "step :  53 loss :  0.045692309737205505\n",
      "step :  54 loss :  0.046191226691007614\n",
      "step :  55 loss :  0.044739093631505966\n",
      "step :  56 loss :  0.04437946900725365\n",
      "step :  57 loss :  0.04619837924838066\n",
      "step :  58 loss :  0.0440368577837944\n",
      "step :  59 loss :  0.043042998760938644\n",
      "step :  60 loss :  0.043823856860399246\n",
      "step :  61 loss :  0.04434524476528168\n",
      "step :  62 loss :  0.04273467883467674\n",
      "step :  63 loss :  0.05780857801437378\n",
      "step :  64 loss :  0.04380316659808159\n",
      "step :  65 loss :  0.041567135602235794\n",
      "step :  66 loss :  0.047149669378995895\n",
      "step :  67 loss :  0.04516452178359032\n",
      "step :  68 loss :  0.04255574569106102\n",
      "step :  69 loss :  0.04180584475398064\n",
      "step :  70 loss :  0.04236618056893349\n",
      "step :  71 loss :  0.04233953729271889\n",
      "step :  72 loss :  0.042869653552770615\n",
      "step :  73 loss :  0.041402846574783325\n",
      "step :  74 loss :  0.0405946783721447\n",
      "step :  75 loss :  0.043461598455905914\n",
      "step :  76 loss :  0.040476180613040924\n",
      "step :  77 loss :  0.04152734950184822\n",
      "step :  78 loss :  0.04034829139709473\n",
      "step :  79 loss :  0.04012662544846535\n",
      "step :  80 loss :  0.0415893979370594\n",
      "step :  81 loss :  0.03986121714115143\n",
      "step :  82 loss :  0.04092158377170563\n",
      "step :  83 loss :  0.040774162858724594\n",
      "step :  84 loss :  0.03963916748762131\n",
      "step :  85 loss :  0.04076182097196579\n",
      "step :  86 loss :  0.039568789303302765\n",
      "step :  87 loss :  0.04053810238838196\n",
      "step :  88 loss :  0.03988385945558548\n",
      "step :  89 loss :  0.03944128006696701\n",
      "step :  90 loss :  0.039857178926467896\n",
      "step :  91 loss :  0.039430487900972366\n",
      "step :  92 loss :  0.0396847240626812\n",
      "step :  93 loss :  0.04027091711759567\n",
      "step :  94 loss :  0.03987676277756691\n",
      "step :  95 loss :  0.03920656442642212\n",
      "step :  96 loss :  0.03979954123497009\n",
      "step :  97 loss :  0.03951791301369667\n",
      "step :  98 loss :  0.03924369066953659\n",
      "step :  99 loss :  0.039243705570697784\n",
      "step :  100 loss :  0.03981262445449829\n",
      "step :  101 loss :  0.039337579160928726\n",
      "step :  102 loss :  0.03901610150933266\n",
      "step :  103 loss :  0.03991517424583435\n",
      "step :  104 loss :  0.038976144045591354\n",
      "step :  105 loss :  0.03955085948109627\n",
      "step :  106 loss :  0.038964319974184036\n",
      "step :  107 loss :  0.039692334830760956\n",
      "step :  108 loss :  0.03896121680736542\n",
      "step :  109 loss :  0.039410658180713654\n",
      "step :  110 loss :  0.03926754370331764\n",
      "step :  111 loss :  0.039127007126808167\n",
      "step :  112 loss :  0.039412014186382294\n",
      "step :  113 loss :  0.039156969636678696\n",
      "step :  114 loss :  0.03928963094949722\n",
      "step :  115 loss :  0.039085231721401215\n",
      "step :  116 loss :  0.039173632860183716\n",
      "step :  117 loss :  0.039246685802936554\n",
      "step :  118 loss :  0.03910389542579651\n",
      "step :  119 loss :  0.03948637843132019\n",
      "step :  120 loss :  0.03889279067516327\n",
      "step :  121 loss :  0.03924208879470825\n",
      "step :  122 loss :  0.03881998360157013\n",
      "step :  123 loss :  0.04016491025686264\n",
      "step :  124 loss :  0.03892508149147034\n",
      "step :  125 loss :  0.04098223149776459\n",
      "step :  126 loss :  0.039314206689596176\n",
      "step :  127 loss :  0.04150161147117615\n",
      "step :  128 loss :  0.03922151029109955\n",
      "step :  129 loss :  0.04168993607163429\n",
      "step :  130 loss :  0.03894314169883728\n",
      "step :  131 loss :  0.040188152343034744\n",
      "step :  132 loss :  0.03887815773487091\n",
      "step :  133 loss :  0.04058494418859482\n",
      "step :  134 loss :  0.0389132983982563\n",
      "step :  135 loss :  0.040522731840610504\n",
      "step :  136 loss :  0.03870806470513344\n",
      "step :  137 loss :  0.039212290197610855\n",
      "step :  138 loss :  0.03893585503101349\n",
      "step :  139 loss :  0.038707610219717026\n",
      "step :  140 loss :  0.03915150836110115\n",
      "step :  141 loss :  0.03862223029136658\n",
      "step :  142 loss :  0.03934521973133087\n",
      "step :  143 loss :  0.03859160467982292\n",
      "step :  144 loss :  0.03917183727025986\n",
      "step :  145 loss :  0.03874386474490166\n",
      "step :  146 loss :  0.03876806050539017\n",
      "step :  147 loss :  0.03904625028371811\n",
      "step :  148 loss :  0.038746125996112823\n",
      "step :  149 loss :  0.03888805955648422\n",
      "step :  150 loss :  0.03875383362174034\n",
      "step :  151 loss :  0.03868649899959564\n",
      "step :  152 loss :  0.038723498582839966\n",
      "step :  153 loss :  0.038526542484760284\n",
      "step :  154 loss :  0.038958244025707245\n",
      "step :  155 loss :  0.03849975764751434\n",
      "step :  156 loss :  0.039034951478242874\n",
      "step :  157 loss :  0.03854311257600784\n",
      "step :  158 loss :  0.03882826864719391\n",
      "step :  159 loss :  0.0387723445892334\n",
      "step :  160 loss :  0.038559019565582275\n",
      "step :  161 loss :  0.038800399750471115\n",
      "step :  162 loss :  0.03861996531486511\n",
      "step :  163 loss :  0.03861352801322937\n",
      "step :  164 loss :  0.038773562759160995\n",
      "step :  165 loss :  0.038611337542533875\n",
      "step :  166 loss :  0.03862909972667694\n",
      "step :  167 loss :  0.038624849170446396\n",
      "step :  168 loss :  0.03855876624584198\n",
      "step :  169 loss :  0.038647107779979706\n",
      "step :  170 loss :  0.03857892379164696\n",
      "step :  171 loss :  0.03867974504828453\n",
      "step :  172 loss :  0.03858872130513191\n",
      "step :  173 loss :  0.038591690361499786\n",
      "step :  174 loss :  0.038652267307043076\n",
      "step :  175 loss :  0.03856580704450607\n",
      "step :  176 loss :  0.03866354376077652\n",
      "step :  177 loss :  0.03856711462140083\n",
      "step :  178 loss :  0.03855212405323982\n",
      "step :  179 loss :  0.03858884796500206\n",
      "step :  180 loss :  0.03852231055498123\n",
      "step :  181 loss :  0.038602083921432495\n",
      "step :  182 loss :  0.03863779082894325\n",
      "step :  183 loss :  0.03862641379237175\n",
      "step :  184 loss :  0.038627348840236664\n",
      "step :  185 loss :  0.03860456123948097\n",
      "step :  186 loss :  0.038648154586553574\n",
      "step :  187 loss :  0.03854275122284889\n",
      "step :  188 loss :  0.0385916605591774\n",
      "step :  189 loss :  0.03859899938106537\n",
      "step :  190 loss :  0.03860202431678772\n",
      "step :  191 loss :  0.038546882569789886\n",
      "step :  192 loss :  0.03855603188276291\n",
      "step :  193 loss :  0.03865642473101616\n",
      "step :  194 loss :  0.038532555103302\n",
      "step :  195 loss :  0.03855246677994728\n",
      "step :  196 loss :  0.03861669823527336\n",
      "step :  197 loss :  0.038657527416944504\n",
      "step :  198 loss :  0.03849346190690994\n",
      "step :  199 loss :  0.03851728141307831\n",
      "step :  200 loss :  0.038665786385536194\n",
      "step :  201 loss :  0.03857291117310524\n",
      "step :  202 loss :  0.03849337622523308\n",
      "step :  203 loss :  0.03851418197154999\n",
      "step :  204 loss :  0.03858570009469986\n",
      "step :  205 loss :  0.03857763111591339\n",
      "step :  206 loss :  0.03858022391796112\n",
      "step :  207 loss :  0.038516730070114136\n",
      "step :  208 loss :  0.03854941576719284\n",
      "step :  209 loss :  0.03860469162464142\n",
      "step :  210 loss :  0.038600847125053406\n",
      "step :  211 loss :  0.038542065769433975\n",
      "step :  212 loss :  0.038532838225364685\n",
      "step :  213 loss :  0.03855135664343834\n",
      "step :  214 loss :  0.038548603653907776\n",
      "step :  215 loss :  0.03854422643780708\n",
      "step :  216 loss :  0.03852817416191101\n",
      "step :  217 loss :  0.03856664523482323\n",
      "step :  218 loss :  0.03854123502969742\n",
      "step :  219 loss :  0.03854428604245186\n",
      "step :  220 loss :  0.0385439358651638\n",
      "step :  221 loss :  0.03852754086256027\n",
      "step :  222 loss :  0.03855983167886734\n",
      "step :  223 loss :  0.03853932023048401\n",
      "step :  224 loss :  0.03851751610636711\n",
      "step :  225 loss :  0.038563650101423264\n",
      "step :  226 loss :  0.038534246385097504\n",
      "step :  227 loss :  0.03850014880299568\n",
      "step :  228 loss :  0.038521941751241684\n",
      "step :  229 loss :  0.03858434036374092\n",
      "step :  230 loss :  0.038509171456098557\n",
      "step :  231 loss :  0.03850437328219414\n",
      "step :  232 loss :  0.038554105907678604\n",
      "step :  233 loss :  0.03853902593255043\n",
      "step :  234 loss :  0.03849131613969803\n",
      "step :  235 loss :  0.03849974274635315\n",
      "step :  236 loss :  0.03855033218860626\n",
      "step :  237 loss :  0.03854445740580559\n",
      "step :  238 loss :  0.03852237015962601\n",
      "step :  239 loss :  0.038521770387887955\n",
      "step :  240 loss :  0.03855332359671593\n",
      "step :  241 loss :  0.03851708024740219\n",
      "step :  242 loss :  0.03852806240320206\n",
      "step :  243 loss :  0.03854021057486534\n",
      "step :  244 loss :  0.038516320288181305\n",
      "step :  245 loss :  0.03854058310389519\n",
      "step :  246 loss :  0.03851834312081337\n",
      "step :  247 loss :  0.03851836547255516\n",
      "step :  248 loss :  0.038523655384778976\n",
      "step :  249 loss :  0.03852112963795662\n",
      "step :  250 loss :  0.038520779460668564\n",
      "step :  251 loss :  0.038516756147146225\n",
      "step :  252 loss :  0.038517627865076065\n",
      "step :  253 loss :  0.03851582109928131\n",
      "step :  254 loss :  0.03852280601859093\n",
      "step :  255 loss :  0.03851590305566788\n",
      "step :  256 loss :  0.038504648953676224\n",
      "step :  257 loss :  0.03851018100976944\n",
      "step :  258 loss :  0.03850359469652176\n",
      "step :  259 loss :  0.03851744532585144\n",
      "step :  260 loss :  0.03854386508464813\n",
      "step :  261 loss :  0.03849180415272713\n",
      "step :  262 loss :  0.038481809198856354\n",
      "step :  263 loss :  0.03849851340055466\n",
      "step :  264 loss :  0.03850813955068588\n",
      "step :  265 loss :  0.03850103169679642\n",
      "step :  266 loss :  0.03850507363677025\n",
      "step :  267 loss :  0.03850090876221657\n",
      "step :  268 loss :  0.0384959913790226\n",
      "step :  269 loss :  0.03850053250789642\n",
      "step :  270 loss :  0.0385042279958725\n",
      "step :  271 loss :  0.03850119933485985\n",
      "step :  272 loss :  0.03848555311560631\n",
      "step :  273 loss :  0.03848849609494209\n",
      "step :  274 loss :  0.038501057773828506\n",
      "step :  275 loss :  0.03850477561354637\n",
      "step :  276 loss :  0.0384858064353466\n",
      "step :  277 loss :  0.038488391786813736\n",
      "step :  278 loss :  0.038494933396577835\n",
      "step :  279 loss :  0.038498010486364365\n",
      "step :  280 loss :  0.03849392384290695\n",
      "step :  281 loss :  0.03848584368824959\n",
      "step :  282 loss :  0.03849445655941963\n",
      "step :  283 loss :  0.03850032016634941\n",
      "step :  284 loss :  0.03847954049706459\n",
      "step :  285 loss :  0.03847567364573479\n",
      "step :  286 loss :  0.03848898038268089\n",
      "step :  287 loss :  0.03849929943680763\n",
      "step :  288 loss :  0.038478653877973557\n",
      "step :  289 loss :  0.038466595113277435\n",
      "step :  290 loss :  0.038472384214401245\n",
      "step :  291 loss :  0.03849029168486595\n",
      "step :  292 loss :  0.038494810461997986\n",
      "step :  293 loss :  0.03847501799464226\n",
      "step :  294 loss :  0.03847053274512291\n",
      "step :  295 loss :  0.03848119080066681\n",
      "step :  296 loss :  0.03848863020539284\n",
      "step :  297 loss :  0.038482435047626495\n",
      "step :  298 loss :  0.038470037281513214\n",
      "step :  299 loss :  0.0384657122194767\n",
      "step :  300 loss :  0.038472797721624374\n",
      "step :  301 loss :  0.0384846031665802\n",
      "step :  302 loss :  0.03848469629883766\n",
      "step :  303 loss :  0.03847506642341614\n",
      "step :  304 loss :  0.03847093880176544\n",
      "step :  305 loss :  0.03847166523337364\n",
      "step :  306 loss :  0.03847551718354225\n",
      "step :  307 loss :  0.03847923129796982\n",
      "step :  308 loss :  0.03847843036055565\n",
      "step :  309 loss :  0.03847139701247215\n",
      "step :  310 loss :  0.0384688563644886\n",
      "step :  311 loss :  0.03847089409828186\n",
      "step :  312 loss :  0.038475580513477325\n",
      "step :  313 loss :  0.0384761206805706\n",
      "step :  314 loss :  0.03847040981054306\n",
      "step :  315 loss :  0.03846855089068413\n",
      "step :  316 loss :  0.03847075253725052\n",
      "step :  317 loss :  0.038472335785627365\n",
      "step :  318 loss :  0.03846887871623039\n",
      "step :  319 loss :  0.03846796229481697\n",
      "step :  320 loss :  0.038467634469270706\n",
      "step :  321 loss :  0.0384659506380558\n",
      "step :  322 loss :  0.038467392325401306\n",
      "step :  323 loss :  0.038468703627586365\n",
      "step :  324 loss :  0.0384664423763752\n",
      "step :  325 loss :  0.03846581652760506\n",
      "step :  326 loss :  0.03846769779920578\n",
      "step :  327 loss :  0.03846663981676102\n",
      "step :  328 loss :  0.038461364805698395\n",
      "step :  329 loss :  0.038462527096271515\n",
      "step :  330 loss :  0.03846702352166176\n",
      "step :  331 loss :  0.03846822306513786\n",
      "step :  332 loss :  0.03846438229084015\n",
      "step :  333 loss :  0.03846031427383423\n",
      "step :  334 loss :  0.0384628027677536\n",
      "step :  335 loss :  0.0384662002325058\n",
      "step :  336 loss :  0.03846607729792595\n",
      "step :  337 loss :  0.03846238553524017\n",
      "step :  338 loss :  0.0384613499045372\n",
      "step :  339 loss :  0.0384625568985939\n",
      "step :  340 loss :  0.038463618606328964\n",
      "step :  341 loss :  0.03846387565135956\n",
      "step :  342 loss :  0.03846311196684837\n",
      "step :  343 loss :  0.038462888449430466\n",
      "step :  344 loss :  0.038462262600660324\n",
      "step :  345 loss :  0.038463108241558075\n",
      "step :  346 loss :  0.038463715463876724\n",
      "step :  347 loss :  0.03846149519085884\n",
      "step :  348 loss :  0.03845876827836037\n",
      "step :  349 loss :  0.038458287715911865\n",
      "step :  350 loss :  0.038458097726106644\n",
      "step :  351 loss :  0.038459111005067825\n",
      "step :  352 loss :  0.038460344076156616\n",
      "step :  353 loss :  0.038461267948150635\n",
      "step :  354 loss :  0.03846019133925438\n",
      "step :  355 loss :  0.03845720738172531\n",
      "step :  356 loss :  0.03845827281475067\n",
      "step :  357 loss :  0.03846011310815811\n",
      "step :  358 loss :  0.038459356874227524\n",
      "step :  359 loss :  0.03845847398042679\n",
      "step :  360 loss :  0.03845880553126335\n",
      "step :  361 loss :  0.03845904767513275\n",
      "step :  362 loss :  0.03845848888158798\n",
      "step :  363 loss :  0.03845829889178276\n",
      "step :  364 loss :  0.03845883905887604\n",
      "step :  365 loss :  0.03845840319991112\n",
      "step :  366 loss :  0.03845704719424248\n",
      "step :  367 loss :  0.038457222282886505\n",
      "step :  368 loss :  0.03845744952559471\n",
      "step :  369 loss :  0.03845734894275665\n",
      "step :  370 loss :  0.03845607116818428\n",
      "step :  371 loss :  0.03845612332224846\n",
      "step :  372 loss :  0.03845634683966637\n",
      "step :  373 loss :  0.03845643624663353\n",
      "step :  374 loss :  0.038456305861473083\n",
      "step :  375 loss :  0.038455531001091\n",
      "step :  376 loss :  0.038455091416835785\n",
      "step :  377 loss :  0.038455281406641006\n",
      "step :  378 loss :  0.0384555384516716\n",
      "step :  379 loss :  0.03845524042844772\n",
      "step :  380 loss :  0.03845519199967384\n",
      "step :  381 loss :  0.03845491260290146\n",
      "step :  382 loss :  0.038455355912446976\n",
      "step :  383 loss :  0.038455162197351456\n",
      "step :  384 loss :  0.038454122841358185\n",
      "step :  385 loss :  0.03845413029193878\n",
      "step :  386 loss :  0.03845406696200371\n",
      "step :  387 loss :  0.03845358267426491\n",
      "step :  388 loss :  0.03845413774251938\n",
      "step :  389 loss :  0.038454655557870865\n",
      "step :  390 loss :  0.0384540893137455\n",
      "step :  391 loss :  0.038452308624982834\n",
      "step :  392 loss :  0.038452498614788055\n",
      "step :  393 loss :  0.038454119116067886\n",
      "step :  394 loss :  0.038454096764326096\n",
      "step :  395 loss :  0.03845281898975372\n",
      "step :  396 loss :  0.03845175728201866\n",
      "step :  397 loss :  0.038452327251434326\n",
      "step :  398 loss :  0.03845378756523132\n",
      "step :  399 loss :  0.03845340013504028\n",
      "step :  400 loss :  0.03845299780368805\n",
      "step :  401 loss :  0.03845241293311119\n",
      "step :  402 loss :  0.038452208042144775\n",
      "step :  403 loss :  0.03845193609595299\n",
      "step :  404 loss :  0.03845281898975372\n",
      "step :  405 loss :  0.038453299552202225\n",
      "step :  406 loss :  0.0384521558880806\n",
      "step :  407 loss :  0.03845157474279404\n",
      "step :  408 loss :  0.03845209628343582\n",
      "step :  409 loss :  0.03845254331827164\n",
      "step :  410 loss :  0.03845280781388283\n",
      "step :  411 loss :  0.038452036678791046\n",
      "step :  412 loss :  0.038452036678791046\n",
      "step :  413 loss :  0.038452133536338806\n",
      "step :  414 loss :  0.0384519100189209\n",
      "step :  415 loss :  0.038451772183179855\n",
      "step :  416 loss :  0.03845157101750374\n",
      "step :  417 loss :  0.03845129907131195\n",
      "step :  418 loss :  0.03845146298408508\n",
      "step :  419 loss :  0.038451917469501495\n",
      "step :  420 loss :  0.03845162317156792\n",
      "step :  421 loss :  0.03845128044486046\n",
      "step :  422 loss :  0.038450952619314194\n",
      "step :  423 loss :  0.03845151141285896\n",
      "step :  424 loss :  0.03845192864537239\n",
      "step :  425 loss :  0.03845120966434479\n",
      "step :  426 loss :  0.038450296968221664\n",
      "step :  427 loss :  0.038450486958026886\n",
      "step :  428 loss :  0.03845113888382912\n",
      "step :  429 loss :  0.038451287895441055\n",
      "step :  430 loss :  0.03845109045505524\n",
      "step :  431 loss :  0.03845100849866867\n",
      "step :  432 loss :  0.0384504608809948\n",
      "step :  433 loss :  0.03845066577196121\n",
      "step :  434 loss :  0.03845108300447464\n",
      "step :  435 loss :  0.03845113888382912\n",
      "step :  436 loss :  0.03844983130693436\n",
      "step :  437 loss :  0.03844955563545227\n",
      "step :  438 loss :  0.0384502075612545\n",
      "step :  439 loss :  0.038451164960861206\n",
      "step :  440 loss :  0.038450900465250015\n",
      "step :  441 loss :  0.03845028951764107\n",
      "step :  442 loss :  0.03845004737377167\n",
      "step :  443 loss :  0.0384504608809948\n",
      "step :  444 loss :  0.03845066577196121\n",
      "step :  445 loss :  0.038450390100479126\n",
      "step :  446 loss :  0.03845039755105972\n",
      "step :  447 loss :  0.03845033422112465\n",
      "step :  448 loss :  0.03844986855983734\n",
      "step :  449 loss :  0.03844999894499779\n",
      "step :  450 loss :  0.03845048323273659\n",
      "step :  451 loss :  0.03845065459609032\n",
      "step :  452 loss :  0.03844981640577316\n",
      "step :  453 loss :  0.03844970092177391\n",
      "step :  454 loss :  0.03845026716589928\n",
      "step :  455 loss :  0.03845062851905823\n",
      "step :  456 loss :  0.03844983130693436\n",
      "step :  457 loss :  0.03844933211803436\n",
      "step :  458 loss :  0.03844956308603287\n",
      "step :  459 loss :  0.03844987973570824\n",
      "step :  460 loss :  0.03845008462667465\n",
      "step :  461 loss :  0.03845002129673958\n",
      "step :  462 loss :  0.03844980522990227\n",
      "step :  463 loss :  0.03844984620809555\n",
      "step :  464 loss :  0.03844991326332092\n",
      "step :  465 loss :  0.03844982758164406\n",
      "step :  466 loss :  0.03844974935054779\n",
      "step :  467 loss :  0.0384497232735157\n",
      "step :  468 loss :  0.03844957426190376\n",
      "step :  469 loss :  0.03844963014125824\n",
      "step :  470 loss :  0.03844968229532242\n",
      "step :  471 loss :  0.038449712097644806\n",
      "step :  472 loss :  0.03844970092177391\n",
      "step :  473 loss :  0.038449499756097794\n",
      "step :  474 loss :  0.03844944387674332\n",
      "step :  475 loss :  0.03844955563545227\n",
      "step :  476 loss :  0.0384494885802269\n",
      "step :  477 loss :  0.03844931721687317\n",
      "step :  478 loss :  0.03844955936074257\n",
      "step :  479 loss :  0.03844967111945152\n",
      "step :  480 loss :  0.0384494848549366\n",
      "step :  481 loss :  0.03844933211803436\n",
      "step :  482 loss :  0.03844932094216347\n",
      "step :  483 loss :  0.03844933584332466\n",
      "step :  484 loss :  0.038449279963970184\n",
      "step :  485 loss :  0.038449328392744064\n",
      "step :  486 loss :  0.038449421525001526\n",
      "step :  487 loss :  0.03844933956861496\n",
      "step :  488 loss :  0.03844915330410004\n",
      "step :  489 loss :  0.03844928741455078\n",
      "step :  490 loss :  0.03844939172267914\n",
      "step :  491 loss :  0.038449227809906006\n",
      "step :  492 loss :  0.038449205458164215\n",
      "step :  493 loss :  0.03844928741455078\n",
      "step :  494 loss :  0.038449253886938095\n",
      "step :  495 loss :  0.03844917193055153\n",
      "step :  496 loss :  0.038449179381132126\n",
      "step :  497 loss :  0.03844918683171272\n",
      "step :  498 loss :  0.03844919800758362\n",
      "step :  499 loss :  0.03844912350177765\n",
      "Training model 3\n",
      "step :  0 loss :  0.4720747172832489\n",
      "step :  1 loss :  0.27705124020576477\n",
      "step :  2 loss :  0.1960456371307373\n",
      "step :  3 loss :  0.24431271851062775\n",
      "step :  4 loss :  0.18558426201343536\n",
      "step :  5 loss :  0.1923750638961792\n",
      "step :  6 loss :  0.1959981620311737\n",
      "step :  7 loss :  0.17462681233882904\n",
      "step :  8 loss :  0.15029925107955933\n",
      "step :  9 loss :  0.14860950410366058\n",
      "step :  10 loss :  0.13063080608844757\n",
      "step :  11 loss :  0.14193229377269745\n",
      "step :  12 loss :  0.14501100778579712\n",
      "step :  13 loss :  0.13530518114566803\n",
      "step :  14 loss :  0.13746550679206848\n",
      "step :  15 loss :  0.14723151922225952\n",
      "step :  16 loss :  0.13859762251377106\n",
      "step :  17 loss :  0.12255017459392548\n",
      "step :  18 loss :  0.11720551550388336\n",
      "step :  19 loss :  0.11690720170736313\n",
      "step :  20 loss :  0.11482773721218109\n",
      "step :  21 loss :  0.11010012030601501\n",
      "step :  22 loss :  0.10831696540117264\n",
      "step :  23 loss :  0.11146418005228043\n",
      "step :  24 loss :  0.10789339244365692\n",
      "step :  25 loss :  0.10188020765781403\n",
      "step :  26 loss :  0.09947604686021805\n",
      "step :  27 loss :  0.09575914591550827\n",
      "step :  28 loss :  0.08748354017734528\n",
      "step :  29 loss :  0.07999803125858307\n",
      "step :  30 loss :  0.07551170885562897\n",
      "step :  31 loss :  0.06933978945016861\n",
      "step :  32 loss :  0.06557560712099075\n",
      "step :  33 loss :  0.061088308691978455\n",
      "step :  34 loss :  0.059379417449235916\n",
      "step :  35 loss :  0.05964028462767601\n",
      "step :  36 loss :  0.061812739819288254\n",
      "step :  37 loss :  0.057246528565883636\n",
      "step :  38 loss :  0.05990305915474892\n",
      "step :  39 loss :  0.057336993515491486\n",
      "step :  40 loss :  0.05697328597307205\n",
      "step :  41 loss :  0.057303428649902344\n",
      "step :  42 loss :  0.05525797978043556\n",
      "step :  43 loss :  0.05464060604572296\n",
      "step :  44 loss :  0.05468491464853287\n",
      "step :  45 loss :  0.05432567372918129\n",
      "step :  46 loss :  0.05366959050297737\n",
      "step :  47 loss :  0.05314333736896515\n",
      "step :  48 loss :  0.05788394808769226\n",
      "step :  49 loss :  0.06665410101413727\n",
      "step :  50 loss :  0.07558061182498932\n",
      "step :  51 loss :  0.08831318467855453\n",
      "step :  52 loss :  0.06494028866291046\n",
      "step :  53 loss :  0.053100742399692535\n",
      "step :  54 loss :  0.06102025881409645\n",
      "step :  55 loss :  0.0630788579583168\n",
      "step :  56 loss :  0.054433904588222504\n",
      "step :  57 loss :  0.05134272947907448\n",
      "step :  58 loss :  0.053646914660930634\n",
      "step :  59 loss :  0.052140526473522186\n",
      "step :  60 loss :  0.05023008957505226\n",
      "step :  61 loss :  0.049706194549798965\n",
      "step :  62 loss :  0.04895283654332161\n",
      "step :  63 loss :  0.05249468609690666\n",
      "step :  64 loss :  0.05308155342936516\n",
      "step :  65 loss :  0.05876148119568825\n",
      "step :  66 loss :  0.05494726449251175\n",
      "step :  67 loss :  0.05446699261665344\n",
      "step :  68 loss :  0.04877518117427826\n",
      "step :  69 loss :  0.0512443482875824\n",
      "step :  70 loss :  0.049307018518447876\n",
      "step :  71 loss :  0.05066747963428497\n",
      "step :  72 loss :  0.05138901621103287\n",
      "step :  73 loss :  0.051926784217357635\n",
      "step :  74 loss :  0.049696069210767746\n",
      "step :  75 loss :  0.05079324543476105\n",
      "step :  76 loss :  0.04682647064328194\n",
      "step :  77 loss :  0.0470428392291069\n",
      "step :  78 loss :  0.04538048803806305\n",
      "step :  79 loss :  0.048652611672878265\n",
      "step :  80 loss :  0.04527942091226578\n",
      "step :  81 loss :  0.048523806035518646\n",
      "step :  82 loss :  0.045486386865377426\n",
      "step :  83 loss :  0.04859634116292\n",
      "step :  84 loss :  0.04546629264950752\n",
      "step :  85 loss :  0.048223815858364105\n",
      "step :  86 loss :  0.0455356240272522\n",
      "step :  87 loss :  0.04949960857629776\n",
      "step :  88 loss :  0.04577166587114334\n",
      "step :  89 loss :  0.04724450409412384\n",
      "step :  90 loss :  0.044087912887334824\n",
      "step :  91 loss :  0.04576684907078743\n",
      "step :  92 loss :  0.043206825852394104\n",
      "step :  93 loss :  0.04534768685698509\n",
      "step :  94 loss :  0.043049056082963943\n",
      "step :  95 loss :  0.04371163621544838\n",
      "step :  96 loss :  0.04315633699297905\n",
      "step :  97 loss :  0.04330962896347046\n",
      "step :  98 loss :  0.04374891519546509\n",
      "step :  99 loss :  0.04779065027832985\n",
      "step :  100 loss :  0.046325623989105225\n",
      "step :  101 loss :  0.04684195667505264\n",
      "step :  102 loss :  0.04300624504685402\n",
      "step :  103 loss :  0.04418892785906792\n",
      "step :  104 loss :  0.04186719283461571\n",
      "step :  105 loss :  0.043165843933820724\n",
      "step :  106 loss :  0.04142361879348755\n",
      "step :  107 loss :  0.0426228828728199\n",
      "step :  108 loss :  0.04125630110502243\n",
      "step :  109 loss :  0.04240782931447029\n",
      "step :  110 loss :  0.041209977120161057\n",
      "step :  111 loss :  0.04249833896756172\n",
      "step :  112 loss :  0.04120858386158943\n",
      "step :  113 loss :  0.04280199110507965\n",
      "step :  114 loss :  0.04120307043194771\n",
      "step :  115 loss :  0.04350389167666435\n",
      "step :  116 loss :  0.04131656140089035\n",
      "step :  117 loss :  0.043623026460409164\n",
      "step :  118 loss :  0.04101603478193283\n",
      "step :  119 loss :  0.04155575484037399\n",
      "step :  120 loss :  0.040847938507795334\n",
      "step :  121 loss :  0.040845148265361786\n",
      "step :  122 loss :  0.04152560606598854\n",
      "step :  123 loss :  0.04072384536266327\n",
      "step :  124 loss :  0.04137660562992096\n",
      "step :  125 loss :  0.040704913437366486\n",
      "step :  126 loss :  0.040996525436639786\n",
      "step :  127 loss :  0.04070001840591431\n",
      "step :  128 loss :  0.04106026515364647\n",
      "step :  129 loss :  0.04059762880206108\n",
      "step :  130 loss :  0.04077781364321709\n",
      "step :  131 loss :  0.0406692698597908\n",
      "step :  132 loss :  0.04052364081144333\n",
      "step :  133 loss :  0.04098105430603027\n",
      "step :  134 loss :  0.040391918271780014\n",
      "step :  135 loss :  0.04074115306138992\n",
      "step :  136 loss :  0.04045826196670532\n",
      "step :  137 loss :  0.04049218073487282\n",
      "step :  138 loss :  0.04030290246009827\n",
      "step :  139 loss :  0.04039885476231575\n",
      "step :  140 loss :  0.040308669209480286\n",
      "step :  141 loss :  0.04027821123600006\n",
      "step :  142 loss :  0.04027024656534195\n",
      "step :  143 loss :  0.0402083545923233\n",
      "step :  144 loss :  0.04034009948372841\n",
      "step :  145 loss :  0.04015986621379852\n",
      "step :  146 loss :  0.040281862020492554\n",
      "step :  147 loss :  0.04009528085589409\n",
      "step :  148 loss :  0.04017196223139763\n",
      "step :  149 loss :  0.040069304406642914\n",
      "step :  150 loss :  0.040091048926115036\n",
      "step :  151 loss :  0.04004984721541405\n",
      "step :  152 loss :  0.04001568257808685\n",
      "step :  153 loss :  0.039987996220588684\n",
      "step :  154 loss :  0.03995629400014877\n",
      "step :  155 loss :  0.03994031250476837\n",
      "step :  156 loss :  0.03991997614502907\n",
      "step :  157 loss :  0.04000811651349068\n",
      "step :  158 loss :  0.03986963629722595\n",
      "step :  159 loss :  0.040007155388593674\n",
      "step :  160 loss :  0.03986575827002525\n",
      "step :  161 loss :  0.039872899651527405\n",
      "step :  162 loss :  0.03988471254706383\n",
      "step :  163 loss :  0.03986622393131256\n",
      "step :  164 loss :  0.03993223235011101\n",
      "step :  165 loss :  0.03986738994717598\n",
      "step :  166 loss :  0.03989868238568306\n",
      "step :  167 loss :  0.0398467555642128\n",
      "step :  168 loss :  0.03994755446910858\n",
      "step :  169 loss :  0.0398225300014019\n",
      "step :  170 loss :  0.039839282631874084\n",
      "step :  171 loss :  0.039808813482522964\n",
      "step :  172 loss :  0.039823003113269806\n",
      "step :  173 loss :  0.039796795696020126\n",
      "step :  174 loss :  0.03981664776802063\n",
      "step :  175 loss :  0.039806779474020004\n",
      "step :  176 loss :  0.0397886224091053\n",
      "step :  177 loss :  0.03982297703623772\n",
      "step :  178 loss :  0.039758600294589996\n",
      "step :  179 loss :  0.03985876217484474\n",
      "step :  180 loss :  0.03975425288081169\n",
      "step :  181 loss :  0.039801616221666336\n",
      "step :  182 loss :  0.03976958245038986\n",
      "step :  183 loss :  0.039744287729263306\n",
      "step :  184 loss :  0.03978671878576279\n",
      "step :  185 loss :  0.039723820984363556\n",
      "step :  186 loss :  0.03971872478723526\n",
      "step :  187 loss :  0.03985907509922981\n",
      "step :  188 loss :  0.03971057012677193\n",
      "step :  189 loss :  0.03973982855677605\n",
      "step :  190 loss :  0.03975217044353485\n",
      "step :  191 loss :  0.03971076384186745\n",
      "step :  192 loss :  0.039764855057001114\n",
      "step :  193 loss :  0.039690058678388596\n",
      "step :  194 loss :  0.0396902933716774\n",
      "step :  195 loss :  0.03978458791971207\n",
      "step :  196 loss :  0.03966701403260231\n",
      "step :  197 loss :  0.03966856375336647\n",
      "step :  198 loss :  0.039762526750564575\n",
      "step :  199 loss :  0.0396469309926033\n",
      "step :  200 loss :  0.03964730352163315\n",
      "step :  201 loss :  0.039848387241363525\n",
      "step :  202 loss :  0.03963475301861763\n",
      "step :  203 loss :  0.03962623327970505\n",
      "step :  204 loss :  0.03970594331622124\n",
      "step :  205 loss :  0.03964288532733917\n",
      "step :  206 loss :  0.03962194174528122\n",
      "step :  207 loss :  0.039669621735811234\n",
      "step :  208 loss :  0.03964248299598694\n",
      "step :  209 loss :  0.03962166979908943\n",
      "step :  210 loss :  0.03969665989279747\n",
      "step :  211 loss :  0.03962220996618271\n",
      "step :  212 loss :  0.03962654992938042\n",
      "step :  213 loss :  0.039706695824861526\n",
      "step :  214 loss :  0.039613112807273865\n",
      "step :  215 loss :  0.03960234299302101\n",
      "step :  216 loss :  0.03962552919983864\n",
      "step :  217 loss :  0.03959832340478897\n",
      "step :  218 loss :  0.03960556164383888\n",
      "step :  219 loss :  0.03959692642092705\n",
      "step :  220 loss :  0.039586856961250305\n",
      "step :  221 loss :  0.039604317396879196\n",
      "step :  222 loss :  0.039584387093782425\n",
      "step :  223 loss :  0.03959923982620239\n",
      "step :  224 loss :  0.03958826884627342\n",
      "step :  225 loss :  0.0395883172750473\n",
      "step :  226 loss :  0.039586059749126434\n",
      "step :  227 loss :  0.03957756981253624\n",
      "step :  228 loss :  0.03958088904619217\n",
      "step :  229 loss :  0.0395810641348362\n",
      "step :  230 loss :  0.03956218436360359\n",
      "step :  231 loss :  0.03958015888929367\n",
      "step :  232 loss :  0.03955845534801483\n",
      "step :  233 loss :  0.039569612592458725\n",
      "step :  234 loss :  0.03955933824181557\n",
      "step :  235 loss :  0.03960365802049637\n",
      "step :  236 loss :  0.03955382481217384\n",
      "step :  237 loss :  0.03956104815006256\n",
      "step :  238 loss :  0.03961995616555214\n",
      "step :  239 loss :  0.0395507887005806\n",
      "step :  240 loss :  0.03954564034938812\n",
      "step :  241 loss :  0.03957705572247505\n",
      "step :  242 loss :  0.03955397382378578\n",
      "step :  243 loss :  0.039548877626657486\n",
      "step :  244 loss :  0.039555542171001434\n",
      "step :  245 loss :  0.03953582048416138\n",
      "step :  246 loss :  0.039581626653671265\n",
      "step :  247 loss :  0.03953690454363823\n",
      "step :  248 loss :  0.03954097628593445\n",
      "step :  249 loss :  0.03954286873340607\n",
      "step :  250 loss :  0.03953160345554352\n",
      "step :  251 loss :  0.03955668956041336\n",
      "step :  252 loss :  0.03953346610069275\n",
      "step :  253 loss :  0.03953765705227852\n",
      "step :  254 loss :  0.03954467549920082\n",
      "step :  255 loss :  0.039525989443063736\n",
      "step :  256 loss :  0.03953433409333229\n",
      "step :  257 loss :  0.039533331990242004\n",
      "step :  258 loss :  0.03951799124479294\n",
      "step :  259 loss :  0.03955255448818207\n",
      "step :  260 loss :  0.039524052292108536\n",
      "step :  261 loss :  0.03951345756649971\n",
      "step :  262 loss :  0.03953174874186516\n",
      "step :  263 loss :  0.03951815515756607\n",
      "step :  264 loss :  0.03951657935976982\n",
      "step :  265 loss :  0.03954307734966278\n",
      "step :  266 loss :  0.03951486572623253\n",
      "step :  267 loss :  0.0395238995552063\n",
      "step :  268 loss :  0.039510395377874374\n",
      "step :  269 loss :  0.03953109681606293\n",
      "step :  270 loss :  0.039509061723947525\n",
      "step :  271 loss :  0.03953367471694946\n",
      "step :  272 loss :  0.039520133286714554\n",
      "step :  273 loss :  0.03950394317507744\n",
      "step :  274 loss :  0.039518244564533234\n",
      "step :  275 loss :  0.03951191157102585\n",
      "step :  276 loss :  0.03950532153248787\n",
      "step :  277 loss :  0.03952717036008835\n",
      "step :  278 loss :  0.039509423077106476\n",
      "step :  279 loss :  0.03950170800089836\n",
      "step :  280 loss :  0.03952966257929802\n",
      "step :  281 loss :  0.039514582604169846\n",
      "step :  282 loss :  0.03949826583266258\n",
      "step :  283 loss :  0.03950328007340431\n",
      "step :  284 loss :  0.039524856954813004\n",
      "step :  285 loss :  0.03950778394937515\n",
      "step :  286 loss :  0.039497848600149155\n",
      "step :  287 loss :  0.03950891271233559\n",
      "step :  288 loss :  0.03950176388025284\n",
      "step :  289 loss :  0.03949727118015289\n",
      "step :  290 loss :  0.03951645642518997\n",
      "step :  291 loss :  0.03949819877743721\n",
      "step :  292 loss :  0.039491988718509674\n",
      "step :  293 loss :  0.03949848562479019\n",
      "step :  294 loss :  0.039490215480327606\n",
      "step :  295 loss :  0.03950278460979462\n",
      "step :  296 loss :  0.03949502855539322\n",
      "step :  297 loss :  0.03948909044265747\n",
      "step :  298 loss :  0.03949660062789917\n",
      "step :  299 loss :  0.03948868811130524\n",
      "step :  300 loss :  0.03949904069304466\n",
      "step :  301 loss :  0.03949138522148132\n",
      "step :  302 loss :  0.039488330483436584\n",
      "step :  303 loss :  0.03950166702270508\n",
      "step :  304 loss :  0.03949512913823128\n",
      "step :  305 loss :  0.039486903697252274\n",
      "step :  306 loss :  0.03949281945824623\n",
      "step :  307 loss :  0.03948882967233658\n",
      "step :  308 loss :  0.03948771208524704\n",
      "step :  309 loss :  0.03949872404336929\n",
      "step :  310 loss :  0.03948776051402092\n",
      "step :  311 loss :  0.03948606923222542\n",
      "step :  312 loss :  0.039501067250967026\n",
      "step :  313 loss :  0.03948668763041496\n",
      "step :  314 loss :  0.03948207572102547\n",
      "step :  315 loss :  0.03948869928717613\n",
      "step :  316 loss :  0.039487842470407486\n",
      "step :  317 loss :  0.0394832119345665\n",
      "step :  318 loss :  0.03948700800538063\n",
      "step :  319 loss :  0.039487797766923904\n",
      "step :  320 loss :  0.03948133438825607\n",
      "step :  321 loss :  0.03948253020644188\n",
      "step :  322 loss :  0.0394953154027462\n",
      "step :  323 loss :  0.03948768973350525\n",
      "step :  324 loss :  0.0394788458943367\n",
      "step :  325 loss :  0.0394890159368515\n",
      "step :  326 loss :  0.03948630392551422\n",
      "step :  327 loss :  0.03947845473885536\n",
      "step :  328 loss :  0.03948342055082321\n",
      "step :  329 loss :  0.039480116218328476\n",
      "step :  330 loss :  0.03948146477341652\n",
      "step :  331 loss :  0.039478179067373276\n",
      "step :  332 loss :  0.03948427364230156\n",
      "step :  333 loss :  0.03947992995381355\n",
      "step :  334 loss :  0.039476823061704636\n",
      "step :  335 loss :  0.03948105126619339\n",
      "step :  336 loss :  0.03947687894105911\n",
      "step :  337 loss :  0.03947809338569641\n",
      "step :  338 loss :  0.03947605565190315\n",
      "step :  339 loss :  0.03947802633047104\n",
      "step :  340 loss :  0.039474908262491226\n",
      "step :  341 loss :  0.03947669640183449\n",
      "step :  342 loss :  0.03948083892464638\n",
      "step :  343 loss :  0.03947509080171585\n",
      "step :  344 loss :  0.03947589918971062\n",
      "step :  345 loss :  0.0394829623401165\n",
      "step :  346 loss :  0.0394776351749897\n",
      "step :  347 loss :  0.03947493061423302\n",
      "step :  348 loss :  0.03948264941573143\n",
      "step :  349 loss :  0.039481792598962784\n",
      "step :  350 loss :  0.03947495296597481\n",
      "step :  351 loss :  0.03947717696428299\n",
      "step :  352 loss :  0.0394778810441494\n",
      "step :  353 loss :  0.03947409987449646\n",
      "step :  354 loss :  0.039476025849580765\n",
      "step :  355 loss :  0.03947649151086807\n",
      "step :  356 loss :  0.03947332873940468\n",
      "step :  357 loss :  0.039475731551647186\n",
      "step :  358 loss :  0.03947271779179573\n",
      "step :  359 loss :  0.039473678916692734\n",
      "step :  360 loss :  0.03947801515460014\n",
      "step :  361 loss :  0.03947333246469498\n",
      "step :  362 loss :  0.0394715778529644\n",
      "step :  363 loss :  0.03947538882493973\n",
      "step :  364 loss :  0.03947353735566139\n",
      "step :  365 loss :  0.039472103118896484\n",
      "step :  366 loss :  0.03947491571307182\n",
      "step :  367 loss :  0.03947218880057335\n",
      "step :  368 loss :  0.03947179764509201\n",
      "step :  369 loss :  0.03947543725371361\n",
      "step :  370 loss :  0.039472274482250214\n",
      "step :  371 loss :  0.03947163745760918\n",
      "step :  372 loss :  0.03947298228740692\n",
      "step :  373 loss :  0.039470624178647995\n",
      "step :  374 loss :  0.039471711963415146\n",
      "step :  375 loss :  0.03947135806083679\n",
      "step :  376 loss :  0.039471205323934555\n",
      "step :  377 loss :  0.039470985531806946\n",
      "step :  378 loss :  0.039470821619033813\n",
      "step :  379 loss :  0.039470307528972626\n",
      "step :  380 loss :  0.03947119787335396\n",
      "step :  381 loss :  0.03946994990110397\n",
      "step :  382 loss :  0.03947329521179199\n",
      "step :  383 loss :  0.03947075828909874\n",
      "step :  384 loss :  0.039468929171562195\n",
      "step :  385 loss :  0.03947124257683754\n",
      "step :  386 loss :  0.03947143629193306\n",
      "step :  387 loss :  0.03946932405233383\n",
      "step :  388 loss :  0.03946996107697487\n",
      "step :  389 loss :  0.03947081044316292\n",
      "step :  390 loss :  0.039469167590141296\n",
      "step :  391 loss :  0.03947043791413307\n",
      "step :  392 loss :  0.03946911171078682\n",
      "step :  393 loss :  0.03946969285607338\n",
      "step :  394 loss :  0.03946907818317413\n",
      "step :  395 loss :  0.03946975991129875\n",
      "step :  396 loss :  0.039468470960855484\n",
      "step :  397 loss :  0.03946943208575249\n",
      "step :  398 loss :  0.039468806236982346\n",
      "step :  399 loss :  0.03946937993168831\n",
      "step :  400 loss :  0.0394681952893734\n",
      "step :  401 loss :  0.03946852684020996\n",
      "step :  402 loss :  0.039469052106142044\n",
      "step :  403 loss :  0.03946761041879654\n",
      "step :  404 loss :  0.039468731731176376\n",
      "step :  405 loss :  0.03946823626756668\n",
      "step :  406 loss :  0.03946786746382713\n",
      "step :  407 loss :  0.039468541741371155\n",
      "step :  408 loss :  0.03946761414408684\n",
      "step :  409 loss :  0.03946807608008385\n",
      "step :  410 loss :  0.03946864232420921\n",
      "step :  411 loss :  0.03946756199002266\n",
      "step :  412 loss :  0.03946777060627937\n",
      "step :  413 loss :  0.03946800157427788\n",
      "step :  414 loss :  0.039467424154281616\n",
      "step :  415 loss :  0.039468273520469666\n",
      "step :  416 loss :  0.039467476308345795\n",
      "step :  417 loss :  0.03946844860911369\n",
      "step :  418 loss :  0.03946762531995773\n",
      "step :  419 loss :  0.03946699574589729\n",
      "step :  420 loss :  0.03946768492460251\n",
      "step :  421 loss :  0.039467863738536835\n",
      "step :  422 loss :  0.039467133581638336\n",
      "step :  423 loss :  0.03946760296821594\n",
      "step :  424 loss :  0.03946734219789505\n",
      "step :  425 loss :  0.039467014372348785\n",
      "step :  426 loss :  0.039467014372348785\n",
      "step :  427 loss :  0.039467792958021164\n",
      "step :  428 loss :  0.03946670889854431\n",
      "step :  429 loss :  0.03946715593338013\n",
      "step :  430 loss :  0.039466846734285355\n",
      "step :  431 loss :  0.039466600865125656\n",
      "step :  432 loss :  0.03946791589260101\n",
      "step :  433 loss :  0.039467133581638336\n",
      "step :  434 loss :  0.03946647047996521\n",
      "step :  435 loss :  0.03946700692176819\n",
      "step :  436 loss :  0.039466533809900284\n",
      "step :  437 loss :  0.03946695476770401\n",
      "step :  438 loss :  0.039466649293899536\n",
      "step :  439 loss :  0.03946680203080177\n",
      "step :  440 loss :  0.03946688771247864\n",
      "step :  441 loss :  0.03946639597415924\n",
      "step :  442 loss :  0.03946676105260849\n",
      "step :  443 loss :  0.03946635127067566\n",
      "step :  444 loss :  0.039466679096221924\n",
      "step :  445 loss :  0.03946630656719208\n",
      "step :  446 loss :  0.039466407150030136\n",
      "step :  447 loss :  0.03946628421545029\n",
      "step :  448 loss :  0.039466407150030136\n",
      "step :  449 loss :  0.039466146379709244\n",
      "step :  450 loss :  0.03946636617183685\n",
      "step :  451 loss :  0.03946605697274208\n",
      "step :  452 loss :  0.03946620598435402\n",
      "step :  453 loss :  0.03946603834629059\n",
      "step :  454 loss :  0.039466001093387604\n",
      "step :  455 loss :  0.03946639597415924\n",
      "step :  456 loss :  0.03946586325764656\n",
      "step :  457 loss :  0.03946623578667641\n",
      "step :  458 loss :  0.0394660085439682\n",
      "step :  459 loss :  0.03946584835648537\n",
      "step :  460 loss :  0.03946654498577118\n",
      "step :  461 loss :  0.039466194808483124\n",
      "step :  462 loss :  0.03946594521403313\n",
      "step :  463 loss :  0.0394660159945488\n",
      "step :  464 loss :  0.03946612402796745\n",
      "step :  465 loss :  0.039465706795454025\n",
      "step :  466 loss :  0.03946582227945328\n",
      "step :  467 loss :  0.03946612775325775\n",
      "step :  468 loss :  0.03946590796113014\n",
      "step :  469 loss :  0.0394657626748085\n",
      "step :  470 loss :  0.03946593776345253\n",
      "step :  471 loss :  0.03946557268500328\n",
      "step :  472 loss :  0.03946564719080925\n",
      "step :  473 loss :  0.03946656733751297\n",
      "step :  474 loss :  0.03946642205119133\n",
      "step :  475 loss :  0.039465636014938354\n",
      "step :  476 loss :  0.03946547955274582\n",
      "step :  477 loss :  0.03946579620242119\n",
      "step :  478 loss :  0.039465755224227905\n",
      "step :  479 loss :  0.03946555033326149\n",
      "step :  480 loss :  0.039465680718421936\n",
      "step :  481 loss :  0.03946569561958313\n",
      "step :  482 loss :  0.039465539157390594\n",
      "step :  483 loss :  0.039465535432100296\n",
      "step :  484 loss :  0.03946549445390701\n",
      "step :  485 loss :  0.039465416222810745\n",
      "step :  486 loss :  0.03946571797132492\n",
      "step :  487 loss :  0.03946540132164955\n",
      "step :  488 loss :  0.039465371519327164\n",
      "step :  489 loss :  0.039465710520744324\n",
      "step :  490 loss :  0.03946537896990776\n",
      "step :  491 loss :  0.03946525603532791\n",
      "step :  492 loss :  0.03946562111377716\n",
      "step :  493 loss :  0.03946562856435776\n",
      "step :  494 loss :  0.03946534916758537\n",
      "step :  495 loss :  0.03946540132164955\n",
      "step :  496 loss :  0.03946562111377716\n",
      "step :  497 loss :  0.03946535289287567\n",
      "step :  498 loss :  0.039465319365262985\n",
      "step :  499 loss :  0.03946559876203537\n",
      "Training model 4\n",
      "step :  0 loss :  0.390754759311676\n",
      "step :  1 loss :  0.27103713154792786\n",
      "step :  2 loss :  0.19620636105537415\n",
      "step :  3 loss :  0.2269575595855713\n",
      "step :  4 loss :  0.18639948964118958\n",
      "step :  5 loss :  0.18143849074840546\n",
      "step :  6 loss :  0.17753520607948303\n",
      "step :  7 loss :  0.15086567401885986\n",
      "step :  8 loss :  0.13603965938091278\n",
      "step :  9 loss :  0.12131018191576004\n",
      "step :  10 loss :  0.1415712535381317\n",
      "step :  11 loss :  0.14302457869052887\n",
      "step :  12 loss :  0.1268431395292282\n",
      "step :  13 loss :  0.13005967438220978\n",
      "step :  14 loss :  0.13814541697502136\n",
      "step :  15 loss :  0.1239529550075531\n",
      "step :  16 loss :  0.11161582916975021\n",
      "step :  17 loss :  0.10811150819063187\n",
      "step :  18 loss :  0.10895227640867233\n",
      "step :  19 loss :  0.10564059764146805\n",
      "step :  20 loss :  0.10179735720157623\n",
      "step :  21 loss :  0.10470131039619446\n",
      "step :  22 loss :  0.1034906655550003\n",
      "step :  23 loss :  0.09666118025779724\n",
      "step :  24 loss :  0.09307798743247986\n",
      "step :  25 loss :  0.08690845221281052\n",
      "step :  26 loss :  0.07701224088668823\n",
      "step :  27 loss :  0.07321670651435852\n",
      "step :  28 loss :  0.06647911667823792\n",
      "step :  29 loss :  0.06771200895309448\n",
      "step :  30 loss :  0.060679011046886444\n",
      "step :  31 loss :  0.05667537450790405\n",
      "step :  32 loss :  0.055952515453100204\n",
      "step :  33 loss :  0.05611304193735123\n",
      "step :  34 loss :  0.061806898564100266\n",
      "step :  35 loss :  0.059797316789627075\n",
      "step :  36 loss :  0.05536142364144325\n",
      "step :  37 loss :  0.06293174624443054\n",
      "step :  38 loss :  0.07213976979255676\n",
      "step :  39 loss :  0.08588165044784546\n",
      "step :  40 loss :  0.07980870455503464\n",
      "step :  41 loss :  0.05794180557131767\n",
      "step :  42 loss :  0.05907163396477699\n",
      "step :  43 loss :  0.07137618958950043\n",
      "step :  44 loss :  0.06150374561548233\n",
      "step :  45 loss :  0.0540255606174469\n",
      "step :  46 loss :  0.06035950779914856\n",
      "step :  47 loss :  0.06355883181095123\n",
      "step :  48 loss :  0.05303706228733063\n",
      "step :  49 loss :  0.051835399121046066\n",
      "step :  50 loss :  0.056836921721696854\n",
      "step :  51 loss :  0.05873236432671547\n",
      "step :  52 loss :  0.051238175481557846\n",
      "step :  53 loss :  0.050889309495687485\n",
      "step :  54 loss :  0.05154214799404144\n",
      "step :  55 loss :  0.0494084395468235\n",
      "step :  56 loss :  0.051413267850875854\n",
      "step :  57 loss :  0.04908354952931404\n",
      "step :  58 loss :  0.05126284062862396\n",
      "step :  59 loss :  0.04864322021603584\n",
      "step :  60 loss :  0.049568403512239456\n",
      "step :  61 loss :  0.04892837256193161\n",
      "step :  62 loss :  0.0480755940079689\n",
      "step :  63 loss :  0.04950498789548874\n",
      "step :  64 loss :  0.04773455858230591\n",
      "step :  65 loss :  0.05184483528137207\n",
      "step :  66 loss :  0.05000334233045578\n",
      "step :  67 loss :  0.05825067684054375\n",
      "step :  68 loss :  0.05230345577001572\n",
      "step :  69 loss :  0.054938994348049164\n",
      "step :  70 loss :  0.04677621275186539\n",
      "step :  71 loss :  0.04650437459349632\n",
      "step :  72 loss :  0.04931710287928581\n",
      "step :  73 loss :  0.04811222106218338\n",
      "step :  74 loss :  0.05343230441212654\n",
      "step :  75 loss :  0.046502817422151566\n",
      "step :  76 loss :  0.049595147371292114\n",
      "step :  77 loss :  0.04511065408587456\n",
      "step :  78 loss :  0.04814198240637779\n",
      "step :  79 loss :  0.04479411244392395\n",
      "step :  80 loss :  0.049242593348026276\n",
      "step :  81 loss :  0.04456881433725357\n",
      "step :  82 loss :  0.04892940819263458\n",
      "step :  83 loss :  0.044129375368356705\n",
      "step :  84 loss :  0.048067666590213776\n",
      "step :  85 loss :  0.04381391406059265\n",
      "step :  86 loss :  0.04583754763007164\n",
      "step :  87 loss :  0.04398169741034508\n",
      "step :  88 loss :  0.04368745535612106\n",
      "step :  89 loss :  0.04540582373738289\n",
      "step :  90 loss :  0.0435432493686676\n",
      "step :  91 loss :  0.043837204575538635\n",
      "step :  92 loss :  0.04305506870150566\n",
      "step :  93 loss :  0.044055432081222534\n",
      "step :  94 loss :  0.04302121326327324\n",
      "step :  95 loss :  0.04301678016781807\n",
      "step :  96 loss :  0.04287140071392059\n",
      "step :  97 loss :  0.04229477792978287\n",
      "step :  98 loss :  0.044213104993104935\n",
      "step :  99 loss :  0.042090147733688354\n",
      "step :  100 loss :  0.04521460458636284\n",
      "step :  101 loss :  0.0422079898416996\n",
      "step :  102 loss :  0.044600315392017365\n",
      "step :  103 loss :  0.04172153025865555\n",
      "step :  104 loss :  0.04447221755981445\n",
      "step :  105 loss :  0.04159817844629288\n",
      "step :  106 loss :  0.04418463632464409\n",
      "step :  107 loss :  0.04138096421957016\n",
      "step :  108 loss :  0.043838515877723694\n",
      "step :  109 loss :  0.04122653231024742\n",
      "step :  110 loss :  0.04358920082449913\n",
      "step :  111 loss :  0.04118261858820915\n",
      "step :  112 loss :  0.04309134930372238\n",
      "step :  113 loss :  0.04117671772837639\n",
      "step :  114 loss :  0.04334035888314247\n",
      "step :  115 loss :  0.04089973866939545\n",
      "step :  116 loss :  0.04161165654659271\n",
      "step :  117 loss :  0.041118159890174866\n",
      "step :  118 loss :  0.040702857077121735\n",
      "step :  119 loss :  0.04112592712044716\n",
      "step :  120 loss :  0.04082454368472099\n",
      "step :  121 loss :  0.04077724367380142\n",
      "step :  122 loss :  0.04119538888335228\n",
      "step :  123 loss :  0.040591370314359665\n",
      "step :  124 loss :  0.040825262665748596\n",
      "step :  125 loss :  0.04051566496491432\n",
      "step :  126 loss :  0.04069258272647858\n",
      "step :  127 loss :  0.04039709270000458\n",
      "step :  128 loss :  0.040554989129304886\n",
      "step :  129 loss :  0.04040824621915817\n",
      "step :  130 loss :  0.04034708812832832\n",
      "step :  131 loss :  0.04044231027364731\n",
      "step :  132 loss :  0.040317028760910034\n",
      "step :  133 loss :  0.040418174117803574\n",
      "step :  134 loss :  0.04034765437245369\n",
      "step :  135 loss :  0.04048402979969978\n",
      "step :  136 loss :  0.04031024128198624\n",
      "step :  137 loss :  0.04036473482847214\n",
      "step :  138 loss :  0.04028983414173126\n",
      "step :  139 loss :  0.040228649973869324\n",
      "step :  140 loss :  0.04015433415770531\n",
      "step :  141 loss :  0.04016662761569023\n",
      "step :  142 loss :  0.04014364629983902\n",
      "step :  143 loss :  0.04017416760325432\n",
      "step :  144 loss :  0.04009975865483284\n",
      "step :  145 loss :  0.04008900001645088\n",
      "step :  146 loss :  0.040094293653964996\n",
      "step :  147 loss :  0.0400308333337307\n",
      "step :  148 loss :  0.040050916373729706\n",
      "step :  149 loss :  0.04002911224961281\n",
      "step :  150 loss :  0.040000781416893005\n",
      "step :  151 loss :  0.03992623835802078\n",
      "step :  152 loss :  0.03989700600504875\n",
      "step :  153 loss :  0.03991807624697685\n",
      "step :  154 loss :  0.03994538262486458\n",
      "step :  155 loss :  0.03989701345562935\n",
      "step :  156 loss :  0.03986925631761551\n",
      "step :  157 loss :  0.03986385837197304\n",
      "step :  158 loss :  0.03988031670451164\n",
      "step :  159 loss :  0.03984427824616432\n",
      "step :  160 loss :  0.03981362283229828\n",
      "step :  161 loss :  0.03976474329829216\n",
      "step :  162 loss :  0.039783962070941925\n",
      "step :  163 loss :  0.039813265204429626\n",
      "step :  164 loss :  0.03976914659142494\n",
      "step :  165 loss :  0.03977133333683014\n",
      "step :  166 loss :  0.03971968591213226\n",
      "step :  167 loss :  0.03976646438241005\n",
      "step :  168 loss :  0.039762940257787704\n",
      "step :  169 loss :  0.0397629477083683\n",
      "step :  170 loss :  0.03973502665758133\n",
      "step :  171 loss :  0.039711128920316696\n",
      "step :  172 loss :  0.0396973192691803\n",
      "step :  173 loss :  0.039687253534793854\n",
      "step :  174 loss :  0.03964998200535774\n",
      "step :  175 loss :  0.03969568759202957\n",
      "step :  176 loss :  0.03964880108833313\n",
      "step :  177 loss :  0.039656322449445724\n",
      "step :  178 loss :  0.039618685841560364\n",
      "step :  179 loss :  0.03963536396622658\n",
      "step :  180 loss :  0.039653267711400986\n",
      "step :  181 loss :  0.03961588069796562\n",
      "step :  182 loss :  0.039624929428100586\n",
      "step :  183 loss :  0.0395747646689415\n",
      "step :  184 loss :  0.0395582839846611\n",
      "step :  185 loss :  0.03959577903151512\n",
      "step :  186 loss :  0.0396188423037529\n",
      "step :  187 loss :  0.03956463560461998\n",
      "step :  188 loss :  0.039536699652671814\n",
      "step :  189 loss :  0.03952933847904205\n",
      "step :  190 loss :  0.03955817222595215\n",
      "step :  191 loss :  0.0395512729883194\n",
      "step :  192 loss :  0.039522334933280945\n",
      "step :  193 loss :  0.03951983526349068\n",
      "step :  194 loss :  0.03951156511902809\n",
      "step :  195 loss :  0.03951718285679817\n",
      "step :  196 loss :  0.03949962183833122\n",
      "step :  197 loss :  0.03949877992272377\n",
      "step :  198 loss :  0.039491381496191025\n",
      "step :  199 loss :  0.03948213532567024\n",
      "step :  200 loss :  0.03947705775499344\n",
      "step :  201 loss :  0.03949998691678047\n",
      "step :  202 loss :  0.03946392983198166\n",
      "step :  203 loss :  0.03948409855365753\n",
      "step :  204 loss :  0.039460189640522\n",
      "step :  205 loss :  0.03947306424379349\n",
      "step :  206 loss :  0.03942682966589928\n",
      "step :  207 loss :  0.03941699489951134\n",
      "step :  208 loss :  0.03947868570685387\n",
      "step :  209 loss :  0.039435215294361115\n",
      "step :  210 loss :  0.03947065770626068\n",
      "step :  211 loss :  0.03945589438080788\n",
      "step :  212 loss :  0.03941584378480911\n",
      "step :  213 loss :  0.03942257538437843\n",
      "step :  214 loss :  0.039409417659044266\n",
      "step :  215 loss :  0.039421286433935165\n",
      "step :  216 loss :  0.039399776607751846\n",
      "step :  217 loss :  0.039390478283166885\n",
      "step :  218 loss :  0.0393877811729908\n",
      "step :  219 loss :  0.03938727453351021\n",
      "step :  220 loss :  0.039403919130563736\n",
      "step :  221 loss :  0.03939800709486008\n",
      "step :  222 loss :  0.03938090056180954\n",
      "step :  223 loss :  0.039387792348861694\n",
      "step :  224 loss :  0.03936724737286568\n",
      "step :  225 loss :  0.039399802684783936\n",
      "step :  226 loss :  0.039369840174913406\n",
      "step :  227 loss :  0.039365753531455994\n",
      "step :  228 loss :  0.03938587009906769\n",
      "step :  229 loss :  0.03935905918478966\n",
      "step :  230 loss :  0.039353374391794205\n",
      "step :  231 loss :  0.03934174031019211\n",
      "step :  232 loss :  0.03935713320970535\n",
      "step :  233 loss :  0.03932752460241318\n",
      "step :  234 loss :  0.03933017700910568\n",
      "step :  235 loss :  0.03935525193810463\n",
      "step :  236 loss :  0.03933554142713547\n",
      "step :  237 loss :  0.03933653607964516\n",
      "step :  238 loss :  0.03936908021569252\n",
      "step :  239 loss :  0.03932051733136177\n",
      "step :  240 loss :  0.03931606188416481\n",
      "step :  241 loss :  0.039341580122709274\n",
      "step :  242 loss :  0.039313316345214844\n",
      "step :  243 loss :  0.03931635618209839\n",
      "step :  244 loss :  0.039311010390520096\n",
      "step :  245 loss :  0.03930174559354782\n",
      "step :  246 loss :  0.039304543286561966\n",
      "step :  247 loss :  0.039302144199609756\n",
      "step :  248 loss :  0.039305783808231354\n",
      "step :  249 loss :  0.0392998531460762\n",
      "step :  250 loss :  0.03930378705263138\n",
      "step :  251 loss :  0.03929680213332176\n",
      "step :  252 loss :  0.03930503875017166\n",
      "step :  253 loss :  0.03929421305656433\n",
      "step :  254 loss :  0.0392889529466629\n",
      "step :  255 loss :  0.03929806500673294\n",
      "step :  256 loss :  0.03928093612194061\n",
      "step :  257 loss :  0.03928607329726219\n",
      "step :  258 loss :  0.03928592801094055\n",
      "step :  259 loss :  0.03928227722644806\n",
      "step :  260 loss :  0.03928836062550545\n",
      "step :  261 loss :  0.03928249701857567\n",
      "step :  262 loss :  0.03929496929049492\n",
      "step :  263 loss :  0.03927595168352127\n",
      "step :  264 loss :  0.03927111625671387\n",
      "step :  265 loss :  0.03927694261074066\n",
      "step :  266 loss :  0.03926979377865791\n",
      "step :  267 loss :  0.03926942124962807\n",
      "step :  268 loss :  0.03926936909556389\n",
      "step :  269 loss :  0.03926834464073181\n",
      "step :  270 loss :  0.03927488997578621\n",
      "step :  271 loss :  0.039265796542167664\n",
      "step :  272 loss :  0.03926650062203407\n",
      "step :  273 loss :  0.0392812080681324\n",
      "step :  274 loss :  0.039261408150196075\n",
      "step :  275 loss :  0.039257463067770004\n",
      "step :  276 loss :  0.03926204890012741\n",
      "step :  277 loss :  0.03925416246056557\n",
      "step :  278 loss :  0.03926435112953186\n",
      "step :  279 loss :  0.0392560139298439\n",
      "step :  280 loss :  0.03925560042262077\n",
      "step :  281 loss :  0.03927614912390709\n",
      "step :  282 loss :  0.039261527359485626\n",
      "step :  283 loss :  0.03925209492444992\n",
      "step :  284 loss :  0.039256058633327484\n",
      "step :  285 loss :  0.03926025703549385\n",
      "step :  286 loss :  0.039246682077646255\n",
      "step :  287 loss :  0.039242222905159\n",
      "step :  288 loss :  0.039242539554834366\n",
      "step :  289 loss :  0.03924844041466713\n",
      "step :  290 loss :  0.03924407809972763\n",
      "step :  291 loss :  0.0392473079264164\n",
      "step :  292 loss :  0.03924241662025452\n",
      "step :  293 loss :  0.03923792019486427\n",
      "step :  294 loss :  0.03924607113003731\n",
      "step :  295 loss :  0.03923924267292023\n",
      "step :  296 loss :  0.039237577468156815\n",
      "step :  297 loss :  0.0392499603331089\n",
      "step :  298 loss :  0.03924107179045677\n",
      "step :  299 loss :  0.03923635929822922\n",
      "step :  300 loss :  0.039236102253198624\n",
      "step :  301 loss :  0.039251770824193954\n",
      "step :  302 loss :  0.03923165425658226\n",
      "step :  303 loss :  0.03922943398356438\n",
      "step :  304 loss :  0.03924326226115227\n",
      "step :  305 loss :  0.03923176974058151\n",
      "step :  306 loss :  0.03923088312149048\n",
      "step :  307 loss :  0.03923457860946655\n",
      "step :  308 loss :  0.03924134373664856\n",
      "step :  309 loss :  0.03922666236758232\n",
      "step :  310 loss :  0.03922587260603905\n",
      "step :  311 loss :  0.039234451949596405\n",
      "step :  312 loss :  0.03923662379384041\n",
      "step :  313 loss :  0.0392252542078495\n",
      "step :  314 loss :  0.039223603904247284\n",
      "step :  315 loss :  0.03923066332936287\n",
      "step :  316 loss :  0.03922160714864731\n",
      "step :  317 loss :  0.03922141343355179\n",
      "step :  318 loss :  0.039232287555933\n",
      "step :  319 loss :  0.03922707587480545\n",
      "step :  320 loss :  0.03922216594219208\n",
      "step :  321 loss :  0.039225537329912186\n",
      "step :  322 loss :  0.03922879323363304\n",
      "step :  323 loss :  0.0392245277762413\n",
      "step :  324 loss :  0.03921953961253166\n",
      "step :  325 loss :  0.03922285884618759\n",
      "step :  326 loss :  0.03921876475214958\n",
      "step :  327 loss :  0.039217665791511536\n",
      "step :  328 loss :  0.03922334313392639\n",
      "step :  329 loss :  0.03921736031770706\n",
      "step :  330 loss :  0.03921734541654587\n",
      "step :  331 loss :  0.039224449545145035\n",
      "step :  332 loss :  0.03922076150774956\n",
      "step :  333 loss :  0.03921549767255783\n",
      "step :  334 loss :  0.039221685379743576\n",
      "step :  335 loss :  0.03922151029109955\n",
      "step :  336 loss :  0.039214666932821274\n",
      "step :  337 loss :  0.039216332137584686\n",
      "step :  338 loss :  0.03921951353549957\n",
      "step :  339 loss :  0.03921537846326828\n",
      "step :  340 loss :  0.039212729781866074\n",
      "step :  341 loss :  0.03921307623386383\n",
      "step :  342 loss :  0.039212778210639954\n",
      "step :  343 loss :  0.0392126627266407\n",
      "step :  344 loss :  0.03921332210302353\n",
      "step :  345 loss :  0.0392146036028862\n",
      "step :  346 loss :  0.039215270429849625\n",
      "step :  347 loss :  0.03921378403902054\n",
      "step :  348 loss :  0.039213333278894424\n",
      "step :  349 loss :  0.03921295329928398\n",
      "step :  350 loss :  0.039212778210639954\n",
      "step :  351 loss :  0.039212148636579514\n",
      "step :  352 loss :  0.039210252463817596\n",
      "step :  353 loss :  0.039212219417095184\n",
      "step :  354 loss :  0.03920995071530342\n",
      "step :  355 loss :  0.03920944035053253\n",
      "step :  356 loss :  0.039211008697748184\n",
      "step :  357 loss :  0.039210859686136246\n",
      "step :  358 loss :  0.0392095185816288\n",
      "step :  359 loss :  0.03921053931117058\n",
      "step :  360 loss :  0.03920942172408104\n",
      "step :  361 loss :  0.039208825677633286\n",
      "step :  362 loss :  0.03920817747712135\n",
      "step :  363 loss :  0.03920980915427208\n",
      "step :  364 loss :  0.03920803964138031\n",
      "step :  365 loss :  0.03920849412679672\n",
      "step :  366 loss :  0.039209600538015366\n",
      "step :  367 loss :  0.03920954465866089\n",
      "step :  368 loss :  0.03920846804976463\n",
      "step :  369 loss :  0.03920767083764076\n",
      "step :  370 loss :  0.03920833021402359\n",
      "step :  371 loss :  0.03920687735080719\n",
      "step :  372 loss :  0.03920707479119301\n",
      "step :  373 loss :  0.039207808673381805\n",
      "step :  374 loss :  0.03920818120241165\n",
      "step :  375 loss :  0.039206407964229584\n",
      "step :  376 loss :  0.03920706361532211\n",
      "step :  377 loss :  0.039205800741910934\n",
      "step :  378 loss :  0.03920556977391243\n",
      "step :  379 loss :  0.03920743986964226\n",
      "step :  380 loss :  0.039205510169267654\n",
      "step :  381 loss :  0.03920547664165497\n",
      "step :  382 loss :  0.03920740634202957\n",
      "step :  383 loss :  0.03920544311404228\n",
      "step :  384 loss :  0.03920482099056244\n",
      "step :  385 loss :  0.03920730575919151\n",
      "step :  386 loss :  0.039207883179187775\n",
      "step :  387 loss :  0.03920518234372139\n",
      "step :  388 loss :  0.0392036996781826\n",
      "step :  389 loss :  0.03920424357056618\n",
      "step :  390 loss :  0.03920367360115051\n",
      "step :  391 loss :  0.03920431807637215\n",
      "step :  392 loss :  0.039203789085149765\n",
      "step :  393 loss :  0.03920387849211693\n",
      "step :  394 loss :  0.03920336067676544\n",
      "step :  395 loss :  0.0392032228410244\n",
      "step :  396 loss :  0.03920351341366768\n",
      "step :  397 loss :  0.03920308127999306\n",
      "step :  398 loss :  0.03920479118824005\n",
      "step :  399 loss :  0.03920460864901543\n",
      "step :  400 loss :  0.039203424006700516\n",
      "step :  401 loss :  0.03920333832502365\n",
      "step :  402 loss :  0.03920302912592888\n",
      "step :  403 loss :  0.03920229896903038\n",
      "step :  404 loss :  0.039202965795993805\n",
      "step :  405 loss :  0.039202213287353516\n",
      "step :  406 loss :  0.03920277953147888\n",
      "step :  407 loss :  0.039202943444252014\n",
      "step :  408 loss :  0.0392024889588356\n",
      "step :  409 loss :  0.039202090352773666\n",
      "step :  410 loss :  0.0392017625272274\n",
      "step :  411 loss :  0.039201684296131134\n",
      "step :  412 loss :  0.03920179605484009\n",
      "step :  413 loss :  0.03920208662748337\n",
      "step :  414 loss :  0.039202071726322174\n",
      "step :  415 loss :  0.03920193761587143\n",
      "step :  416 loss :  0.039201781153678894\n",
      "step :  417 loss :  0.03920122981071472\n",
      "step :  418 loss :  0.03920155391097069\n",
      "step :  419 loss :  0.039201151579618454\n",
      "step :  420 loss :  0.039201389998197556\n",
      "step :  421 loss :  0.03920098766684532\n",
      "step :  422 loss :  0.03920124098658562\n",
      "step :  423 loss :  0.039200544357299805\n",
      "step :  424 loss :  0.039200589060783386\n",
      "step :  425 loss :  0.039201900362968445\n",
      "step :  426 loss :  0.039201512932777405\n",
      "step :  427 loss :  0.03920047730207443\n",
      "step :  428 loss :  0.039201006293296814\n",
      "step :  429 loss :  0.03920143097639084\n",
      "step :  430 loss :  0.0392010360956192\n",
      "step :  431 loss :  0.03920036554336548\n",
      "step :  432 loss :  0.03920014202594757\n",
      "step :  433 loss :  0.03920022398233414\n",
      "step :  434 loss :  0.03920123353600502\n",
      "step :  435 loss :  0.039200976490974426\n",
      "step :  436 loss :  0.03920004144310951\n",
      "step :  437 loss :  0.03920000419020653\n",
      "step :  438 loss :  0.039200980216264725\n",
      "step :  439 loss :  0.03920101374387741\n",
      "step :  440 loss :  0.039200309664011\n",
      "step :  441 loss :  0.03919991850852966\n",
      "step :  442 loss :  0.0392007976770401\n",
      "step :  443 loss :  0.039201151579618454\n",
      "step :  444 loss :  0.03920020908117294\n",
      "step :  445 loss :  0.039199501276016235\n",
      "step :  446 loss :  0.03919969126582146\n",
      "step :  447 loss :  0.039200309664011\n",
      "step :  448 loss :  0.03920001536607742\n",
      "step :  449 loss :  0.03919953852891922\n",
      "step :  450 loss :  0.03919997066259384\n",
      "step :  451 loss :  0.03919975459575653\n",
      "step :  452 loss :  0.039199333637952805\n",
      "step :  453 loss :  0.039199501276016235\n",
      "step :  454 loss :  0.03919950872659683\n",
      "step :  455 loss :  0.03919960558414459\n",
      "step :  456 loss :  0.03919946029782295\n",
      "step :  457 loss :  0.0391993410885334\n",
      "step :  458 loss :  0.03919953852891922\n",
      "step :  459 loss :  0.03919915854930878\n",
      "step :  460 loss :  0.039199098944664\n",
      "step :  461 loss :  0.039199311286211014\n",
      "step :  462 loss :  0.03919922560453415\n",
      "step :  463 loss :  0.03919929638504982\n",
      "step :  464 loss :  0.03919924050569534\n",
      "step :  465 loss :  0.039199162274599075\n",
      "step :  466 loss :  0.03919920697808266\n",
      "step :  467 loss :  0.03919894993305206\n",
      "step :  468 loss :  0.03919897973537445\n",
      "step :  469 loss :  0.03919927775859833\n",
      "step :  470 loss :  0.039199020713567734\n",
      "step :  471 loss :  0.03919877111911774\n",
      "step :  472 loss :  0.03919891640543938\n",
      "step :  473 loss :  0.039198972284793854\n",
      "step :  474 loss :  0.03919891640543938\n",
      "step :  475 loss :  0.03919918090105057\n",
      "step :  476 loss :  0.03919900208711624\n",
      "step :  477 loss :  0.039198748767375946\n",
      "step :  478 loss :  0.03919892758131027\n",
      "step :  479 loss :  0.03919905796647072\n",
      "step :  480 loss :  0.03919893875718117\n",
      "step :  481 loss :  0.03919869288802147\n",
      "step :  482 loss :  0.03919883072376251\n",
      "step :  483 loss :  0.039198629558086395\n",
      "step :  484 loss :  0.039198655635118484\n",
      "step :  485 loss :  0.03919890150427818\n",
      "step :  486 loss :  0.03919871523976326\n",
      "step :  487 loss :  0.03919852897524834\n",
      "step :  488 loss :  0.039198797196149826\n",
      "step :  489 loss :  0.03919902816414833\n",
      "step :  490 loss :  0.03919888287782669\n",
      "step :  491 loss :  0.03919852897524834\n",
      "step :  492 loss :  0.039198510348796844\n",
      "step :  493 loss :  0.039198633283376694\n",
      "step :  494 loss :  0.03919844701886177\n",
      "step :  495 loss :  0.039198391139507294\n",
      "step :  496 loss :  0.039198581129312515\n",
      "step :  497 loss :  0.03919849917292595\n",
      "step :  498 loss :  0.03919835388660431\n",
      "step :  499 loss :  0.039198536425828934\n",
      "Training model 5\n",
      "step :  0 loss :  0.48035135865211487\n",
      "step :  1 loss :  0.26593080163002014\n",
      "step :  2 loss :  0.20478741824626923\n",
      "step :  3 loss :  0.23612141609191895\n",
      "step :  4 loss :  0.17869988083839417\n",
      "step :  5 loss :  0.19285109639167786\n",
      "step :  6 loss :  0.18981187045574188\n",
      "step :  7 loss :  0.15325194597244263\n",
      "step :  8 loss :  0.1410835087299347\n",
      "step :  9 loss :  0.12669743597507477\n",
      "step :  10 loss :  0.13331951200962067\n",
      "step :  11 loss :  0.14141136407852173\n",
      "step :  12 loss :  0.12967751920223236\n",
      "step :  13 loss :  0.13517118990421295\n",
      "step :  14 loss :  0.1430458128452301\n",
      "step :  15 loss :  0.1267218440771103\n",
      "step :  16 loss :  0.11738332360982895\n",
      "step :  17 loss :  0.11307960748672485\n",
      "step :  18 loss :  0.11056052148342133\n",
      "step :  19 loss :  0.10810130834579468\n",
      "step :  20 loss :  0.10706911981105804\n",
      "step :  21 loss :  0.10757923126220703\n",
      "step :  22 loss :  0.10556544363498688\n",
      "step :  23 loss :  0.10643282532691956\n",
      "step :  24 loss :  0.1072361022233963\n",
      "step :  25 loss :  0.10359928011894226\n",
      "step :  26 loss :  0.10089625418186188\n",
      "step :  27 loss :  0.09533444792032242\n",
      "step :  28 loss :  0.09015127271413803\n",
      "step :  29 loss :  0.0901143029332161\n",
      "step :  30 loss :  0.08592920750379562\n",
      "step :  31 loss :  0.07729199528694153\n",
      "step :  32 loss :  0.07521313428878784\n",
      "step :  33 loss :  0.06814859062433243\n",
      "step :  34 loss :  0.0667235478758812\n",
      "step :  35 loss :  0.05638241022825241\n",
      "step :  36 loss :  0.05902751162648201\n",
      "step :  37 loss :  0.05357860401272774\n",
      "step :  38 loss :  0.05723358318209648\n",
      "step :  39 loss :  0.05105327442288399\n",
      "step :  40 loss :  0.05828116461634636\n",
      "step :  41 loss :  0.06009106710553169\n",
      "step :  42 loss :  0.054876890033483505\n",
      "step :  43 loss :  0.062063660472631454\n",
      "step :  44 loss :  0.08257434517145157\n",
      "step :  45 loss :  0.08948098868131638\n",
      "step :  46 loss :  0.071390300989151\n",
      "step :  47 loss :  0.05081100016832352\n",
      "step :  48 loss :  0.059648267924785614\n",
      "step :  49 loss :  0.06760261207818985\n",
      "step :  50 loss :  0.05145261064171791\n",
      "step :  51 loss :  0.04981914162635803\n",
      "step :  52 loss :  0.054786909371614456\n",
      "step :  53 loss :  0.049122683703899384\n",
      "step :  54 loss :  0.049653828144073486\n",
      "step :  55 loss :  0.04736277833580971\n",
      "step :  56 loss :  0.04673231765627861\n",
      "step :  57 loss :  0.046297211199998856\n",
      "step :  58 loss :  0.04568762332201004\n",
      "step :  59 loss :  0.045556746423244476\n",
      "step :  60 loss :  0.04527055099606514\n",
      "step :  61 loss :  0.04615071043372154\n",
      "step :  62 loss :  0.05097769573330879\n",
      "step :  63 loss :  0.052637822926044464\n",
      "step :  64 loss :  0.06006866320967674\n",
      "step :  65 loss :  0.053172435611486435\n",
      "step :  66 loss :  0.050621289759874344\n",
      "step :  67 loss :  0.04803556576371193\n",
      "step :  68 loss :  0.0485224723815918\n",
      "step :  69 loss :  0.04795955866575241\n",
      "step :  70 loss :  0.046700865030288696\n",
      "step :  71 loss :  0.04584811255335808\n",
      "step :  72 loss :  0.044784657657146454\n",
      "step :  73 loss :  0.045506954193115234\n",
      "step :  74 loss :  0.04603110998868942\n",
      "step :  75 loss :  0.045481156557798386\n",
      "step :  76 loss :  0.047154270112514496\n",
      "step :  77 loss :  0.045708704739809036\n",
      "step :  78 loss :  0.04655557870864868\n",
      "step :  79 loss :  0.04541083052754402\n",
      "step :  80 loss :  0.047819193452596664\n",
      "step :  81 loss :  0.04579966887831688\n",
      "step :  82 loss :  0.04951794818043709\n",
      "step :  83 loss :  0.046428173780441284\n",
      "step :  84 loss :  0.04652409255504608\n",
      "step :  85 loss :  0.04332554712891579\n",
      "step :  86 loss :  0.044028349220752716\n",
      "step :  87 loss :  0.041589874774217606\n",
      "step :  88 loss :  0.04181946814060211\n",
      "step :  89 loss :  0.041100990027189255\n",
      "step :  90 loss :  0.04243689402937889\n",
      "step :  91 loss :  0.04151720926165581\n",
      "step :  92 loss :  0.04361529275774956\n",
      "step :  93 loss :  0.04187759757041931\n",
      "step :  94 loss :  0.04477664828300476\n",
      "step :  95 loss :  0.04317301884293556\n",
      "step :  96 loss :  0.045839328318834305\n",
      "step :  97 loss :  0.041580185294151306\n",
      "step :  98 loss :  0.04356914013624191\n",
      "step :  99 loss :  0.040891408920288086\n",
      "step :  100 loss :  0.04235756769776344\n",
      "step :  101 loss :  0.04076440632343292\n",
      "step :  102 loss :  0.04274534806609154\n",
      "step :  103 loss :  0.041102226823568344\n",
      "step :  104 loss :  0.043038733303546906\n",
      "step :  105 loss :  0.0406101830303669\n",
      "step :  106 loss :  0.042401835322380066\n",
      "step :  107 loss :  0.04030098766088486\n",
      "step :  108 loss :  0.04221072047948837\n",
      "step :  109 loss :  0.040044382214546204\n",
      "step :  110 loss :  0.04157930985093117\n",
      "step :  111 loss :  0.03976815938949585\n",
      "step :  112 loss :  0.040511615574359894\n",
      "step :  113 loss :  0.03965173289179802\n",
      "step :  114 loss :  0.039998967200517654\n",
      "step :  115 loss :  0.03958834335207939\n",
      "step :  116 loss :  0.0397331640124321\n",
      "step :  117 loss :  0.03954077512025833\n",
      "step :  118 loss :  0.03949051350355148\n",
      "step :  119 loss :  0.039550743997097015\n",
      "step :  120 loss :  0.03947525471448898\n",
      "step :  121 loss :  0.03950875997543335\n",
      "step :  122 loss :  0.03949250653386116\n",
      "step :  123 loss :  0.03970116749405861\n",
      "step :  124 loss :  0.03945052996277809\n",
      "step :  125 loss :  0.03970013186335564\n",
      "step :  126 loss :  0.03942454978823662\n",
      "step :  127 loss :  0.03955483064055443\n",
      "step :  128 loss :  0.03943925350904465\n",
      "step :  129 loss :  0.0399383120238781\n",
      "step :  130 loss :  0.039410579949617386\n",
      "step :  131 loss :  0.04022933542728424\n",
      "step :  132 loss :  0.0393775999546051\n",
      "step :  133 loss :  0.03984757885336876\n",
      "step :  134 loss :  0.03947953134775162\n",
      "step :  135 loss :  0.03958456218242645\n",
      "step :  136 loss :  0.03959023207426071\n",
      "step :  137 loss :  0.039451714605093\n",
      "step :  138 loss :  0.039428941905498505\n",
      "step :  139 loss :  0.03948823735117912\n",
      "step :  140 loss :  0.03958764299750328\n",
      "step :  141 loss :  0.0393226258456707\n",
      "step :  142 loss :  0.03958795592188835\n",
      "step :  143 loss :  0.03930538147687912\n",
      "step :  144 loss :  0.03957146778702736\n",
      "step :  145 loss :  0.03930020332336426\n",
      "step :  146 loss :  0.03956088423728943\n",
      "step :  147 loss :  0.03929893672466278\n",
      "step :  148 loss :  0.03946210443973541\n",
      "step :  149 loss :  0.03924770653247833\n",
      "step :  150 loss :  0.039550650864839554\n",
      "step :  151 loss :  0.03924182429909706\n",
      "step :  152 loss :  0.039487067610025406\n",
      "step :  153 loss :  0.03926531970500946\n",
      "step :  154 loss :  0.03921150788664818\n",
      "step :  155 loss :  0.03945354372262955\n",
      "step :  156 loss :  0.03918420150876045\n",
      "step :  157 loss :  0.03944046050310135\n",
      "step :  158 loss :  0.039189741015434265\n",
      "step :  159 loss :  0.03924389183521271\n",
      "step :  160 loss :  0.03915831819176674\n",
      "step :  161 loss :  0.03931494802236557\n",
      "step :  162 loss :  0.039134785532951355\n",
      "step :  163 loss :  0.03926850110292435\n",
      "step :  164 loss :  0.039186690002679825\n",
      "step :  165 loss :  0.03921035677194595\n",
      "step :  166 loss :  0.03913942724466324\n",
      "step :  167 loss :  0.03918500617146492\n",
      "step :  168 loss :  0.03911352902650833\n",
      "step :  169 loss :  0.03919109329581261\n",
      "step :  170 loss :  0.039094168692827225\n",
      "step :  171 loss :  0.039201341569423676\n",
      "step :  172 loss :  0.039074137806892395\n",
      "step :  173 loss :  0.03918978571891785\n",
      "step :  174 loss :  0.03906403109431267\n",
      "step :  175 loss :  0.03911386430263519\n",
      "step :  176 loss :  0.03914187476038933\n",
      "step :  177 loss :  0.039088401943445206\n",
      "step :  178 loss :  0.03906843438744545\n",
      "step :  179 loss :  0.03906494379043579\n",
      "step :  180 loss :  0.03906533494591713\n",
      "step :  181 loss :  0.039027802646160126\n",
      "step :  182 loss :  0.03913215920329094\n",
      "step :  183 loss :  0.03900520130991936\n",
      "step :  184 loss :  0.03913935646414757\n",
      "step :  185 loss :  0.039054788649082184\n",
      "step :  186 loss :  0.03903074935078621\n",
      "step :  187 loss :  0.039046067744493484\n",
      "step :  188 loss :  0.03902146592736244\n",
      "step :  189 loss :  0.039032209664583206\n",
      "step :  190 loss :  0.039018090814352036\n",
      "step :  191 loss :  0.03901452571153641\n",
      "step :  192 loss :  0.038989339023828506\n",
      "step :  193 loss :  0.03901179879903793\n",
      "step :  194 loss :  0.038981612771749496\n",
      "step :  195 loss :  0.03904944285750389\n",
      "step :  196 loss :  0.038979653269052505\n",
      "step :  197 loss :  0.03904017060995102\n",
      "step :  198 loss :  0.038967423141002655\n",
      "step :  199 loss :  0.03900879994034767\n",
      "step :  200 loss :  0.039006300270557404\n",
      "step :  201 loss :  0.03898371011018753\n",
      "step :  202 loss :  0.038992319256067276\n",
      "step :  203 loss :  0.038948576897382736\n",
      "step :  204 loss :  0.038989629596471786\n",
      "step :  205 loss :  0.038970641791820526\n",
      "step :  206 loss :  0.03897305205464363\n",
      "step :  207 loss :  0.038938965648412704\n",
      "step :  208 loss :  0.03893953934311867\n",
      "step :  209 loss :  0.03896552696824074\n",
      "step :  210 loss :  0.038944486528635025\n",
      "step :  211 loss :  0.03897007927298546\n",
      "step :  212 loss :  0.03892562910914421\n",
      "step :  213 loss :  0.03896116837859154\n",
      "step :  214 loss :  0.03893394395709038\n",
      "step :  215 loss :  0.038915615528821945\n",
      "step :  216 loss :  0.03896269574761391\n",
      "step :  217 loss :  0.0389096699655056\n",
      "step :  218 loss :  0.038912538439035416\n",
      "step :  219 loss :  0.03891821578145027\n",
      "step :  220 loss :  0.03890702500939369\n",
      "step :  221 loss :  0.038924165070056915\n",
      "step :  222 loss :  0.03889282047748566\n",
      "step :  223 loss :  0.038906827569007874\n",
      "step :  224 loss :  0.03889453038573265\n",
      "step :  225 loss :  0.03888511285185814\n",
      "step :  226 loss :  0.03890816867351532\n",
      "step :  227 loss :  0.03887555003166199\n",
      "step :  228 loss :  0.038903310894966125\n",
      "step :  229 loss :  0.03888189420104027\n",
      "step :  230 loss :  0.03892558813095093\n",
      "step :  231 loss :  0.03890442103147507\n",
      "step :  232 loss :  0.038889072835445404\n",
      "step :  233 loss :  0.03887486830353737\n",
      "step :  234 loss :  0.03888363018631935\n",
      "step :  235 loss :  0.038882456719875336\n",
      "step :  236 loss :  0.03888155519962311\n",
      "step :  237 loss :  0.03888477385044098\n",
      "step :  238 loss :  0.038884762674570084\n",
      "step :  239 loss :  0.03888769820332527\n",
      "step :  240 loss :  0.03884938359260559\n",
      "step :  241 loss :  0.03886181861162186\n",
      "step :  242 loss :  0.03891536593437195\n",
      "step :  243 loss :  0.038864705711603165\n",
      "step :  244 loss :  0.0388433039188385\n",
      "step :  245 loss :  0.038859348744153976\n",
      "step :  246 loss :  0.03887796029448509\n",
      "step :  247 loss :  0.038856152445077896\n",
      "step :  248 loss :  0.03883795067667961\n",
      "step :  249 loss :  0.038851577788591385\n",
      "step :  250 loss :  0.03887056186795235\n",
      "step :  251 loss :  0.03884023427963257\n",
      "step :  252 loss :  0.0388340950012207\n",
      "step :  253 loss :  0.03885021433234215\n",
      "step :  254 loss :  0.038836196064949036\n",
      "step :  255 loss :  0.03882766515016556\n",
      "step :  256 loss :  0.0388462133705616\n",
      "step :  257 loss :  0.038847099989652634\n",
      "step :  258 loss :  0.03882836550474167\n",
      "step :  259 loss :  0.03882361948490143\n",
      "step :  260 loss :  0.03882835805416107\n",
      "step :  261 loss :  0.03882532939314842\n",
      "step :  262 loss :  0.03881686553359032\n",
      "step :  263 loss :  0.03882085159420967\n",
      "step :  264 loss :  0.038826100528240204\n",
      "step :  265 loss :  0.03881832957267761\n",
      "step :  266 loss :  0.03881579637527466\n",
      "step :  267 loss :  0.03880876675248146\n",
      "step :  268 loss :  0.038821037858724594\n",
      "step :  269 loss :  0.03882358595728874\n",
      "step :  270 loss :  0.038811177015304565\n",
      "step :  271 loss :  0.03881104290485382\n",
      "step :  272 loss :  0.0388130284845829\n",
      "step :  273 loss :  0.03881325200200081\n",
      "step :  274 loss :  0.03881494328379631\n",
      "step :  275 loss :  0.038803040981292725\n",
      "step :  276 loss :  0.03880254179239273\n",
      "step :  277 loss :  0.03880873695015907\n",
      "step :  278 loss :  0.03880848363041878\n",
      "step :  279 loss :  0.03880367800593376\n",
      "step :  280 loss :  0.03879503905773163\n",
      "step :  281 loss :  0.03880220279097557\n",
      "step :  282 loss :  0.038802292197942734\n",
      "step :  283 loss :  0.03879407420754433\n",
      "step :  284 loss :  0.03880137577652931\n",
      "step :  285 loss :  0.03880850225687027\n",
      "step :  286 loss :  0.0387905016541481\n",
      "step :  287 loss :  0.038780976086854935\n",
      "step :  288 loss :  0.038788650184869766\n",
      "step :  289 loss :  0.03879604861140251\n",
      "step :  290 loss :  0.03879404813051224\n",
      "step :  291 loss :  0.038783419877290726\n",
      "step :  292 loss :  0.03878175467252731\n",
      "step :  293 loss :  0.03878408670425415\n",
      "step :  294 loss :  0.038783345371484756\n",
      "step :  295 loss :  0.038782570511102676\n",
      "step :  296 loss :  0.038787174969911575\n",
      "step :  297 loss :  0.038789305835962296\n",
      "step :  298 loss :  0.03878456726670265\n",
      "step :  299 loss :  0.03878212720155716\n",
      "step :  300 loss :  0.03877692669630051\n",
      "step :  301 loss :  0.03877874091267586\n",
      "step :  302 loss :  0.03878065571188927\n",
      "step :  303 loss :  0.038781825453042984\n",
      "step :  304 loss :  0.03878379240632057\n",
      "step :  305 loss :  0.038782037794589996\n",
      "step :  306 loss :  0.038780566304922104\n",
      "step :  307 loss :  0.03878315910696983\n",
      "step :  308 loss :  0.03878381848335266\n",
      "step :  309 loss :  0.03877473995089531\n",
      "step :  310 loss :  0.03876923397183418\n",
      "step :  311 loss :  0.03877667337656021\n",
      "step :  312 loss :  0.038780033588409424\n",
      "step :  313 loss :  0.03877605125308037\n",
      "step :  314 loss :  0.038770779967308044\n",
      "step :  315 loss :  0.03877092897891998\n",
      "step :  316 loss :  0.03877415508031845\n",
      "step :  317 loss :  0.038774214684963226\n",
      "step :  318 loss :  0.03877207636833191\n",
      "step :  319 loss :  0.03877037763595581\n",
      "step :  320 loss :  0.038772549480199814\n",
      "step :  321 loss :  0.03877335786819458\n",
      "step :  322 loss :  0.03876843303442001\n",
      "step :  323 loss :  0.038767844438552856\n",
      "step :  324 loss :  0.0387694425880909\n",
      "step :  325 loss :  0.03876841813325882\n",
      "step :  326 loss :  0.03876572847366333\n",
      "step :  327 loss :  0.038763705641031265\n",
      "step :  328 loss :  0.03876833617687225\n",
      "step :  329 loss :  0.038771964609622955\n",
      "step :  330 loss :  0.03876796364784241\n",
      "step :  331 loss :  0.03876066580414772\n",
      "step :  332 loss :  0.038762252777814865\n",
      "step :  333 loss :  0.03876543417572975\n",
      "step :  334 loss :  0.03876439854502678\n",
      "step :  335 loss :  0.03875935822725296\n",
      "step :  336 loss :  0.03875964134931564\n",
      "step :  337 loss :  0.038760796189308167\n",
      "step :  338 loss :  0.038759734481573105\n",
      "step :  339 loss :  0.03875875473022461\n",
      "step :  340 loss :  0.03875900059938431\n",
      "step :  341 loss :  0.038759034126996994\n",
      "step :  342 loss :  0.03875991329550743\n",
      "step :  343 loss :  0.03875884413719177\n",
      "step :  344 loss :  0.038757890462875366\n",
      "step :  345 loss :  0.0387580431997776\n",
      "step :  346 loss :  0.03875679150223732\n",
      "step :  347 loss :  0.03875577822327614\n",
      "step :  348 loss :  0.03875929117202759\n",
      "step :  349 loss :  0.03876231238245964\n",
      "step :  350 loss :  0.03876005485653877\n",
      "step :  351 loss :  0.038755279034376144\n",
      "step :  352 loss :  0.038755152374506\n",
      "step :  353 loss :  0.038757164031267166\n",
      "step :  354 loss :  0.03875637426972389\n",
      "step :  355 loss :  0.03875373676419258\n",
      "step :  356 loss :  0.03875606879591942\n",
      "step :  357 loss :  0.03875744715332985\n",
      "step :  358 loss :  0.038755591958761215\n",
      "step :  359 loss :  0.03875400498509407\n",
      "step :  360 loss :  0.03875318542122841\n",
      "step :  361 loss :  0.03875241056084633\n",
      "step :  362 loss :  0.038752444088459015\n",
      "step :  363 loss :  0.038751620799303055\n",
      "step :  364 loss :  0.038752276450395584\n",
      "step :  365 loss :  0.038752730935811996\n",
      "step :  366 loss :  0.03875342011451721\n",
      "step :  367 loss :  0.038752783089876175\n",
      "step :  368 loss :  0.03875216469168663\n",
      "step :  369 loss :  0.0387522391974926\n",
      "step :  370 loss :  0.03875144198536873\n",
      "step :  371 loss :  0.03875083103775978\n",
      "step :  372 loss :  0.038751665502786636\n",
      "step :  373 loss :  0.03875201195478439\n",
      "step :  374 loss :  0.03875087574124336\n",
      "step :  375 loss :  0.03875058516860008\n",
      "step :  376 loss :  0.038750167936086655\n",
      "step :  377 loss :  0.03875023126602173\n",
      "step :  378 loss :  0.03875033184885979\n",
      "step :  379 loss :  0.03875003755092621\n",
      "step :  380 loss :  0.038749776780605316\n",
      "step :  381 loss :  0.0387505404651165\n",
      "step :  382 loss :  0.038750775158405304\n",
      "step :  383 loss :  0.038749221712350845\n",
      "step :  384 loss :  0.03874802961945534\n",
      "step :  385 loss :  0.038749776780605316\n",
      "step :  386 loss :  0.03875085338950157\n",
      "step :  387 loss :  0.038749393075704575\n",
      "step :  388 loss :  0.03874746337532997\n",
      "step :  389 loss :  0.03874931111931801\n",
      "step :  390 loss :  0.03875095397233963\n",
      "step :  391 loss :  0.03874994441866875\n",
      "step :  392 loss :  0.03874761238694191\n",
      "step :  393 loss :  0.038746703416109085\n",
      "step :  394 loss :  0.03874751552939415\n",
      "step :  395 loss :  0.03874777629971504\n",
      "step :  396 loss :  0.03874742239713669\n",
      "step :  397 loss :  0.03874724730849266\n",
      "step :  398 loss :  0.0387488417327404\n",
      "step :  399 loss :  0.03874978423118591\n",
      "step :  400 loss :  0.03874912112951279\n",
      "step :  401 loss :  0.038748182356357574\n",
      "step :  402 loss :  0.03874702751636505\n",
      "step :  403 loss :  0.03874696418642998\n",
      "step :  404 loss :  0.03874752297997475\n",
      "step :  405 loss :  0.038747407495975494\n",
      "step :  406 loss :  0.038747262209653854\n",
      "step :  407 loss :  0.038747385144233704\n",
      "step :  408 loss :  0.03874700516462326\n",
      "step :  409 loss :  0.03874663636088371\n",
      "step :  410 loss :  0.03874701261520386\n",
      "step :  411 loss :  0.03874749690294266\n",
      "step :  412 loss :  0.03874702379107475\n",
      "step :  413 loss :  0.038746025413274765\n",
      "step :  414 loss :  0.03874584659934044\n",
      "step :  415 loss :  0.03874597325921059\n",
      "step :  416 loss :  0.038745686411857605\n",
      "step :  417 loss :  0.038745492696762085\n",
      "step :  418 loss :  0.03874578699469566\n",
      "step :  419 loss :  0.0387461856007576\n",
      "step :  420 loss :  0.03874615579843521\n",
      "step :  421 loss :  0.03874578699469566\n",
      "step :  422 loss :  0.0387452095746994\n",
      "step :  423 loss :  0.038744810968637466\n",
      "step :  424 loss :  0.0387449786067009\n",
      "step :  425 loss :  0.038745276629924774\n",
      "step :  426 loss :  0.0387452058494091\n",
      "step :  427 loss :  0.03874515742063522\n",
      "step :  428 loss :  0.0387452132999897\n",
      "step :  429 loss :  0.03874494135379791\n",
      "step :  430 loss :  0.03874504193663597\n",
      "step :  431 loss :  0.03874516114592552\n",
      "step :  432 loss :  0.03874478116631508\n",
      "step :  433 loss :  0.038744863122701645\n",
      "step :  434 loss :  0.038744766265153885\n",
      "step :  435 loss :  0.0387442484498024\n",
      "step :  436 loss :  0.03874428570270538\n",
      "step :  437 loss :  0.038744304329156876\n",
      "step :  438 loss :  0.03874434903264046\n",
      "step :  439 loss :  0.03874470666050911\n",
      "step :  440 loss :  0.03874506801366806\n",
      "step :  441 loss :  0.03874490037560463\n",
      "step :  442 loss :  0.03874416649341583\n",
      "step :  443 loss :  0.038743652403354645\n",
      "step :  444 loss :  0.03874397277832031\n",
      "step :  445 loss :  0.03874438628554344\n",
      "step :  446 loss :  0.038744453340768814\n",
      "step :  447 loss :  0.03874431550502777\n",
      "step :  448 loss :  0.03874402865767479\n",
      "step :  449 loss :  0.0387437678873539\n",
      "step :  450 loss :  0.03874384984374046\n",
      "step :  451 loss :  0.038744039833545685\n",
      "step :  452 loss :  0.03874398022890091\n",
      "step :  453 loss :  0.03874384984374046\n",
      "step :  454 loss :  0.03874389827251434\n",
      "step :  455 loss :  0.03874402865767479\n",
      "step :  456 loss :  0.038743965327739716\n",
      "step :  457 loss :  0.038743697106838226\n",
      "step :  458 loss :  0.03874349966645241\n",
      "step :  459 loss :  0.038743600249290466\n",
      "step :  460 loss :  0.03874362260103226\n",
      "step :  461 loss :  0.03874339163303375\n",
      "step :  462 loss :  0.038743529468774796\n",
      "step :  463 loss :  0.038743603974580765\n",
      "step :  464 loss :  0.038743406534194946\n",
      "step :  465 loss :  0.03874332457780838\n",
      "step :  466 loss :  0.03874324634671211\n",
      "step :  467 loss :  0.0387432835996151\n",
      "step :  468 loss :  0.038743190467357635\n",
      "step :  469 loss :  0.038743119686841965\n",
      "step :  470 loss :  0.03874311223626137\n",
      "step :  471 loss :  0.03874331712722778\n",
      "step :  472 loss :  0.0387435220181942\n",
      "step :  473 loss :  0.038743384182453156\n",
      "step :  474 loss :  0.0387430377304554\n",
      "step :  475 loss :  0.03874296322464943\n",
      "step :  476 loss :  0.03874298185110092\n",
      "step :  477 loss :  0.03874289616942406\n",
      "step :  478 loss :  0.03874292969703674\n",
      "step :  479 loss :  0.03874306380748749\n",
      "step :  480 loss :  0.038743071258068085\n",
      "step :  481 loss :  0.03874296694993973\n",
      "step :  482 loss :  0.03874286636710167\n",
      "step :  483 loss :  0.03874293342232704\n",
      "step :  484 loss :  0.038743045181035995\n",
      "step :  485 loss :  0.03874301537871361\n",
      "step :  486 loss :  0.03874288871884346\n",
      "step :  487 loss :  0.0387427844107151\n",
      "step :  488 loss :  0.03874284401535988\n",
      "step :  489 loss :  0.03874295577406883\n",
      "step :  490 loss :  0.03874291479587555\n",
      "step :  491 loss :  0.0387427993118763\n",
      "step :  492 loss :  0.03874272480607033\n",
      "step :  493 loss :  0.038742780685424805\n",
      "step :  494 loss :  0.038742776960134506\n",
      "step :  495 loss :  0.038742609322071075\n",
      "step :  496 loss :  0.03874261677265167\n",
      "step :  497 loss :  0.038742795586586\n",
      "step :  498 loss :  0.038742806762456894\n",
      "step :  499 loss :  0.038742680102586746\n",
      "Training model 6\n",
      "step :  0 loss :  0.5435652136802673\n",
      "step :  1 loss :  0.3549843430519104\n",
      "step :  2 loss :  0.20564018189907074\n",
      "step :  3 loss :  0.23368620872497559\n",
      "step :  4 loss :  0.21082916855812073\n",
      "step :  5 loss :  0.18517223000526428\n",
      "step :  6 loss :  0.19131609797477722\n",
      "step :  7 loss :  0.1799362152814865\n",
      "step :  8 loss :  0.15416914224624634\n",
      "step :  9 loss :  0.14431414008140564\n",
      "step :  10 loss :  0.12896813452243805\n",
      "step :  11 loss :  0.13610824942588806\n",
      "step :  12 loss :  0.1404031366109848\n",
      "step :  13 loss :  0.13253901898860931\n",
      "step :  14 loss :  0.13774356245994568\n",
      "step :  15 loss :  0.15174803137779236\n",
      "step :  16 loss :  0.13586899638175964\n",
      "step :  17 loss :  0.120119109749794\n",
      "step :  18 loss :  0.11668044328689575\n",
      "step :  19 loss :  0.11689963936805725\n",
      "step :  20 loss :  0.11104703694581985\n",
      "step :  21 loss :  0.10945959389209747\n",
      "step :  22 loss :  0.10863450914621353\n",
      "step :  23 loss :  0.1080976352095604\n",
      "step :  24 loss :  0.10687541961669922\n",
      "step :  25 loss :  0.10574161261320114\n",
      "step :  26 loss :  0.10410957783460617\n",
      "step :  27 loss :  0.09924548119306564\n",
      "step :  28 loss :  0.09468782693147659\n",
      "step :  29 loss :  0.08686359226703644\n",
      "step :  30 loss :  0.07863130420446396\n",
      "step :  31 loss :  0.07199159264564514\n",
      "step :  32 loss :  0.0636226013302803\n",
      "step :  33 loss :  0.06509296596050262\n",
      "step :  34 loss :  0.05726179480552673\n",
      "step :  35 loss :  0.05185769870877266\n",
      "step :  36 loss :  0.06604736298322678\n",
      "step :  37 loss :  0.051409319043159485\n",
      "step :  38 loss :  0.04985412582755089\n",
      "step :  39 loss :  0.050791237503290176\n",
      "step :  40 loss :  0.050281181931495667\n",
      "step :  41 loss :  0.04945985972881317\n",
      "step :  42 loss :  0.04886774346232414\n",
      "step :  43 loss :  0.04875544086098671\n",
      "step :  44 loss :  0.04826386272907257\n",
      "step :  45 loss :  0.04786396399140358\n",
      "step :  46 loss :  0.04667234793305397\n",
      "step :  47 loss :  0.045961666852235794\n",
      "step :  48 loss :  0.04556690901517868\n",
      "step :  49 loss :  0.04663560912013054\n",
      "step :  50 loss :  0.045016560703516006\n",
      "step :  51 loss :  0.04498107731342316\n",
      "step :  52 loss :  0.044814955443143845\n",
      "step :  53 loss :  0.04554952681064606\n",
      "step :  54 loss :  0.05446388199925423\n",
      "step :  55 loss :  0.08119100332260132\n",
      "step :  56 loss :  0.07280533015727997\n",
      "step :  57 loss :  0.060860246419906616\n",
      "step :  58 loss :  0.04643435403704643\n",
      "step :  59 loss :  0.048161838203668594\n",
      "step :  60 loss :  0.04853060841560364\n",
      "step :  61 loss :  0.04979877918958664\n",
      "step :  62 loss :  0.04920625314116478\n",
      "step :  63 loss :  0.0488627552986145\n",
      "step :  64 loss :  0.04556449130177498\n",
      "step :  65 loss :  0.04558220133185387\n",
      "step :  66 loss :  0.04594814404845238\n",
      "step :  67 loss :  0.047714922577142715\n",
      "step :  68 loss :  0.05487677827477455\n",
      "step :  69 loss :  0.050413310527801514\n",
      "step :  70 loss :  0.054179564118385315\n",
      "step :  71 loss :  0.04757989943027496\n",
      "step :  72 loss :  0.04745792970061302\n",
      "step :  73 loss :  0.044021908193826675\n",
      "step :  74 loss :  0.04393773898482323\n",
      "step :  75 loss :  0.042036429047584534\n",
      "step :  76 loss :  0.04574207216501236\n",
      "step :  77 loss :  0.046799663454294205\n",
      "step :  78 loss :  0.04951063171029091\n",
      "step :  79 loss :  0.046142373234033585\n",
      "step :  80 loss :  0.04845474287867546\n",
      "step :  81 loss :  0.04453727975487709\n",
      "step :  82 loss :  0.04620346426963806\n",
      "step :  83 loss :  0.042305707931518555\n",
      "step :  84 loss :  0.042114343494176865\n",
      "step :  85 loss :  0.04153795912861824\n",
      "step :  86 loss :  0.04412434995174408\n",
      "step :  87 loss :  0.04212529957294464\n",
      "step :  88 loss :  0.04483560845255852\n",
      "step :  89 loss :  0.042541902512311935\n",
      "step :  90 loss :  0.044001709669828415\n",
      "step :  91 loss :  0.041250236332416534\n",
      "step :  92 loss :  0.04232200235128403\n",
      "step :  93 loss :  0.0409427210688591\n",
      "step :  94 loss :  0.04279868304729462\n",
      "step :  95 loss :  0.040684401988983154\n",
      "step :  96 loss :  0.04267187789082527\n",
      "step :  97 loss :  0.04082537069916725\n",
      "step :  98 loss :  0.04386434704065323\n",
      "step :  99 loss :  0.04099346697330475\n",
      "step :  100 loss :  0.04157513380050659\n",
      "step :  101 loss :  0.039925433695316315\n",
      "step :  102 loss :  0.04062047228217125\n",
      "step :  103 loss :  0.039492715150117874\n",
      "step :  104 loss :  0.039860956370830536\n",
      "step :  105 loss :  0.03959299251437187\n",
      "step :  106 loss :  0.03963295370340347\n",
      "step :  107 loss :  0.03966298699378967\n",
      "step :  108 loss :  0.03950957581400871\n",
      "step :  109 loss :  0.04011492803692818\n",
      "step :  110 loss :  0.039401885122060776\n",
      "step :  111 loss :  0.03964674845337868\n",
      "step :  112 loss :  0.039388179779052734\n",
      "step :  113 loss :  0.03969603031873703\n",
      "step :  114 loss :  0.03947383910417557\n",
      "step :  115 loss :  0.039735693484544754\n",
      "step :  116 loss :  0.03958660364151001\n",
      "step :  117 loss :  0.03945866972208023\n",
      "step :  118 loss :  0.03962377831339836\n",
      "step :  119 loss :  0.03952205926179886\n",
      "step :  120 loss :  0.03937821835279465\n",
      "step :  121 loss :  0.03959556296467781\n",
      "step :  122 loss :  0.03934602439403534\n",
      "step :  123 loss :  0.03972795233130455\n",
      "step :  124 loss :  0.03926588594913483\n",
      "step :  125 loss :  0.0402693897485733\n",
      "step :  126 loss :  0.03926938399672508\n",
      "step :  127 loss :  0.03944331407546997\n",
      "step :  128 loss :  0.0392826646566391\n",
      "step :  129 loss :  0.03937472403049469\n",
      "step :  130 loss :  0.03927917778491974\n",
      "step :  131 loss :  0.03927837312221527\n",
      "step :  132 loss :  0.03937305137515068\n",
      "step :  133 loss :  0.03926542401313782\n",
      "step :  134 loss :  0.039285141974687576\n",
      "step :  135 loss :  0.039270415902137756\n",
      "step :  136 loss :  0.03931565210223198\n",
      "step :  137 loss :  0.03921931982040405\n",
      "step :  138 loss :  0.03918090835213661\n",
      "step :  139 loss :  0.03934631124138832\n",
      "step :  140 loss :  0.0391571968793869\n",
      "step :  141 loss :  0.03926745057106018\n",
      "step :  142 loss :  0.03921524062752724\n",
      "step :  143 loss :  0.039192765951156616\n",
      "step :  144 loss :  0.03935488313436508\n",
      "step :  145 loss :  0.039198797196149826\n",
      "step :  146 loss :  0.03918156027793884\n",
      "step :  147 loss :  0.039194412529468536\n",
      "step :  148 loss :  0.03919648751616478\n",
      "step :  149 loss :  0.03917849436402321\n",
      "step :  150 loss :  0.0391511395573616\n",
      "step :  151 loss :  0.03912610560655594\n",
      "step :  152 loss :  0.03914526849985123\n",
      "step :  153 loss :  0.039168067276477814\n",
      "step :  154 loss :  0.039079923182725906\n",
      "step :  155 loss :  0.03916163370013237\n",
      "step :  156 loss :  0.03910278156399727\n",
      "step :  157 loss :  0.03913711756467819\n",
      "step :  158 loss :  0.03912409022450447\n",
      "step :  159 loss :  0.03911428898572922\n",
      "step :  160 loss :  0.039132725447416306\n",
      "step :  161 loss :  0.039101049304008484\n",
      "step :  162 loss :  0.0391291119158268\n",
      "step :  163 loss :  0.03911124914884567\n",
      "step :  164 loss :  0.03904400393366814\n",
      "step :  165 loss :  0.03907197341322899\n",
      "step :  166 loss :  0.03902847319841385\n",
      "step :  167 loss :  0.03898106515407562\n",
      "step :  168 loss :  0.03909611701965332\n",
      "step :  169 loss :  0.03904647007584572\n",
      "step :  170 loss :  0.03899702802300453\n",
      "step :  171 loss :  0.03904365003108978\n",
      "step :  172 loss :  0.03901182487607002\n",
      "step :  173 loss :  0.03904154524207115\n",
      "step :  174 loss :  0.03901815786957741\n",
      "step :  175 loss :  0.03901146352291107\n",
      "step :  176 loss :  0.03907456994056702\n",
      "step :  177 loss :  0.03901664540171623\n",
      "step :  178 loss :  0.03897596150636673\n",
      "step :  179 loss :  0.03901046887040138\n",
      "step :  180 loss :  0.039002515375614166\n",
      "step :  181 loss :  0.03889090567827225\n",
      "step :  182 loss :  0.038960304111242294\n",
      "step :  183 loss :  0.03905417397618294\n",
      "step :  184 loss :  0.038862161338329315\n",
      "step :  185 loss :  0.03886852413415909\n",
      "step :  186 loss :  0.039018746465444565\n",
      "step :  187 loss :  0.03895827382802963\n",
      "step :  188 loss :  0.03884495794773102\n",
      "step :  189 loss :  0.038896504789590836\n",
      "step :  190 loss :  0.03897557035088539\n",
      "step :  191 loss :  0.03886913135647774\n",
      "step :  192 loss :  0.038854677230119705\n",
      "step :  193 loss :  0.03888874500989914\n",
      "step :  194 loss :  0.03889612480998039\n",
      "step :  195 loss :  0.038863640278577805\n",
      "step :  196 loss :  0.03886198252439499\n",
      "step :  197 loss :  0.03893466293811798\n",
      "step :  198 loss :  0.03886672854423523\n",
      "step :  199 loss :  0.038808587938547134\n",
      "step :  200 loss :  0.03885495290160179\n",
      "step :  201 loss :  0.03891895338892937\n",
      "step :  202 loss :  0.03881363570690155\n",
      "step :  203 loss :  0.038818780332803726\n",
      "step :  204 loss :  0.03888939693570137\n",
      "step :  205 loss :  0.03882492333650589\n",
      "step :  206 loss :  0.03879431262612343\n",
      "step :  207 loss :  0.03884350135922432\n",
      "step :  208 loss :  0.03881656005978584\n",
      "step :  209 loss :  0.03878503665328026\n",
      "step :  210 loss :  0.038796570152044296\n",
      "step :  211 loss :  0.03882070630788803\n",
      "step :  212 loss :  0.03880301117897034\n",
      "step :  213 loss :  0.03880738839507103\n",
      "step :  214 loss :  0.03880220279097557\n",
      "step :  215 loss :  0.038793083280324936\n",
      "step :  216 loss :  0.03880982846021652\n",
      "step :  217 loss :  0.03879179432988167\n",
      "step :  218 loss :  0.038791075348854065\n",
      "step :  219 loss :  0.03879356384277344\n",
      "step :  220 loss :  0.03878534212708473\n",
      "step :  221 loss :  0.03878623992204666\n",
      "step :  222 loss :  0.03878454118967056\n",
      "step :  223 loss :  0.03876934573054314\n",
      "step :  224 loss :  0.03879440948367119\n",
      "step :  225 loss :  0.0387515127658844\n",
      "step :  226 loss :  0.03875424340367317\n",
      "step :  227 loss :  0.038757242262363434\n",
      "step :  228 loss :  0.03874542936682701\n",
      "step :  229 loss :  0.038782134652137756\n",
      "step :  230 loss :  0.03874639421701431\n",
      "step :  231 loss :  0.038714442402124405\n",
      "step :  232 loss :  0.03872630000114441\n",
      "step :  233 loss :  0.03877301514148712\n",
      "step :  234 loss :  0.038735561072826385\n",
      "step :  235 loss :  0.038692958652973175\n",
      "step :  236 loss :  0.03870761767029762\n",
      "step :  237 loss :  0.038769591599702835\n",
      "step :  238 loss :  0.038755763322114944\n",
      "step :  239 loss :  0.038691386580467224\n",
      "step :  240 loss :  0.03868788480758667\n",
      "step :  241 loss :  0.038717132061719894\n",
      "step :  242 loss :  0.03872747719287872\n",
      "step :  243 loss :  0.03871803730726242\n",
      "step :  244 loss :  0.038707610219717026\n",
      "step :  245 loss :  0.03871649503707886\n",
      "step :  246 loss :  0.03873712569475174\n",
      "step :  247 loss :  0.03871181979775429\n",
      "step :  248 loss :  0.03870896250009537\n",
      "step :  249 loss :  0.038721032440662384\n",
      "step :  250 loss :  0.0387132428586483\n",
      "step :  251 loss :  0.03869970887899399\n",
      "step :  252 loss :  0.03872166574001312\n",
      "step :  253 loss :  0.03870655223727226\n",
      "step :  254 loss :  0.03868713229894638\n",
      "step :  255 loss :  0.038693856447935104\n",
      "step :  256 loss :  0.038713667541742325\n",
      "step :  257 loss :  0.03869118168950081\n",
      "step :  258 loss :  0.038686223328113556\n",
      "step :  259 loss :  0.03870939090847969\n",
      "step :  260 loss :  0.0387028269469738\n",
      "step :  261 loss :  0.038684312254190445\n",
      "step :  262 loss :  0.038692161440849304\n",
      "step :  263 loss :  0.0387025810778141\n",
      "step :  264 loss :  0.038695771247148514\n",
      "step :  265 loss :  0.03869202733039856\n",
      "step :  266 loss :  0.038698069751262665\n",
      "step :  267 loss :  0.0386839285492897\n",
      "step :  268 loss :  0.0386878103017807\n",
      "step :  269 loss :  0.03870553523302078\n",
      "step :  270 loss :  0.03867841884493828\n",
      "step :  271 loss :  0.03867081180214882\n",
      "step :  272 loss :  0.03868554159998894\n",
      "step :  273 loss :  0.03868437558412552\n",
      "step :  274 loss :  0.038674306124448776\n",
      "step :  275 loss :  0.03866499662399292\n",
      "step :  276 loss :  0.038672659546136856\n",
      "step :  277 loss :  0.03867959603667259\n",
      "step :  278 loss :  0.03867306187748909\n",
      "step :  279 loss :  0.03866933286190033\n",
      "step :  280 loss :  0.03868255019187927\n",
      "step :  281 loss :  0.03867083787918091\n",
      "step :  282 loss :  0.03866203874349594\n",
      "step :  283 loss :  0.03866659104824066\n",
      "step :  284 loss :  0.03868034482002258\n",
      "step :  285 loss :  0.03866501525044441\n",
      "step :  286 loss :  0.03865371271967888\n",
      "step :  287 loss :  0.03865811228752136\n",
      "step :  288 loss :  0.03867831826210022\n",
      "step :  289 loss :  0.03867131471633911\n",
      "step :  290 loss :  0.03865376114845276\n",
      "step :  291 loss :  0.038653403520584106\n",
      "step :  292 loss :  0.03866687789559364\n",
      "step :  293 loss :  0.03866628557443619\n",
      "step :  294 loss :  0.038655225187540054\n",
      "step :  295 loss :  0.038656238466501236\n",
      "step :  296 loss :  0.03866564482450485\n",
      "step :  297 loss :  0.03865892440080643\n",
      "step :  298 loss :  0.0386514775454998\n",
      "step :  299 loss :  0.03865860030055046\n",
      "step :  300 loss :  0.03866281360387802\n",
      "step :  301 loss :  0.03865361958742142\n",
      "step :  302 loss :  0.03864956274628639\n",
      "step :  303 loss :  0.038658507168293\n",
      "step :  304 loss :  0.03865735977888107\n",
      "step :  305 loss :  0.03864894062280655\n",
      "step :  306 loss :  0.03865211829543114\n",
      "step :  307 loss :  0.03865614905953407\n",
      "step :  308 loss :  0.0386514775454998\n",
      "step :  309 loss :  0.03864911198616028\n",
      "step :  310 loss :  0.038654156029224396\n",
      "step :  311 loss :  0.038652073591947556\n",
      "step :  312 loss :  0.03864757716655731\n",
      "step :  313 loss :  0.0386502742767334\n",
      "step :  314 loss :  0.038649871945381165\n",
      "step :  315 loss :  0.0386478491127491\n",
      "step :  316 loss :  0.03865179046988487\n",
      "step :  317 loss :  0.038650307804346085\n",
      "step :  318 loss :  0.03865045681595802\n",
      "step :  319 loss :  0.03864829242229462\n",
      "step :  320 loss :  0.038647908717393875\n",
      "step :  321 loss :  0.03864716738462448\n",
      "step :  322 loss :  0.038648877292871475\n",
      "step :  323 loss :  0.03864479064941406\n",
      "step :  324 loss :  0.038646794855594635\n",
      "step :  325 loss :  0.0386434905230999\n",
      "step :  326 loss :  0.03864621743559837\n",
      "step :  327 loss :  0.038642965257167816\n",
      "step :  328 loss :  0.03864428028464317\n",
      "step :  329 loss :  0.038642749190330505\n",
      "step :  330 loss :  0.038642518222332\n",
      "step :  331 loss :  0.03864211589097977\n",
      "step :  332 loss :  0.03864334151148796\n",
      "step :  333 loss :  0.038640230894088745\n",
      "step :  334 loss :  0.03864191100001335\n",
      "step :  335 loss :  0.03864181414246559\n",
      "step :  336 loss :  0.03864060342311859\n",
      "step :  337 loss :  0.03864312544465065\n",
      "step :  338 loss :  0.038641173392534256\n",
      "step :  339 loss :  0.03864065185189247\n",
      "step :  340 loss :  0.03864235803484917\n",
      "step :  341 loss :  0.03863982856273651\n",
      "step :  342 loss :  0.038635462522506714\n",
      "step :  343 loss :  0.038637515157461166\n",
      "step :  344 loss :  0.038640737533569336\n",
      "step :  345 loss :  0.03863925486803055\n",
      "step :  346 loss :  0.03863474354147911\n",
      "step :  347 loss :  0.03863689303398132\n",
      "step :  348 loss :  0.03863715007901192\n",
      "step :  349 loss :  0.03863593563437462\n",
      "step :  350 loss :  0.038636233657598495\n",
      "step :  351 loss :  0.038637675344944\n",
      "step :  352 loss :  0.03863660246133804\n",
      "step :  353 loss :  0.03863734379410744\n",
      "step :  354 loss :  0.03863711655139923\n",
      "step :  355 loss :  0.03863571956753731\n",
      "step :  356 loss :  0.03863516077399254\n",
      "step :  357 loss :  0.03863668069243431\n",
      "step :  358 loss :  0.03863679617643356\n",
      "step :  359 loss :  0.038634203374385834\n",
      "step :  360 loss :  0.038635216653347015\n",
      "step :  361 loss :  0.03863349184393883\n",
      "step :  362 loss :  0.038633208721876144\n",
      "step :  363 loss :  0.038634732365608215\n",
      "step :  364 loss :  0.0386357456445694\n",
      "step :  365 loss :  0.0386342853307724\n",
      "step :  366 loss :  0.038633279502391815\n",
      "step :  367 loss :  0.03863459452986717\n",
      "step :  368 loss :  0.03863384202122688\n",
      "step :  369 loss :  0.038633596152067184\n",
      "step :  370 loss :  0.038634464144706726\n",
      "step :  371 loss :  0.0386323556303978\n",
      "step :  372 loss :  0.038632456213235855\n",
      "step :  373 loss :  0.038633257150650024\n",
      "step :  374 loss :  0.03863251581788063\n",
      "step :  375 loss :  0.03863230347633362\n",
      "step :  376 loss :  0.038631975650787354\n",
      "step :  377 loss :  0.038632094860076904\n",
      "step :  378 loss :  0.03863150626420975\n",
      "step :  379 loss :  0.03863223269581795\n",
      "step :  380 loss :  0.03863164782524109\n",
      "step :  381 loss :  0.03863095864653587\n",
      "step :  382 loss :  0.03863207623362541\n",
      "step :  383 loss :  0.03863118588924408\n",
      "step :  384 loss :  0.03863174095749855\n",
      "step :  385 loss :  0.03863079845905304\n",
      "step :  386 loss :  0.03863074257969856\n",
      "step :  387 loss :  0.03863120824098587\n",
      "step :  388 loss :  0.03863098844885826\n",
      "step :  389 loss :  0.03863000124692917\n",
      "step :  390 loss :  0.038629818707704544\n",
      "step :  391 loss :  0.0386301726102829\n",
      "step :  392 loss :  0.038630180060863495\n",
      "step :  393 loss :  0.038629814982414246\n",
      "step :  394 loss :  0.038630418479442596\n",
      "step :  395 loss :  0.03863024339079857\n",
      "step :  396 loss :  0.03862995654344559\n",
      "step :  397 loss :  0.038629475980997086\n",
      "step :  398 loss :  0.03862995281815529\n",
      "step :  399 loss :  0.038629669696092606\n",
      "step :  400 loss :  0.038628336042165756\n",
      "step :  401 loss :  0.03862908110022545\n",
      "step :  402 loss :  0.03862928971648216\n",
      "step :  403 loss :  0.03862888365983963\n",
      "step :  404 loss :  0.03862855210900307\n",
      "step :  405 loss :  0.038629330694675446\n",
      "step :  406 loss :  0.03862902522087097\n",
      "step :  407 loss :  0.03862861171364784\n",
      "step :  408 loss :  0.038628898561000824\n",
      "step :  409 loss :  0.03862851485610008\n",
      "step :  410 loss :  0.03862854838371277\n",
      "step :  411 loss :  0.03862810507416725\n",
      "step :  412 loss :  0.03862810879945755\n",
      "step :  413 loss :  0.038628436625003815\n",
      "step :  414 loss :  0.03862800449132919\n",
      "step :  415 loss :  0.038627829402685165\n",
      "step :  416 loss :  0.03862893953919411\n",
      "step :  417 loss :  0.03862854838371277\n",
      "step :  418 loss :  0.038627251982688904\n",
      "step :  419 loss :  0.03862733766436577\n",
      "step :  420 loss :  0.038628000766038895\n",
      "step :  421 loss :  0.038627587258815765\n",
      "step :  422 loss :  0.03862724453210831\n",
      "step :  423 loss :  0.03862740099430084\n",
      "step :  424 loss :  0.03862763196229935\n",
      "step :  425 loss :  0.03862746059894562\n",
      "step :  426 loss :  0.03862706944346428\n",
      "step :  427 loss :  0.03862728178501129\n",
      "step :  428 loss :  0.03862772509455681\n",
      "step :  429 loss :  0.03862708806991577\n",
      "step :  430 loss :  0.03862680867314339\n",
      "step :  431 loss :  0.03862758353352547\n",
      "step :  432 loss :  0.03862728551030159\n",
      "step :  433 loss :  0.03862648829817772\n",
      "step :  434 loss :  0.03862687200307846\n",
      "step :  435 loss :  0.038627807050943375\n",
      "step :  436 loss :  0.038627345114946365\n",
      "step :  437 loss :  0.03862627223134041\n",
      "step :  438 loss :  0.03862627223134041\n",
      "step :  439 loss :  0.03862680494785309\n",
      "step :  440 loss :  0.03862684965133667\n",
      "step :  441 loss :  0.038626350462436676\n",
      "step :  442 loss :  0.03862626105546951\n",
      "step :  443 loss :  0.038626380264759064\n",
      "step :  444 loss :  0.03862665593624115\n",
      "step :  445 loss :  0.038626451045274734\n",
      "step :  446 loss :  0.03862639516592026\n",
      "step :  447 loss :  0.03862658143043518\n",
      "step :  448 loss :  0.03862622380256653\n",
      "step :  449 loss :  0.0386262983083725\n",
      "step :  450 loss :  0.0386265330016613\n",
      "step :  451 loss :  0.038626134395599365\n",
      "step :  452 loss :  0.038626156747341156\n",
      "step :  453 loss :  0.038626085966825485\n",
      "step :  454 loss :  0.038625940680503845\n",
      "step :  455 loss :  0.038626138120889664\n",
      "step :  456 loss :  0.03862626850605011\n",
      "step :  457 loss :  0.038626138120889664\n",
      "step :  458 loss :  0.038626112043857574\n",
      "step :  459 loss :  0.03862607106566429\n",
      "step :  460 loss :  0.0386260561645031\n",
      "step :  461 loss :  0.038625795394182205\n",
      "step :  462 loss :  0.038625989109277725\n",
      "step :  463 loss :  0.038625866174697876\n",
      "step :  464 loss :  0.038625895977020264\n",
      "step :  465 loss :  0.038625817745923996\n",
      "step :  466 loss :  0.038625992834568024\n",
      "step :  467 loss :  0.03862575814127922\n",
      "step :  468 loss :  0.03862546384334564\n",
      "step :  469 loss :  0.03862558305263519\n",
      "step :  470 loss :  0.03862592205405235\n",
      "step :  471 loss :  0.03862570598721504\n",
      "step :  472 loss :  0.038625381886959076\n",
      "step :  473 loss :  0.038625434041023254\n",
      "step :  474 loss :  0.03862547501921654\n",
      "step :  475 loss :  0.038625434041023254\n",
      "step :  476 loss :  0.03862548619508743\n",
      "step :  477 loss :  0.03862553834915161\n",
      "step :  478 loss :  0.03862549364566803\n",
      "step :  479 loss :  0.038625411689281464\n",
      "step :  480 loss :  0.038625527173280716\n",
      "step :  481 loss :  0.038625411689281464\n",
      "step :  482 loss :  0.038625482469797134\n",
      "step :  483 loss :  0.03862542659044266\n",
      "step :  484 loss :  0.03862541541457176\n",
      "step :  485 loss :  0.03862537816166878\n",
      "step :  486 loss :  0.03862539678812027\n",
      "step :  487 loss :  0.03862527012825012\n",
      "step :  488 loss :  0.038625314831733704\n",
      "step :  489 loss :  0.03862529620528221\n",
      "step :  490 loss :  0.03862537071108818\n",
      "step :  491 loss :  0.03862527012825012\n",
      "step :  492 loss :  0.038625214248895645\n",
      "step :  493 loss :  0.038625363260507584\n",
      "step :  494 loss :  0.038625191897153854\n",
      "step :  495 loss :  0.03862496092915535\n",
      "step :  496 loss :  0.03862509876489639\n",
      "step :  497 loss :  0.03862534090876579\n",
      "step :  498 loss :  0.03862523287534714\n",
      "step :  499 loss :  0.03862496092915535\n",
      "Training model 7\n",
      "step :  0 loss :  0.4504822790622711\n",
      "step :  1 loss :  0.28971365094184875\n",
      "step :  2 loss :  0.1910349279642105\n",
      "step :  3 loss :  0.24136821925640106\n",
      "step :  4 loss :  0.1813729852437973\n",
      "step :  5 loss :  0.18209640681743622\n",
      "step :  6 loss :  0.1829419881105423\n",
      "step :  7 loss :  0.15731151401996613\n",
      "step :  8 loss :  0.13740274310112\n",
      "step :  9 loss :  0.13279898464679718\n",
      "step :  10 loss :  0.13304652273654938\n",
      "step :  11 loss :  0.1482502669095993\n",
      "step :  12 loss :  0.13497129082679749\n",
      "step :  13 loss :  0.13144159317016602\n",
      "step :  14 loss :  0.13722552359104156\n",
      "step :  15 loss :  0.13525620102882385\n",
      "step :  16 loss :  0.12247467786073685\n",
      "step :  17 loss :  0.11469969153404236\n",
      "step :  18 loss :  0.11087772995233536\n",
      "step :  19 loss :  0.10945142060518265\n",
      "step :  20 loss :  0.10673649609088898\n",
      "step :  21 loss :  0.10554192960262299\n",
      "step :  22 loss :  0.10668881237506866\n",
      "step :  23 loss :  0.10442505031824112\n",
      "step :  24 loss :  0.10315986722707748\n",
      "step :  25 loss :  0.1031147763133049\n",
      "step :  26 loss :  0.09849217534065247\n",
      "step :  27 loss :  0.09028919041156769\n",
      "step :  28 loss :  0.08748720586299896\n",
      "step :  29 loss :  0.08565026521682739\n",
      "step :  30 loss :  0.07793128490447998\n",
      "step :  31 loss :  0.07566749304533005\n",
      "step :  32 loss :  0.06545832753181458\n",
      "step :  33 loss :  0.06334798783063889\n",
      "step :  34 loss :  0.05792805552482605\n",
      "step :  35 loss :  0.056943755596876144\n",
      "step :  36 loss :  0.06095064431428909\n",
      "step :  37 loss :  0.055571265518665314\n",
      "step :  38 loss :  0.05433397740125656\n",
      "step :  39 loss :  0.05826804041862488\n",
      "step :  40 loss :  0.06270809471607208\n",
      "step :  41 loss :  0.07596035301685333\n",
      "step :  42 loss :  0.0859813466668129\n",
      "step :  43 loss :  0.06873996555805206\n",
      "step :  44 loss :  0.05414493754506111\n",
      "step :  45 loss :  0.06288734823465347\n",
      "step :  46 loss :  0.06984065473079681\n",
      "step :  47 loss :  0.05429556593298912\n",
      "step :  48 loss :  0.051156722009181976\n",
      "step :  49 loss :  0.055844757705926895\n",
      "step :  50 loss :  0.05053993687033653\n",
      "step :  51 loss :  0.050112903118133545\n",
      "step :  52 loss :  0.048290327191352844\n",
      "step :  53 loss :  0.047805413603782654\n",
      "step :  54 loss :  0.04663249850273132\n",
      "step :  55 loss :  0.0459592267870903\n",
      "step :  56 loss :  0.04552111402153969\n",
      "step :  57 loss :  0.04505744203925133\n",
      "step :  58 loss :  0.04451905936002731\n",
      "step :  59 loss :  0.044389158487319946\n",
      "step :  60 loss :  0.04386236146092415\n",
      "step :  61 loss :  0.043011803179979324\n",
      "step :  62 loss :  0.042496707290410995\n",
      "step :  63 loss :  0.042540520429611206\n",
      "step :  64 loss :  0.04236697405576706\n",
      "step :  65 loss :  0.04181528836488724\n",
      "step :  66 loss :  0.04256671294569969\n",
      "step :  67 loss :  0.04083951190114021\n",
      "step :  68 loss :  0.04284241423010826\n",
      "step :  69 loss :  0.041124723851680756\n",
      "step :  70 loss :  0.04362277314066887\n",
      "step :  71 loss :  0.04342622309923172\n",
      "step :  72 loss :  0.04045375809073448\n",
      "step :  73 loss :  0.04184754565358162\n",
      "step :  74 loss :  0.04034099355340004\n",
      "step :  75 loss :  0.04020925611257553\n",
      "step :  76 loss :  0.04015189781785011\n",
      "step :  77 loss :  0.0402967631816864\n",
      "step :  78 loss :  0.04280920699238777\n",
      "step :  79 loss :  0.03998623415827751\n",
      "step :  80 loss :  0.04080628603696823\n",
      "step :  81 loss :  0.04055965319275856\n",
      "step :  82 loss :  0.03986328840255737\n",
      "step :  83 loss :  0.041314497590065\n",
      "step :  84 loss :  0.0395255908370018\n",
      "step :  85 loss :  0.03934619948267937\n",
      "step :  86 loss :  0.0407709926366806\n",
      "step :  87 loss :  0.039490655064582825\n",
      "step :  88 loss :  0.04151042178273201\n",
      "step :  89 loss :  0.03935099393129349\n",
      "step :  90 loss :  0.03996282070875168\n",
      "step :  91 loss :  0.03944069519639015\n",
      "step :  92 loss :  0.03982673957943916\n",
      "step :  93 loss :  0.03915562853217125\n",
      "step :  94 loss :  0.044993095099925995\n",
      "step :  95 loss :  0.042778030037879944\n",
      "step :  96 loss :  0.055735208094120026\n",
      "step :  97 loss :  0.04566733539104462\n",
      "step :  98 loss :  0.042045317590236664\n",
      "step :  99 loss :  0.040056679397821426\n",
      "step :  100 loss :  0.0396847203373909\n",
      "step :  101 loss :  0.044735029339790344\n",
      "step :  102 loss :  0.04014291986823082\n",
      "step :  103 loss :  0.04419427737593651\n",
      "step :  104 loss :  0.040051378309726715\n",
      "step :  105 loss :  0.04319099709391594\n",
      "step :  106 loss :  0.039395563304424286\n",
      "step :  107 loss :  0.04133239760994911\n",
      "step :  108 loss :  0.03894658386707306\n",
      "step :  109 loss :  0.039984386414289474\n",
      "step :  110 loss :  0.03921666368842125\n",
      "step :  111 loss :  0.03908573463559151\n",
      "step :  112 loss :  0.04006271809339523\n",
      "step :  113 loss :  0.03884375840425491\n",
      "step :  114 loss :  0.04059510678052902\n",
      "step :  115 loss :  0.03914909437298775\n",
      "step :  116 loss :  0.04188723862171173\n",
      "step :  117 loss :  0.03894289582967758\n",
      "step :  118 loss :  0.04026278480887413\n",
      "step :  119 loss :  0.03888832405209541\n",
      "step :  120 loss :  0.03919730708003044\n",
      "step :  121 loss :  0.03901860490441322\n",
      "step :  122 loss :  0.03904051333665848\n",
      "step :  123 loss :  0.03896490857005119\n",
      "step :  124 loss :  0.0390157550573349\n",
      "step :  125 loss :  0.03900361433625221\n",
      "step :  126 loss :  0.03869470953941345\n",
      "step :  127 loss :  0.03971036896109581\n",
      "step :  128 loss :  0.03868309035897255\n",
      "step :  129 loss :  0.038896311074495316\n",
      "step :  130 loss :  0.03903732821345329\n",
      "step :  131 loss :  0.0385160930454731\n",
      "step :  132 loss :  0.03941400721669197\n",
      "step :  133 loss :  0.03849663957953453\n",
      "step :  134 loss :  0.03928394615650177\n",
      "step :  135 loss :  0.038502372801303864\n",
      "step :  136 loss :  0.03946322202682495\n",
      "step :  137 loss :  0.03851171210408211\n",
      "step :  138 loss :  0.03868972510099411\n",
      "step :  139 loss :  0.03907337784767151\n",
      "step :  140 loss :  0.03846375271677971\n",
      "step :  141 loss :  0.039334915578365326\n",
      "step :  142 loss :  0.038468137383461\n",
      "step :  143 loss :  0.03880473971366882\n",
      "step :  144 loss :  0.03870098292827606\n",
      "step :  145 loss :  0.03868301585316658\n",
      "step :  146 loss :  0.03877871483564377\n",
      "step :  147 loss :  0.03863275796175003\n",
      "step :  148 loss :  0.03898225352168083\n",
      "step :  149 loss :  0.03851791098713875\n",
      "step :  150 loss :  0.03878450021147728\n",
      "step :  151 loss :  0.03872841224074364\n",
      "step :  152 loss :  0.03848408907651901\n",
      "step :  153 loss :  0.03906495124101639\n",
      "step :  154 loss :  0.03842511773109436\n",
      "step :  155 loss :  0.03872217610478401\n",
      "step :  156 loss :  0.038644496351480484\n",
      "step :  157 loss :  0.03846140205860138\n",
      "step :  158 loss :  0.03885261341929436\n",
      "step :  159 loss :  0.03850123658776283\n",
      "step :  160 loss :  0.0385478138923645\n",
      "step :  161 loss :  0.038675885647535324\n",
      "step :  162 loss :  0.038443081080913544\n",
      "step :  163 loss :  0.03866899386048317\n",
      "step :  164 loss :  0.038532380014657974\n",
      "step :  165 loss :  0.03869464993476868\n",
      "step :  166 loss :  0.03859962522983551\n",
      "step :  167 loss :  0.03857193514704704\n",
      "step :  168 loss :  0.03860228881239891\n",
      "step :  169 loss :  0.03857439383864403\n",
      "step :  170 loss :  0.03854905068874359\n",
      "step :  171 loss :  0.03851623460650444\n",
      "step :  172 loss :  0.0385454036295414\n",
      "step :  173 loss :  0.03850070387125015\n",
      "step :  174 loss :  0.03844910115003586\n",
      "step :  175 loss :  0.03852340951561928\n",
      "step :  176 loss :  0.03843406215310097\n",
      "step :  177 loss :  0.03857044503092766\n",
      "step :  178 loss :  0.03839554265141487\n",
      "step :  179 loss :  0.03858279436826706\n",
      "step :  180 loss :  0.038437001407146454\n",
      "step :  181 loss :  0.03852006793022156\n",
      "step :  182 loss :  0.03848004713654518\n",
      "step :  183 loss :  0.03847116231918335\n",
      "step :  184 loss :  0.03843240067362785\n",
      "step :  185 loss :  0.03840634226799011\n",
      "step :  186 loss :  0.038457293063402176\n",
      "step :  187 loss :  0.03835059329867363\n",
      "step :  188 loss :  0.03855462744832039\n",
      "step :  189 loss :  0.0383417047560215\n",
      "step :  190 loss :  0.03839259222149849\n",
      "step :  191 loss :  0.03851616010069847\n",
      "step :  192 loss :  0.038283493369817734\n",
      "step :  193 loss :  0.03837330639362335\n",
      "step :  194 loss :  0.03852923959493637\n",
      "step :  195 loss :  0.03830684721469879\n",
      "step :  196 loss :  0.038356099277734756\n",
      "step :  197 loss :  0.03848618268966675\n",
      "step :  198 loss :  0.038310859352350235\n",
      "step :  199 loss :  0.038337256759405136\n",
      "step :  200 loss :  0.03847144916653633\n",
      "step :  201 loss :  0.03831258788704872\n",
      "step :  202 loss :  0.03836434707045555\n",
      "step :  203 loss :  0.038382627069950104\n",
      "step :  204 loss :  0.038315482437610626\n",
      "step :  205 loss :  0.03838514909148216\n",
      "step :  206 loss :  0.03830182924866676\n",
      "step :  207 loss :  0.03836091607809067\n",
      "step :  208 loss :  0.03834479674696922\n",
      "step :  209 loss :  0.03831322491168976\n",
      "step :  210 loss :  0.03832598030567169\n",
      "step :  211 loss :  0.038345254957675934\n",
      "step :  212 loss :  0.03834033012390137\n",
      "step :  213 loss :  0.038350965827703476\n",
      "step :  214 loss :  0.03833139315247536\n",
      "step :  215 loss :  0.03831018507480621\n",
      "step :  216 loss :  0.03835691511631012\n",
      "step :  217 loss :  0.03831224888563156\n",
      "step :  218 loss :  0.03830597922205925\n",
      "step :  219 loss :  0.038371678441762924\n",
      "step :  220 loss :  0.03827103599905968\n",
      "step :  221 loss :  0.03830399364233017\n",
      "step :  222 loss :  0.03837979957461357\n",
      "step :  223 loss :  0.03826986253261566\n",
      "step :  224 loss :  0.038290854543447495\n",
      "step :  225 loss :  0.03837081789970398\n",
      "step :  226 loss :  0.038279104977846146\n",
      "step :  227 loss :  0.03826017305254936\n",
      "step :  228 loss :  0.03830650448799133\n",
      "step :  229 loss :  0.03832163289189339\n",
      "step :  230 loss :  0.03827234357595444\n",
      "step :  231 loss :  0.038271330296993256\n",
      "step :  232 loss :  0.03834617882966995\n",
      "step :  233 loss :  0.03827652707695961\n",
      "step :  234 loss :  0.03826572373509407\n",
      "step :  235 loss :  0.03830089792609215\n",
      "step :  236 loss :  0.03829943761229515\n",
      "step :  237 loss :  0.03827686980366707\n",
      "step :  238 loss :  0.03826969116926193\n",
      "step :  239 loss :  0.03828578069806099\n",
      "step :  240 loss :  0.038286108523607254\n",
      "step :  241 loss :  0.038286272436380386\n",
      "step :  242 loss :  0.03829529508948326\n",
      "step :  243 loss :  0.038293808698654175\n",
      "step :  244 loss :  0.03828921169042587\n",
      "step :  245 loss :  0.03827902302145958\n",
      "step :  246 loss :  0.03827491030097008\n",
      "step :  247 loss :  0.038277674466371536\n",
      "step :  248 loss :  0.038271207362413406\n",
      "step :  249 loss :  0.038282934576272964\n",
      "step :  250 loss :  0.03827117010951042\n",
      "step :  251 loss :  0.038279056549072266\n",
      "step :  252 loss :  0.03826985880732536\n",
      "step :  253 loss :  0.038287483155727386\n",
      "step :  254 loss :  0.03827366605401039\n",
      "step :  255 loss :  0.038250528275966644\n",
      "step :  256 loss :  0.03826042637228966\n",
      "step :  257 loss :  0.038269709795713425\n",
      "step :  258 loss :  0.03826513886451721\n",
      "step :  259 loss :  0.03826781362295151\n",
      "step :  260 loss :  0.03825155645608902\n",
      "step :  261 loss :  0.03824878856539726\n",
      "step :  262 loss :  0.03828870877623558\n",
      "step :  263 loss :  0.038276880979537964\n",
      "step :  264 loss :  0.038237422704696655\n",
      "step :  265 loss :  0.03823351487517357\n",
      "step :  266 loss :  0.038262881338596344\n",
      "step :  267 loss :  0.03827107697725296\n",
      "step :  268 loss :  0.03824552148580551\n",
      "step :  269 loss :  0.03824133425951004\n",
      "step :  270 loss :  0.03826068714261055\n",
      "step :  271 loss :  0.03825556859374046\n",
      "step :  272 loss :  0.038241490721702576\n",
      "step :  273 loss :  0.03823849931359291\n",
      "step :  274 loss :  0.038252342492341995\n",
      "step :  275 loss :  0.03826027736067772\n",
      "step :  276 loss :  0.038240715861320496\n",
      "step :  277 loss :  0.03824099153280258\n",
      "step :  278 loss :  0.03826180472970009\n",
      "step :  279 loss :  0.038253750652074814\n",
      "step :  280 loss :  0.038237541913986206\n",
      "step :  281 loss :  0.03823312744498253\n",
      "step :  282 loss :  0.03824653476476669\n",
      "step :  283 loss :  0.038250479847192764\n",
      "step :  284 loss :  0.038238734006881714\n",
      "step :  285 loss :  0.038232505321502686\n",
      "step :  286 loss :  0.03824552521109581\n",
      "step :  287 loss :  0.03824474290013313\n",
      "step :  288 loss :  0.03824445232748985\n",
      "step :  289 loss :  0.03824668005108833\n",
      "step :  290 loss :  0.03824068233370781\n",
      "step :  291 loss :  0.0382423922419548\n",
      "step :  292 loss :  0.03825397044420242\n",
      "step :  293 loss :  0.03824169188737869\n",
      "step :  294 loss :  0.03823770210146904\n",
      "step :  295 loss :  0.03824787959456444\n",
      "step :  296 loss :  0.03824583441019058\n",
      "step :  297 loss :  0.038234610110521317\n",
      "step :  298 loss :  0.03823424130678177\n",
      "step :  299 loss :  0.038242582231760025\n",
      "step :  300 loss :  0.03824138641357422\n",
      "step :  301 loss :  0.03823567181825638\n",
      "step :  302 loss :  0.03824123367667198\n",
      "step :  303 loss :  0.038248226046562195\n",
      "step :  304 loss :  0.038233548402786255\n",
      "step :  305 loss :  0.03822670131921768\n",
      "step :  306 loss :  0.038236960768699646\n",
      "step :  307 loss :  0.038237810134887695\n",
      "step :  308 loss :  0.03823260962963104\n",
      "step :  309 loss :  0.03823113068938255\n",
      "step :  310 loss :  0.03823584318161011\n",
      "step :  311 loss :  0.03823583945631981\n",
      "step :  312 loss :  0.03823362663388252\n",
      "step :  313 loss :  0.03823193535208702\n",
      "step :  314 loss :  0.03823317214846611\n",
      "step :  315 loss :  0.038236718624830246\n",
      "step :  316 loss :  0.038231562823057175\n",
      "step :  317 loss :  0.03823014348745346\n",
      "step :  318 loss :  0.038234055042266846\n",
      "step :  319 loss :  0.03823317587375641\n",
      "step :  320 loss :  0.038230959326028824\n",
      "step :  321 loss :  0.03823164850473404\n",
      "step :  322 loss :  0.0382327139377594\n",
      "step :  323 loss :  0.03823033720254898\n",
      "step :  324 loss :  0.038232143968343735\n",
      "step :  325 loss :  0.03823498263955116\n",
      "step :  326 loss :  0.03822914883494377\n",
      "step :  327 loss :  0.038226839154958725\n",
      "step :  328 loss :  0.038230787962675095\n",
      "step :  329 loss :  0.038231171667575836\n",
      "step :  330 loss :  0.03822868689894676\n",
      "step :  331 loss :  0.03822639212012291\n",
      "step :  332 loss :  0.038228753954172134\n",
      "step :  333 loss :  0.03823169693350792\n",
      "step :  334 loss :  0.038227107375860214\n",
      "step :  335 loss :  0.03822595253586769\n",
      "step :  336 loss :  0.038229379802942276\n",
      "step :  337 loss :  0.03822846710681915\n",
      "step :  338 loss :  0.03822674602270126\n",
      "step :  339 loss :  0.038226597011089325\n",
      "step :  340 loss :  0.03822741657495499\n",
      "step :  341 loss :  0.038225602358579636\n",
      "step :  342 loss :  0.03822685405611992\n",
      "step :  343 loss :  0.03822768107056618\n",
      "step :  344 loss :  0.03822609409689903\n",
      "step :  345 loss :  0.03822622448205948\n",
      "step :  346 loss :  0.03822966292500496\n",
      "step :  347 loss :  0.03822827711701393\n",
      "step :  348 loss :  0.038225237280130386\n",
      "step :  349 loss :  0.03822707012295723\n",
      "step :  350 loss :  0.0382293201982975\n",
      "step :  351 loss :  0.038226451724767685\n",
      "step :  352 loss :  0.03822421282529831\n",
      "step :  353 loss :  0.03822587430477142\n",
      "step :  354 loss :  0.03822537511587143\n",
      "step :  355 loss :  0.03822217881679535\n",
      "step :  356 loss :  0.038223449140787125\n",
      "step :  357 loss :  0.038227129727602005\n",
      "step :  358 loss :  0.03822629153728485\n",
      "step :  359 loss :  0.038221634924411774\n",
      "step :  360 loss :  0.03822122886776924\n",
      "step :  361 loss :  0.03822669759392738\n",
      "step :  362 loss :  0.038227930665016174\n",
      "step :  363 loss :  0.038224902004003525\n",
      "step :  364 loss :  0.03822283446788788\n",
      "step :  365 loss :  0.03822445496916771\n",
      "step :  366 loss :  0.038225188851356506\n",
      "step :  367 loss :  0.0382237546145916\n",
      "step :  368 loss :  0.03822319209575653\n",
      "step :  369 loss :  0.038225170224905014\n",
      "step :  370 loss :  0.03822525218129158\n",
      "step :  371 loss :  0.038223106414079666\n",
      "step :  372 loss :  0.0382210947573185\n",
      "step :  373 loss :  0.038223691284656525\n",
      "step :  374 loss :  0.03822636604309082\n",
      "step :  375 loss :  0.0382252037525177\n",
      "step :  376 loss :  0.038221318274736404\n",
      "step :  377 loss :  0.03821950405836105\n",
      "step :  378 loss :  0.03822313994169235\n",
      "step :  379 loss :  0.03822517395019531\n",
      "step :  380 loss :  0.038223810493946075\n",
      "step :  381 loss :  0.03822125867009163\n",
      "step :  382 loss :  0.038220833986997604\n",
      "step :  383 loss :  0.03822280466556549\n",
      "step :  384 loss :  0.03822418674826622\n",
      "step :  385 loss :  0.03822300210595131\n",
      "step :  386 loss :  0.038220543414354324\n",
      "step :  387 loss :  0.038219962269067764\n",
      "step :  388 loss :  0.03822217509150505\n",
      "step :  389 loss :  0.03822265565395355\n",
      "step :  390 loss :  0.03822178766131401\n",
      "step :  391 loss :  0.03822118788957596\n",
      "step :  392 loss :  0.03822221979498863\n",
      "step :  393 loss :  0.03822311386466026\n",
      "step :  394 loss :  0.03822213411331177\n",
      "step :  395 loss :  0.03822091594338417\n",
      "step :  396 loss :  0.03822147101163864\n",
      "step :  397 loss :  0.03822219744324684\n",
      "step :  398 loss :  0.03822173923254013\n",
      "step :  399 loss :  0.03822043538093567\n",
      "step :  400 loss :  0.038220275193452835\n",
      "step :  401 loss :  0.038221318274736404\n",
      "step :  402 loss :  0.038221366703510284\n",
      "step :  403 loss :  0.03822104260325432\n",
      "step :  404 loss :  0.0382208377122879\n",
      "step :  405 loss :  0.03822068125009537\n",
      "step :  406 loss :  0.03822125121951103\n",
      "step :  407 loss :  0.03822092339396477\n",
      "step :  408 loss :  0.03822039067745209\n",
      "step :  409 loss :  0.03822046145796776\n",
      "step :  410 loss :  0.0382206104695797\n",
      "step :  411 loss :  0.03822013735771179\n",
      "step :  412 loss :  0.03822039067745209\n",
      "step :  413 loss :  0.03822138532996178\n",
      "step :  414 loss :  0.03822096809744835\n",
      "step :  415 loss :  0.0382198803126812\n",
      "step :  416 loss :  0.03821973130106926\n",
      "step :  417 loss :  0.03822087496519089\n",
      "step :  418 loss :  0.0382208451628685\n",
      "step :  419 loss :  0.038220129907131195\n",
      "step :  420 loss :  0.038220178335905075\n",
      "step :  421 loss :  0.038220930844545364\n",
      "step :  422 loss :  0.038220372051000595\n",
      "step :  423 loss :  0.03821953758597374\n",
      "step :  424 loss :  0.038220103830099106\n",
      "step :  425 loss :  0.03822128102183342\n",
      "step :  426 loss :  0.038221199065446854\n",
      "step :  427 loss :  0.03821996971964836\n",
      "step :  428 loss :  0.03821945935487747\n",
      "step :  429 loss :  0.03822030499577522\n",
      "step :  430 loss :  0.038220472633838654\n",
      "step :  431 loss :  0.03821989521384239\n",
      "step :  432 loss :  0.03821934014558792\n",
      "step :  433 loss :  0.03821994736790657\n",
      "step :  434 loss :  0.03822097182273865\n",
      "step :  435 loss :  0.03822087123990059\n",
      "step :  436 loss :  0.038219839334487915\n",
      "step :  437 loss :  0.038219012320041656\n",
      "step :  438 loss :  0.03821936622262001\n",
      "step :  439 loss :  0.03821974992752075\n",
      "step :  440 loss :  0.03821966052055359\n",
      "step :  441 loss :  0.03821919485926628\n",
      "step :  442 loss :  0.03821900859475136\n",
      "step :  443 loss :  0.038219574838876724\n",
      "step :  444 loss :  0.03822018578648567\n",
      "step :  445 loss :  0.03821995481848717\n",
      "step :  446 loss :  0.03821926936507225\n",
      "step :  447 loss :  0.038219157606363297\n",
      "step :  448 loss :  0.03821950778365135\n",
      "step :  449 loss :  0.0382193997502327\n",
      "step :  450 loss :  0.03821916505694389\n",
      "step :  451 loss :  0.03821924701333046\n",
      "step :  452 loss :  0.0382193848490715\n",
      "step :  453 loss :  0.0382191464304924\n",
      "step :  454 loss :  0.03821912780404091\n",
      "step :  455 loss :  0.03821924328804016\n",
      "step :  456 loss :  0.038219138979911804\n",
      "step :  457 loss :  0.038219254463911057\n",
      "step :  458 loss :  0.03821959346532822\n",
      "step :  459 loss :  0.03821951150894165\n",
      "step :  460 loss :  0.038219187408685684\n",
      "step :  461 loss :  0.03821900114417076\n",
      "step :  462 loss :  0.03821906819939613\n",
      "step :  463 loss :  0.038219064474105835\n",
      "step :  464 loss :  0.03821905329823494\n",
      "step :  465 loss :  0.038218941539525986\n",
      "step :  466 loss :  0.03821892663836479\n",
      "step :  467 loss :  0.03821906819939613\n",
      "step :  468 loss :  0.038219258189201355\n",
      "step :  469 loss :  0.03821912035346031\n",
      "step :  470 loss :  0.038218915462493896\n",
      "step :  471 loss :  0.03821907937526703\n",
      "step :  472 loss :  0.03821946308016777\n",
      "step :  473 loss :  0.038219355046749115\n",
      "step :  474 loss :  0.03821912407875061\n",
      "step :  475 loss :  0.03821906819939613\n",
      "step :  476 loss :  0.03821903094649315\n",
      "step :  477 loss :  0.03821881115436554\n",
      "step :  478 loss :  0.03821878880262375\n",
      "step :  479 loss :  0.038218844681978226\n",
      "step :  480 loss :  0.038218844681978226\n",
      "step :  481 loss :  0.038218915462493896\n",
      "step :  482 loss :  0.03821912035346031\n",
      "step :  483 loss :  0.03821910172700882\n",
      "step :  484 loss :  0.0382189005613327\n",
      "step :  485 loss :  0.03821868076920509\n",
      "step :  486 loss :  0.038218870759010315\n",
      "step :  487 loss :  0.03821922466158867\n",
      "step :  488 loss :  0.03821917995810509\n",
      "step :  489 loss :  0.038218941539525986\n",
      "step :  490 loss :  0.03821871057152748\n",
      "step :  491 loss :  0.038218751549720764\n",
      "step :  492 loss :  0.03821887820959091\n",
      "step :  493 loss :  0.03821883350610733\n",
      "step :  494 loss :  0.038218677043914795\n",
      "step :  495 loss :  0.03821870684623718\n",
      "step :  496 loss :  0.03821887820959091\n",
      "step :  497 loss :  0.038218867033720016\n",
      "step :  498 loss :  0.03821879252791405\n",
      "step :  499 loss :  0.03821874409914017\n",
      "Training model 8\n",
      "step :  0 loss :  0.595242977142334\n",
      "step :  1 loss :  0.4397505223751068\n",
      "step :  2 loss :  0.24729116261005402\n",
      "step :  3 loss :  0.21125242114067078\n",
      "step :  4 loss :  0.23274515569210052\n",
      "step :  5 loss :  0.17984452843666077\n",
      "step :  6 loss :  0.18805401027202606\n",
      "step :  7 loss :  0.18764512240886688\n",
      "step :  8 loss :  0.16156315803527832\n",
      "step :  9 loss :  0.1425430327653885\n",
      "step :  10 loss :  0.13937193155288696\n",
      "step :  11 loss :  0.1259167641401291\n",
      "step :  12 loss :  0.1420210599899292\n",
      "step :  13 loss :  0.14001968502998352\n",
      "step :  14 loss :  0.13629841804504395\n",
      "step :  15 loss :  0.1404445916414261\n",
      "step :  16 loss :  0.13628268241882324\n",
      "step :  17 loss :  0.12491682916879654\n",
      "step :  18 loss :  0.11660058796405792\n",
      "step :  19 loss :  0.11324519664049149\n",
      "step :  20 loss :  0.11099628359079361\n",
      "step :  21 loss :  0.10751354694366455\n",
      "step :  22 loss :  0.10535021871328354\n",
      "step :  23 loss :  0.10398872196674347\n",
      "step :  24 loss :  0.09899719059467316\n",
      "step :  25 loss :  0.09597646445035934\n",
      "step :  26 loss :  0.09171105176210403\n",
      "step :  27 loss :  0.0873769149184227\n",
      "step :  28 loss :  0.08013135939836502\n",
      "step :  29 loss :  0.07492358982563019\n",
      "step :  30 loss :  0.06808202713727951\n",
      "step :  31 loss :  0.05981667339801788\n",
      "step :  32 loss :  0.05795011296868324\n",
      "step :  33 loss :  0.05342443287372589\n",
      "step :  34 loss :  0.06067388504743576\n",
      "step :  35 loss :  0.054669465869665146\n",
      "step :  36 loss :  0.052335236221551895\n",
      "step :  37 loss :  0.053061697632074356\n",
      "step :  38 loss :  0.050954192876815796\n",
      "step :  39 loss :  0.050910912454128265\n",
      "step :  40 loss :  0.05545860156416893\n",
      "step :  41 loss :  0.05018066242337227\n",
      "step :  42 loss :  0.050026051700115204\n",
      "step :  43 loss :  0.05013011768460274\n",
      "step :  44 loss :  0.05002058297395706\n",
      "step :  45 loss :  0.04945460334420204\n",
      "step :  46 loss :  0.04781322181224823\n",
      "step :  47 loss :  0.04798862338066101\n",
      "step :  48 loss :  0.047391049563884735\n",
      "step :  49 loss :  0.04789552465081215\n",
      "step :  50 loss :  0.04622741416096687\n",
      "step :  51 loss :  0.04747767746448517\n",
      "step :  52 loss :  0.047772061079740524\n",
      "step :  53 loss :  0.0456831119954586\n",
      "step :  54 loss :  0.044989213347435\n",
      "step :  55 loss :  0.04485161602497101\n",
      "step :  56 loss :  0.044877879321575165\n",
      "step :  57 loss :  0.04549647867679596\n",
      "step :  58 loss :  0.04436006769537926\n",
      "step :  59 loss :  0.04541303589940071\n",
      "step :  60 loss :  0.04412711039185524\n",
      "step :  61 loss :  0.04298614338040352\n",
      "step :  62 loss :  0.04334641620516777\n",
      "step :  63 loss :  0.04497953876852989\n",
      "step :  64 loss :  0.042204298079013824\n",
      "step :  65 loss :  0.042167097330093384\n",
      "step :  66 loss :  0.04277752339839935\n",
      "step :  67 loss :  0.04388619214296341\n",
      "step :  68 loss :  0.041137129068374634\n",
      "step :  69 loss :  0.04083341360092163\n",
      "step :  70 loss :  0.042677853256464005\n",
      "step :  71 loss :  0.04234803467988968\n",
      "step :  72 loss :  0.04007770121097565\n",
      "step :  73 loss :  0.04005248844623566\n",
      "step :  74 loss :  0.042226534336805344\n",
      "step :  75 loss :  0.04082489386200905\n",
      "step :  76 loss :  0.039788708090782166\n",
      "step :  77 loss :  0.04068206250667572\n",
      "step :  78 loss :  0.04155799746513367\n",
      "step :  79 loss :  0.04032009094953537\n",
      "step :  80 loss :  0.03924299404025078\n",
      "step :  81 loss :  0.040411874651908875\n",
      "step :  82 loss :  0.04028116539120674\n",
      "step :  83 loss :  0.0395495742559433\n",
      "step :  84 loss :  0.03957651928067207\n",
      "step :  85 loss :  0.03970771282911301\n",
      "step :  86 loss :  0.039075978100299835\n",
      "step :  87 loss :  0.03955667465925217\n",
      "step :  88 loss :  0.039884716272354126\n",
      "step :  89 loss :  0.03973159193992615\n",
      "step :  90 loss :  0.03965424746274948\n",
      "step :  91 loss :  0.03977423906326294\n",
      "step :  92 loss :  0.03915683552622795\n",
      "step :  93 loss :  0.03904829919338226\n",
      "step :  94 loss :  0.03948596864938736\n",
      "step :  95 loss :  0.038860272616147995\n",
      "step :  96 loss :  0.038756199181079865\n",
      "step :  97 loss :  0.0390753336250782\n",
      "step :  98 loss :  0.038671400398015976\n",
      "step :  99 loss :  0.038617320358753204\n",
      "step :  100 loss :  0.03985557705163956\n",
      "step :  101 loss :  0.03858362138271332\n",
      "step :  102 loss :  0.038590773940086365\n",
      "step :  103 loss :  0.038739196956157684\n",
      "step :  104 loss :  0.03869196027517319\n",
      "step :  105 loss :  0.038470085710287094\n",
      "step :  106 loss :  0.038578130304813385\n",
      "step :  107 loss :  0.03909464180469513\n",
      "step :  108 loss :  0.0385623574256897\n",
      "step :  109 loss :  0.038437385112047195\n",
      "step :  110 loss :  0.03851284831762314\n",
      "step :  111 loss :  0.03892955929040909\n",
      "step :  112 loss :  0.03854094073176384\n",
      "step :  113 loss :  0.03862759843468666\n",
      "step :  114 loss :  0.038723140954971313\n",
      "step :  115 loss :  0.03841795772314072\n",
      "step :  116 loss :  0.03871402516961098\n",
      "step :  117 loss :  0.03849012032151222\n",
      "step :  118 loss :  0.03882889822125435\n",
      "step :  119 loss :  0.03887765854597092\n",
      "step :  120 loss :  0.04307660833001137\n",
      "step :  121 loss :  0.04180029779672623\n",
      "step :  122 loss :  0.045927975326776505\n",
      "step :  123 loss :  0.04198296740651131\n",
      "step :  124 loss :  0.04203016310930252\n",
      "step :  125 loss :  0.03844345360994339\n",
      "step :  126 loss :  0.03852119669318199\n",
      "step :  127 loss :  0.0382998064160347\n",
      "step :  128 loss :  0.03850949555635452\n",
      "step :  129 loss :  0.03862917795777321\n",
      "step :  130 loss :  0.03872011601924896\n",
      "step :  131 loss :  0.03841691464185715\n",
      "step :  132 loss :  0.03914043679833412\n",
      "step :  133 loss :  0.03836023434996605\n",
      "step :  134 loss :  0.03990764170885086\n",
      "step :  135 loss :  0.03859145939350128\n",
      "step :  136 loss :  0.04058738425374031\n",
      "step :  137 loss :  0.03865205496549606\n",
      "step :  138 loss :  0.039508212357759476\n",
      "step :  139 loss :  0.03829355910420418\n",
      "step :  140 loss :  0.03883785381913185\n",
      "step :  141 loss :  0.03829243406653404\n",
      "step :  142 loss :  0.03878486901521683\n",
      "step :  143 loss :  0.03829488530755043\n",
      "step :  144 loss :  0.03851919621229172\n",
      "step :  145 loss :  0.03823882341384888\n",
      "step :  146 loss :  0.038449764251708984\n",
      "step :  147 loss :  0.03820858150720596\n",
      "step :  148 loss :  0.038831889629364014\n",
      "step :  149 loss :  0.038211021572351456\n",
      "step :  150 loss :  0.03856190666556358\n",
      "step :  151 loss :  0.038287203758955\n",
      "step :  152 loss :  0.0384247861802578\n",
      "step :  153 loss :  0.03820923715829849\n",
      "step :  154 loss :  0.03860080987215042\n",
      "step :  155 loss :  0.038190413266420364\n",
      "step :  156 loss :  0.03836871683597565\n",
      "step :  157 loss :  0.038223206996917725\n",
      "step :  158 loss :  0.038303252309560776\n",
      "step :  159 loss :  0.0381462499499321\n",
      "step :  160 loss :  0.03832307830452919\n",
      "step :  161 loss :  0.038148168474435806\n",
      "step :  162 loss :  0.038360148668289185\n",
      "step :  163 loss :  0.03812842071056366\n",
      "step :  164 loss :  0.038238972425460815\n",
      "step :  165 loss :  0.038159407675266266\n",
      "step :  166 loss :  0.03818240016698837\n",
      "step :  167 loss :  0.03811568766832352\n",
      "step :  168 loss :  0.038313742727041245\n",
      "step :  169 loss :  0.03808235377073288\n",
      "step :  170 loss :  0.038222216069698334\n",
      "step :  171 loss :  0.03808293119072914\n",
      "step :  172 loss :  0.038152143359184265\n",
      "step :  173 loss :  0.03804997354745865\n",
      "step :  174 loss :  0.03830353915691376\n",
      "step :  175 loss :  0.03805612772703171\n",
      "step :  176 loss :  0.03818539157509804\n",
      "step :  177 loss :  0.03813514485955238\n",
      "step :  178 loss :  0.03812485933303833\n",
      "step :  179 loss :  0.03819330036640167\n",
      "step :  180 loss :  0.038091547787189484\n",
      "step :  181 loss :  0.038241755217313766\n",
      "step :  182 loss :  0.03803744912147522\n",
      "step :  183 loss :  0.038119107484817505\n",
      "step :  184 loss :  0.03805968165397644\n",
      "step :  185 loss :  0.038079459220170975\n",
      "step :  186 loss :  0.0380813404917717\n",
      "step :  187 loss :  0.03805677592754364\n",
      "step :  188 loss :  0.0381082221865654\n",
      "step :  189 loss :  0.03803860768675804\n",
      "step :  190 loss :  0.03819853812456131\n",
      "step :  191 loss :  0.038009773939847946\n",
      "step :  192 loss :  0.038055047392845154\n",
      "step :  193 loss :  0.03802110254764557\n",
      "step :  194 loss :  0.03805960714817047\n",
      "step :  195 loss :  0.03800029680132866\n",
      "step :  196 loss :  0.038102563470602036\n",
      "step :  197 loss :  0.03799162805080414\n",
      "step :  198 loss :  0.03808264061808586\n",
      "step :  199 loss :  0.038048531860113144\n",
      "step :  200 loss :  0.03801093250513077\n",
      "step :  201 loss :  0.03806241974234581\n",
      "step :  202 loss :  0.03801960125565529\n",
      "step :  203 loss :  0.03805117309093475\n",
      "step :  204 loss :  0.037996139377355576\n",
      "step :  205 loss :  0.038089215755462646\n",
      "step :  206 loss :  0.03798536956310272\n",
      "step :  207 loss :  0.03803747519850731\n",
      "step :  208 loss :  0.03799205645918846\n",
      "step :  209 loss :  0.03803032264113426\n",
      "step :  210 loss :  0.037992656230926514\n",
      "step :  211 loss :  0.03801554813981056\n",
      "step :  212 loss :  0.03799230232834816\n",
      "step :  213 loss :  0.03798851743340492\n",
      "step :  214 loss :  0.03797949105501175\n",
      "step :  215 loss :  0.0380055233836174\n",
      "step :  216 loss :  0.03796060383319855\n",
      "step :  217 loss :  0.03799647092819214\n",
      "step :  218 loss :  0.03796523064374924\n",
      "step :  219 loss :  0.03796682879328728\n",
      "step :  220 loss :  0.03798714280128479\n",
      "step :  221 loss :  0.03796213120222092\n",
      "step :  222 loss :  0.03802924230694771\n",
      "step :  223 loss :  0.03795043006539345\n",
      "step :  224 loss :  0.03798002377152443\n",
      "step :  225 loss :  0.03795364499092102\n",
      "step :  226 loss :  0.03799882531166077\n",
      "step :  227 loss :  0.03796530142426491\n",
      "step :  228 loss :  0.037955012172460556\n",
      "step :  229 loss :  0.03796758875250816\n",
      "step :  230 loss :  0.0379476398229599\n",
      "step :  231 loss :  0.037981171160936356\n",
      "step :  232 loss :  0.037944160401821136\n",
      "step :  233 loss :  0.03796764090657234\n",
      "step :  234 loss :  0.037942271679639816\n",
      "step :  235 loss :  0.03796464204788208\n",
      "step :  236 loss :  0.03793394938111305\n",
      "step :  237 loss :  0.03796648606657982\n",
      "step :  238 loss :  0.03793763741850853\n",
      "step :  239 loss :  0.037928760051727295\n",
      "step :  240 loss :  0.03794427588582039\n",
      "step :  241 loss :  0.03793313726782799\n",
      "step :  242 loss :  0.03794164955615997\n",
      "step :  243 loss :  0.03793316334486008\n",
      "step :  244 loss :  0.037941984832286835\n",
      "step :  245 loss :  0.03793051466345787\n",
      "step :  246 loss :  0.03793979808688164\n",
      "step :  247 loss :  0.03792732208967209\n",
      "step :  248 loss :  0.0379440002143383\n",
      "step :  249 loss :  0.037923384457826614\n",
      "step :  250 loss :  0.0379510298371315\n",
      "step :  251 loss :  0.03791336342692375\n",
      "step :  252 loss :  0.03793085366487503\n",
      "step :  253 loss :  0.03793896362185478\n",
      "step :  254 loss :  0.037909135222435\n",
      "step :  255 loss :  0.03792162239551544\n",
      "step :  256 loss :  0.03795149549841881\n",
      "step :  257 loss :  0.03790897876024246\n",
      "step :  258 loss :  0.03792500123381615\n",
      "step :  259 loss :  0.037919674068689346\n",
      "step :  260 loss :  0.03792007267475128\n",
      "step :  261 loss :  0.03792260214686394\n",
      "step :  262 loss :  0.0379098542034626\n",
      "step :  263 loss :  0.03793526813387871\n",
      "step :  264 loss :  0.03791351243853569\n",
      "step :  265 loss :  0.037918198853731155\n",
      "step :  266 loss :  0.03791619464755058\n",
      "step :  267 loss :  0.03791946545243263\n",
      "step :  268 loss :  0.037912849336862564\n",
      "step :  269 loss :  0.03792119771242142\n",
      "step :  270 loss :  0.03790969401597977\n",
      "step :  271 loss :  0.03791779279708862\n",
      "step :  272 loss :  0.0379120409488678\n",
      "step :  273 loss :  0.03791226074099541\n",
      "step :  274 loss :  0.037916336208581924\n",
      "step :  275 loss :  0.03791102021932602\n",
      "step :  276 loss :  0.03790940344333649\n",
      "step :  277 loss :  0.037918850779533386\n",
      "step :  278 loss :  0.03790992498397827\n",
      "step :  279 loss :  0.03791661188006401\n",
      "step :  280 loss :  0.0379054993391037\n",
      "step :  281 loss :  0.03791779652237892\n",
      "step :  282 loss :  0.03790545091032982\n",
      "step :  283 loss :  0.03791999816894531\n",
      "step :  284 loss :  0.03790793567895889\n",
      "step :  285 loss :  0.037907324731349945\n",
      "step :  286 loss :  0.037915244698524475\n",
      "step :  287 loss :  0.037901636213064194\n",
      "step :  288 loss :  0.03790833055973053\n",
      "step :  289 loss :  0.03790688514709473\n",
      "step :  290 loss :  0.03790445625782013\n",
      "step :  291 loss :  0.037905845791101456\n",
      "step :  292 loss :  0.03790551796555519\n",
      "step :  293 loss :  0.03790450468659401\n",
      "step :  294 loss :  0.037902962416410446\n",
      "step :  295 loss :  0.037907153367996216\n",
      "step :  296 loss :  0.03789731115102768\n",
      "step :  297 loss :  0.0379033237695694\n",
      "step :  298 loss :  0.03790656849741936\n",
      "step :  299 loss :  0.037894874811172485\n",
      "step :  300 loss :  0.03790583834052086\n",
      "step :  301 loss :  0.037900086492300034\n",
      "step :  302 loss :  0.03790032118558884\n",
      "step :  303 loss :  0.03790450096130371\n",
      "step :  304 loss :  0.03789530321955681\n",
      "step :  305 loss :  0.03790004551410675\n",
      "step :  306 loss :  0.03790048882365227\n",
      "step :  307 loss :  0.03789643943309784\n",
      "step :  308 loss :  0.037903353571891785\n",
      "step :  309 loss :  0.037896931171417236\n",
      "step :  310 loss :  0.03789743781089783\n",
      "step :  311 loss :  0.03790001943707466\n",
      "step :  312 loss :  0.03788980469107628\n",
      "step :  313 loss :  0.03789490833878517\n",
      "step :  314 loss :  0.037901539355516434\n",
      "step :  315 loss :  0.03789098560810089\n",
      "step :  316 loss :  0.03789426386356354\n",
      "step :  317 loss :  0.037900879979133606\n",
      "step :  318 loss :  0.037891943007707596\n",
      "step :  319 loss :  0.03789229318499565\n",
      "step :  320 loss :  0.03790131211280823\n",
      "step :  321 loss :  0.037896156311035156\n",
      "step :  322 loss :  0.037890560925006866\n",
      "step :  323 loss :  0.037895046174526215\n",
      "step :  324 loss :  0.03790150582790375\n",
      "step :  325 loss :  0.03789122775197029\n",
      "step :  326 loss :  0.03789352998137474\n",
      "step :  327 loss :  0.03789927065372467\n",
      "step :  328 loss :  0.03789110854268074\n",
      "step :  329 loss :  0.0378919318318367\n",
      "step :  330 loss :  0.03789571672677994\n",
      "step :  331 loss :  0.037891536951065063\n",
      "step :  332 loss :  0.037892282009124756\n",
      "step :  333 loss :  0.037894945591688156\n",
      "step :  334 loss :  0.03789069876074791\n",
      "step :  335 loss :  0.03789256513118744\n",
      "step :  336 loss :  0.03789573535323143\n",
      "step :  337 loss :  0.03788915276527405\n",
      "step :  338 loss :  0.037891119718551636\n",
      "step :  339 loss :  0.03789700195193291\n",
      "step :  340 loss :  0.037892214953899384\n",
      "step :  341 loss :  0.03789128363132477\n",
      "step :  342 loss :  0.03789348155260086\n",
      "step :  343 loss :  0.03789244964718819\n",
      "step :  344 loss :  0.037891797721385956\n",
      "step :  345 loss :  0.037891898304224014\n",
      "step :  346 loss :  0.03789258748292923\n",
      "step :  347 loss :  0.03789162263274193\n",
      "step :  348 loss :  0.03789154067635536\n",
      "step :  349 loss :  0.037892065942287445\n",
      "step :  350 loss :  0.03789182007312775\n",
      "step :  351 loss :  0.03789166361093521\n",
      "step :  352 loss :  0.03789209574460983\n",
      "step :  353 loss :  0.03789111226797104\n",
      "step :  354 loss :  0.03789098560810089\n",
      "step :  355 loss :  0.03789135441184044\n",
      "step :  356 loss :  0.03789226710796356\n",
      "step :  357 loss :  0.037891339510679245\n",
      "step :  358 loss :  0.03789125010371208\n",
      "step :  359 loss :  0.03789162263274193\n",
      "step :  360 loss :  0.037892621010541916\n",
      "step :  361 loss :  0.03789034113287926\n",
      "step :  362 loss :  0.03789066523313522\n",
      "step :  363 loss :  0.03789195045828819\n",
      "step :  364 loss :  0.03789044916629791\n",
      "step :  365 loss :  0.037890441715717316\n",
      "step :  366 loss :  0.037891265004873276\n",
      "step :  367 loss :  0.037889786064624786\n",
      "step :  368 loss :  0.037889767438173294\n",
      "step :  369 loss :  0.03789053484797478\n",
      "step :  370 loss :  0.03789062425494194\n",
      "step :  371 loss :  0.03789073973894119\n",
      "step :  372 loss :  0.03789069876074791\n",
      "step :  373 loss :  0.0378899946808815\n",
      "step :  374 loss :  0.03788987919688225\n",
      "step :  375 loss :  0.037890106439590454\n",
      "step :  376 loss :  0.03788990154862404\n",
      "step :  377 loss :  0.03788979351520538\n",
      "step :  378 loss :  0.03789108246564865\n",
      "step :  379 loss :  0.03788875415921211\n",
      "step :  380 loss :  0.037889059633016586\n",
      "step :  381 loss :  0.03789142519235611\n",
      "step :  382 loss :  0.03788960352540016\n",
      "step :  383 loss :  0.037889257073402405\n",
      "step :  384 loss :  0.03789040073752403\n",
      "step :  385 loss :  0.037889786064624786\n",
      "step :  386 loss :  0.03788907080888748\n",
      "step :  387 loss :  0.0378899872303009\n",
      "step :  388 loss :  0.03788936883211136\n",
      "step :  389 loss :  0.03788836672902107\n",
      "step :  390 loss :  0.037889569997787476\n",
      "step :  391 loss :  0.03788984566926956\n",
      "step :  392 loss :  0.037887658923864365\n",
      "step :  393 loss :  0.03788777440786362\n",
      "step :  394 loss :  0.03788935765624046\n",
      "step :  395 loss :  0.03789003565907478\n",
      "step :  396 loss :  0.03788866475224495\n",
      "step :  397 loss :  0.03788885474205017\n",
      "step :  398 loss :  0.03788963705301285\n",
      "step :  399 loss :  0.03788812458515167\n",
      "step :  400 loss :  0.03788787126541138\n",
      "step :  401 loss :  0.037888526916503906\n",
      "step :  402 loss :  0.03788905218243599\n",
      "step :  403 loss :  0.03788812458515167\n",
      "step :  404 loss :  0.03788787126541138\n",
      "step :  405 loss :  0.03788841888308525\n",
      "step :  406 loss :  0.03788883984088898\n",
      "step :  407 loss :  0.03788837417960167\n",
      "step :  408 loss :  0.03788822889328003\n",
      "step :  409 loss :  0.03788882493972778\n",
      "step :  410 loss :  0.037887975573539734\n",
      "step :  411 loss :  0.03788793459534645\n",
      "step :  412 loss :  0.03788823261857033\n",
      "step :  413 loss :  0.03788824379444122\n",
      "step :  414 loss :  0.03788795694708824\n",
      "step :  415 loss :  0.037887752056121826\n",
      "step :  416 loss :  0.0378880612552166\n",
      "step :  417 loss :  0.03788873180747032\n",
      "step :  418 loss :  0.03788827359676361\n",
      "step :  419 loss :  0.03788776323199272\n",
      "step :  420 loss :  0.03788801655173302\n",
      "step :  421 loss :  0.037888601422309875\n",
      "step :  422 loss :  0.037887685000896454\n",
      "step :  423 loss :  0.0378875732421875\n",
      "step :  424 loss :  0.037887733429670334\n",
      "step :  425 loss :  0.037887994199991226\n",
      "step :  426 loss :  0.03788841888308525\n",
      "step :  427 loss :  0.037887558341026306\n",
      "step :  428 loss :  0.03788718953728676\n",
      "step :  429 loss :  0.03788771107792854\n",
      "step :  430 loss :  0.03788840398192406\n",
      "step :  431 loss :  0.03788800165057182\n",
      "step :  432 loss :  0.037887316197156906\n",
      "step :  433 loss :  0.037887223064899445\n",
      "step :  434 loss :  0.03788760304450989\n",
      "step :  435 loss :  0.03788815066218376\n",
      "step :  436 loss :  0.037887606769800186\n",
      "step :  437 loss :  0.0378873310983181\n",
      "step :  438 loss :  0.03788748383522034\n",
      "step :  439 loss :  0.03788778558373451\n",
      "step :  440 loss :  0.03788750618696213\n",
      "step :  441 loss :  0.037887342274188995\n",
      "step :  442 loss :  0.037887345999479294\n",
      "step :  443 loss :  0.037887416779994965\n",
      "step :  444 loss :  0.0378875769674778\n",
      "step :  445 loss :  0.0378875806927681\n",
      "step :  446 loss :  0.03788759931921959\n",
      "step :  447 loss :  0.037887707352638245\n",
      "step :  448 loss :  0.03788750618696213\n",
      "step :  449 loss :  0.03788749873638153\n",
      "step :  450 loss :  0.0378875806927681\n",
      "step :  451 loss :  0.03788742795586586\n",
      "step :  452 loss :  0.03788754716515541\n",
      "step :  453 loss :  0.03788751736283302\n",
      "step :  454 loss :  0.037887539714574814\n",
      "step :  455 loss :  0.037887364625930786\n",
      "step :  456 loss :  0.037887297570705414\n",
      "step :  457 loss :  0.03788748383522034\n",
      "step :  458 loss :  0.037887632846832275\n",
      "step :  459 loss :  0.03788728266954422\n",
      "step :  460 loss :  0.037887219339609146\n",
      "step :  461 loss :  0.03788742423057556\n",
      "step :  462 loss :  0.03788786008954048\n",
      "step :  463 loss :  0.037887293845415115\n",
      "step :  464 loss :  0.037887152284383774\n",
      "step :  465 loss :  0.037887293845415115\n",
      "step :  466 loss :  0.037887394428253174\n",
      "step :  467 loss :  0.03788710758090019\n",
      "step :  468 loss :  0.03788704425096512\n",
      "step :  469 loss :  0.03788723051548004\n",
      "step :  470 loss :  0.03788735345005989\n",
      "step :  471 loss :  0.03788722679018974\n",
      "step :  472 loss :  0.03788721188902855\n",
      "step :  473 loss :  0.03788723796606064\n",
      "step :  474 loss :  0.03788730874657631\n",
      "step :  475 loss :  0.037887223064899445\n",
      "step :  476 loss :  0.03788720443844795\n",
      "step :  477 loss :  0.03788725659251213\n",
      "step :  478 loss :  0.03788736090064049\n",
      "step :  479 loss :  0.03788726031780243\n",
      "step :  480 loss :  0.03788727521896362\n",
      "step :  481 loss :  0.0378873273730278\n",
      "step :  482 loss :  0.03788713365793228\n",
      "step :  483 loss :  0.037887122482061386\n",
      "step :  484 loss :  0.03788715973496437\n",
      "step :  485 loss :  0.03788724169135094\n",
      "step :  486 loss :  0.037887394428253174\n",
      "step :  487 loss :  0.03788713738322258\n",
      "step :  488 loss :  0.03788707032799721\n",
      "step :  489 loss :  0.03788713365793228\n",
      "step :  490 loss :  0.03788719326257706\n",
      "step :  491 loss :  0.03788714110851288\n",
      "step :  492 loss :  0.03788714110851288\n",
      "step :  493 loss :  0.03788715973496437\n",
      "step :  494 loss :  0.037887152284383774\n",
      "step :  495 loss :  0.03788713738322258\n",
      "step :  496 loss :  0.03788718208670616\n",
      "step :  497 loss :  0.03788710758090019\n",
      "step :  498 loss :  0.037887092679739\n",
      "step :  499 loss :  0.03788711503148079\n",
      "Training model 9\n",
      "step :  0 loss :  0.47093528509140015\n",
      "step :  1 loss :  0.30004385113716125\n",
      "step :  2 loss :  0.1962672472000122\n",
      "step :  3 loss :  0.24133126437664032\n",
      "step :  4 loss :  0.18690524995326996\n",
      "step :  5 loss :  0.190204456448555\n",
      "step :  6 loss :  0.19079019129276276\n",
      "step :  7 loss :  0.16517525911331177\n",
      "step :  8 loss :  0.1468183398246765\n",
      "step :  9 loss :  0.14005184173583984\n",
      "step :  10 loss :  0.13356643915176392\n",
      "step :  11 loss :  0.14596708118915558\n",
      "step :  12 loss :  0.1323489397764206\n",
      "step :  13 loss :  0.12819579243659973\n",
      "step :  14 loss :  0.1349601149559021\n",
      "step :  15 loss :  0.13799194991588593\n",
      "step :  16 loss :  0.1253821700811386\n",
      "step :  17 loss :  0.11655283719301224\n",
      "step :  18 loss :  0.11321821808815002\n",
      "step :  19 loss :  0.11225675791501999\n",
      "step :  20 loss :  0.1086312085390091\n",
      "step :  21 loss :  0.10654722899198532\n",
      "step :  22 loss :  0.10553378611803055\n",
      "step :  23 loss :  0.10487941652536392\n",
      "step :  24 loss :  0.10172402858734131\n",
      "step :  25 loss :  0.10087184607982635\n",
      "step :  26 loss :  0.09831316769123077\n",
      "step :  27 loss :  0.09365708380937576\n",
      "step :  28 loss :  0.08688748627901077\n",
      "step :  29 loss :  0.0771954283118248\n",
      "step :  30 loss :  0.07020401209592819\n",
      "step :  31 loss :  0.059298500418663025\n",
      "step :  32 loss :  0.06209200993180275\n",
      "step :  33 loss :  0.05682045593857765\n",
      "step :  34 loss :  0.05782864987850189\n",
      "step :  35 loss :  0.05639556422829628\n",
      "step :  36 loss :  0.061126548796892166\n",
      "step :  37 loss :  0.054713692516088486\n",
      "step :  38 loss :  0.05903175473213196\n",
      "step :  39 loss :  0.058185748755931854\n",
      "step :  40 loss :  0.0740053728222847\n",
      "step :  41 loss :  0.08495054394006729\n",
      "step :  42 loss :  0.0738641768693924\n",
      "step :  43 loss :  0.05455213412642479\n",
      "step :  44 loss :  0.05509142577648163\n",
      "step :  45 loss :  0.06199479103088379\n",
      "step :  46 loss :  0.05501610040664673\n",
      "step :  47 loss :  0.05367892608046532\n",
      "step :  48 loss :  0.05289391428232193\n",
      "step :  49 loss :  0.053643498569726944\n",
      "step :  50 loss :  0.054766908288002014\n",
      "step :  51 loss :  0.05715680867433548\n",
      "step :  52 loss :  0.05986244976520538\n",
      "step :  53 loss :  0.0625218003988266\n",
      "step :  54 loss :  0.054532524198293686\n",
      "step :  55 loss :  0.05284203961491585\n",
      "step :  56 loss :  0.04908058047294617\n",
      "step :  57 loss :  0.04926848039031029\n",
      "step :  58 loss :  0.04803124815225601\n",
      "step :  59 loss :  0.047985248267650604\n",
      "step :  60 loss :  0.04774847254157066\n",
      "step :  61 loss :  0.04780295863747597\n",
      "step :  62 loss :  0.046996109187603\n",
      "step :  63 loss :  0.04833569750189781\n",
      "step :  64 loss :  0.04670992121100426\n",
      "step :  65 loss :  0.04979581758379936\n",
      "step :  66 loss :  0.046490833163261414\n",
      "step :  67 loss :  0.04947247728705406\n",
      "step :  68 loss :  0.04788628965616226\n",
      "step :  69 loss :  0.05524701997637749\n",
      "step :  70 loss :  0.05295819044113159\n",
      "step :  71 loss :  0.05502626299858093\n",
      "step :  72 loss :  0.04634533077478409\n",
      "step :  73 loss :  0.047826137393713\n",
      "step :  74 loss :  0.04482300952076912\n",
      "step :  75 loss :  0.04511125385761261\n",
      "step :  76 loss :  0.044798266142606735\n",
      "step :  77 loss :  0.044191110879182816\n",
      "step :  78 loss :  0.045368120074272156\n",
      "step :  79 loss :  0.04510536789894104\n",
      "step :  80 loss :  0.051502808928489685\n",
      "step :  81 loss :  0.04927404224872589\n",
      "step :  82 loss :  0.04996069520711899\n",
      "step :  83 loss :  0.043805498629808426\n",
      "step :  84 loss :  0.04461779445409775\n",
      "step :  85 loss :  0.04280305653810501\n",
      "step :  86 loss :  0.04345341771841049\n",
      "step :  87 loss :  0.04257839918136597\n",
      "step :  88 loss :  0.045588418841362\n",
      "step :  89 loss :  0.043490368872880936\n",
      "step :  90 loss :  0.0471186563372612\n",
      "step :  91 loss :  0.045094866305589676\n",
      "step :  92 loss :  0.04926897585391998\n",
      "step :  93 loss :  0.04629381000995636\n",
      "step :  94 loss :  0.04672760143876076\n",
      "step :  95 loss :  0.043051283806562424\n",
      "step :  96 loss :  0.0438789539039135\n",
      "step :  97 loss :  0.041865553706884384\n",
      "step :  98 loss :  0.043687306344509125\n",
      "step :  99 loss :  0.04191154986619949\n",
      "step :  100 loss :  0.044526323676109314\n",
      "step :  101 loss :  0.04187712445855141\n",
      "step :  102 loss :  0.04507676884531975\n",
      "step :  103 loss :  0.04289379343390465\n",
      "step :  104 loss :  0.0456039234995842\n",
      "step :  105 loss :  0.04394740238785744\n",
      "step :  106 loss :  0.04455834999680519\n",
      "step :  107 loss :  0.04139706492424011\n",
      "step :  108 loss :  0.04238195717334747\n",
      "step :  109 loss :  0.04061802476644516\n",
      "step :  110 loss :  0.04145187512040138\n",
      "step :  111 loss :  0.04047654941678047\n",
      "step :  112 loss :  0.04133574292063713\n",
      "step :  113 loss :  0.040418993681669235\n",
      "step :  114 loss :  0.04141688346862793\n",
      "step :  115 loss :  0.04040195047855377\n",
      "step :  116 loss :  0.0410035103559494\n",
      "step :  117 loss :  0.040383998304605484\n",
      "step :  118 loss :  0.041812870651483536\n",
      "step :  119 loss :  0.040463369339704514\n",
      "step :  120 loss :  0.04206971451640129\n",
      "step :  121 loss :  0.04030372202396393\n",
      "step :  122 loss :  0.04160469025373459\n",
      "step :  123 loss :  0.04023841768503189\n",
      "step :  124 loss :  0.04118132218718529\n",
      "step :  125 loss :  0.040185753256082535\n",
      "step :  126 loss :  0.040897998958826065\n",
      "step :  127 loss :  0.0401250496506691\n",
      "step :  128 loss :  0.04075309634208679\n",
      "step :  129 loss :  0.040104132145643234\n",
      "step :  130 loss :  0.0408259779214859\n",
      "step :  131 loss :  0.040050406008958817\n",
      "step :  132 loss :  0.04112223535776138\n",
      "step :  133 loss :  0.04000808298587799\n",
      "step :  134 loss :  0.041013672947883606\n",
      "step :  135 loss :  0.04011840745806694\n",
      "step :  136 loss :  0.04003920033574104\n",
      "step :  137 loss :  0.04058465734124184\n",
      "step :  138 loss :  0.03991561010479927\n",
      "step :  139 loss :  0.04046143591403961\n",
      "step :  140 loss :  0.03995371609926224\n",
      "step :  141 loss :  0.040067508816719055\n",
      "step :  142 loss :  0.039960674941539764\n",
      "step :  143 loss :  0.03983224928379059\n",
      "step :  144 loss :  0.04042603075504303\n",
      "step :  145 loss :  0.039773836731910706\n",
      "step :  146 loss :  0.04009229689836502\n",
      "step :  147 loss :  0.03984803333878517\n",
      "step :  148 loss :  0.03989287093281746\n",
      "step :  149 loss :  0.03999795764684677\n",
      "step :  150 loss :  0.03969382867217064\n",
      "step :  151 loss :  0.040217895060777664\n",
      "step :  152 loss :  0.03970794007182121\n",
      "step :  153 loss :  0.03970271721482277\n",
      "step :  154 loss :  0.04005403444170952\n",
      "step :  155 loss :  0.03962702676653862\n",
      "step :  156 loss :  0.039758093655109406\n",
      "step :  157 loss :  0.03995643928647041\n",
      "step :  158 loss :  0.03959945961833\n",
      "step :  159 loss :  0.04007188230752945\n",
      "step :  160 loss :  0.039577893912792206\n",
      "step :  161 loss :  0.03981243818998337\n",
      "step :  162 loss :  0.0397375114262104\n",
      "step :  163 loss :  0.03958514705300331\n",
      "step :  164 loss :  0.03981877118349075\n",
      "step :  165 loss :  0.03962405025959015\n",
      "step :  166 loss :  0.03963255509734154\n",
      "step :  167 loss :  0.039777521044015884\n",
      "step :  168 loss :  0.03952614590525627\n",
      "step :  169 loss :  0.039928972721099854\n",
      "step :  170 loss :  0.039503064006567\n",
      "step :  171 loss :  0.03961019963026047\n",
      "step :  172 loss :  0.039555806666612625\n",
      "step :  173 loss :  0.03953990712761879\n",
      "step :  174 loss :  0.03958255052566528\n",
      "step :  175 loss :  0.03957542032003403\n",
      "step :  176 loss :  0.039495568722486496\n",
      "step :  177 loss :  0.03961566463112831\n",
      "step :  178 loss :  0.03958897665143013\n",
      "step :  179 loss :  0.03958185017108917\n",
      "step :  180 loss :  0.03956075757741928\n",
      "step :  181 loss :  0.039481401443481445\n",
      "step :  182 loss :  0.039535749703645706\n",
      "step :  183 loss :  0.03946331888437271\n",
      "step :  184 loss :  0.03955291211605072\n",
      "step :  185 loss :  0.03948226198554039\n",
      "step :  186 loss :  0.03946477174758911\n",
      "step :  187 loss :  0.03952028229832649\n",
      "step :  188 loss :  0.039400339126586914\n",
      "step :  189 loss :  0.03954382240772247\n",
      "step :  190 loss :  0.039412450045347214\n",
      "step :  191 loss :  0.03941982984542847\n",
      "step :  192 loss :  0.039626188576221466\n",
      "step :  193 loss :  0.03941749781370163\n",
      "step :  194 loss :  0.03938474506139755\n",
      "step :  195 loss :  0.039712660014629364\n",
      "step :  196 loss :  0.03936384618282318\n",
      "step :  197 loss :  0.0393800251185894\n",
      "step :  198 loss :  0.039458081126213074\n",
      "step :  199 loss :  0.03936408832669258\n",
      "step :  200 loss :  0.03938015177845955\n",
      "step :  201 loss :  0.03936946019530296\n",
      "step :  202 loss :  0.03937481343746185\n",
      "step :  203 loss :  0.03933797404170036\n",
      "step :  204 loss :  0.03947097808122635\n",
      "step :  205 loss :  0.03934227675199509\n",
      "step :  206 loss :  0.03934311866760254\n",
      "step :  207 loss :  0.03939588740468025\n",
      "step :  208 loss :  0.039319925010204315\n",
      "step :  209 loss :  0.03941469267010689\n",
      "step :  210 loss :  0.03934984654188156\n",
      "step :  211 loss :  0.039313144981861115\n",
      "step :  212 loss :  0.03942132368683815\n",
      "step :  213 loss :  0.039326876401901245\n",
      "step :  214 loss :  0.03933511674404144\n",
      "step :  215 loss :  0.03933219984173775\n",
      "step :  216 loss :  0.03930726274847984\n",
      "step :  217 loss :  0.03935444727540016\n",
      "step :  218 loss :  0.039312928915023804\n",
      "step :  219 loss :  0.03930852189660072\n",
      "step :  220 loss :  0.039314527064561844\n",
      "step :  221 loss :  0.03928808495402336\n",
      "step :  222 loss :  0.039379753172397614\n",
      "step :  223 loss :  0.039286788552999496\n",
      "step :  224 loss :  0.039283085614442825\n",
      "step :  225 loss :  0.039338119328022\n",
      "step :  226 loss :  0.03927532956004143\n",
      "step :  227 loss :  0.03931869938969612\n",
      "step :  228 loss :  0.039277881383895874\n",
      "step :  229 loss :  0.03926916792988777\n",
      "step :  230 loss :  0.03933494910597801\n",
      "step :  231 loss :  0.03925396502017975\n",
      "step :  232 loss :  0.03926593437790871\n",
      "step :  233 loss :  0.03929378464818001\n",
      "step :  234 loss :  0.03925062343478203\n",
      "step :  235 loss :  0.0392790362238884\n",
      "step :  236 loss :  0.03924611955881119\n",
      "step :  237 loss :  0.03925235569477081\n",
      "step :  238 loss :  0.0393027700483799\n",
      "step :  239 loss :  0.039243683218955994\n",
      "step :  240 loss :  0.03925475850701332\n",
      "step :  241 loss :  0.03926363214850426\n",
      "step :  242 loss :  0.03923535719513893\n",
      "step :  243 loss :  0.03925618156790733\n",
      "step :  244 loss :  0.03926178440451622\n",
      "step :  245 loss :  0.0392279326915741\n",
      "step :  246 loss :  0.03924025595188141\n",
      "step :  247 loss :  0.039246611297130585\n",
      "step :  248 loss :  0.03922928497195244\n",
      "step :  249 loss :  0.039240725338459015\n",
      "step :  250 loss :  0.03922340273857117\n",
      "step :  251 loss :  0.03923095762729645\n",
      "step :  252 loss :  0.039233818650245667\n",
      "step :  253 loss :  0.03922084718942642\n",
      "step :  254 loss :  0.03923368081450462\n",
      "step :  255 loss :  0.03923889994621277\n",
      "step :  256 loss :  0.03921990841627121\n",
      "step :  257 loss :  0.03922862187027931\n",
      "step :  258 loss :  0.03923654183745384\n",
      "step :  259 loss :  0.039215490221977234\n",
      "step :  260 loss :  0.03923000022768974\n",
      "step :  261 loss :  0.03923136368393898\n",
      "step :  262 loss :  0.039207447320222855\n",
      "step :  263 loss :  0.03922763466835022\n",
      "step :  264 loss :  0.03921879455447197\n",
      "step :  265 loss :  0.0392039455473423\n",
      "step :  266 loss :  0.03921644017100334\n",
      "step :  267 loss :  0.039215296506881714\n",
      "step :  268 loss :  0.03920074179768562\n",
      "step :  269 loss :  0.03920750692486763\n",
      "step :  270 loss :  0.039233144372701645\n",
      "step :  271 loss :  0.03919459879398346\n",
      "step :  272 loss :  0.03920725733041763\n",
      "step :  273 loss :  0.039241619408130646\n",
      "step :  274 loss :  0.039193857461214066\n",
      "step :  275 loss :  0.03918636962771416\n",
      "step :  276 loss :  0.039241474121809006\n",
      "step :  277 loss :  0.03922899812459946\n",
      "step :  278 loss :  0.039186105132102966\n",
      "step :  279 loss :  0.0391998291015625\n",
      "step :  280 loss :  0.03920508548617363\n",
      "step :  281 loss :  0.03920101746916771\n",
      "step :  282 loss :  0.039190832525491714\n",
      "step :  283 loss :  0.03919432684779167\n",
      "step :  284 loss :  0.03920801356434822\n",
      "step :  285 loss :  0.03918956220149994\n",
      "step :  286 loss :  0.03918880224227905\n",
      "step :  287 loss :  0.03919648379087448\n",
      "step :  288 loss :  0.039191149175167084\n",
      "step :  289 loss :  0.039190467447042465\n",
      "step :  290 loss :  0.03918984532356262\n",
      "step :  291 loss :  0.03918822482228279\n",
      "step :  292 loss :  0.03918973356485367\n",
      "step :  293 loss :  0.03919001668691635\n",
      "step :  294 loss :  0.03918776661157608\n",
      "step :  295 loss :  0.03918048366904259\n",
      "step :  296 loss :  0.03918485343456268\n",
      "step :  297 loss :  0.039194073528051376\n",
      "step :  298 loss :  0.0391913503408432\n",
      "step :  299 loss :  0.03918304666876793\n",
      "step :  300 loss :  0.03918223828077316\n",
      "step :  301 loss :  0.039186473935842514\n",
      "step :  302 loss :  0.03918422386050224\n",
      "step :  303 loss :  0.0391814224421978\n",
      "step :  304 loss :  0.03918107971549034\n",
      "step :  305 loss :  0.039183661341667175\n",
      "step :  306 loss :  0.039182886481285095\n",
      "step :  307 loss :  0.03918061777949333\n",
      "step :  308 loss :  0.03918248414993286\n",
      "step :  309 loss :  0.03917805105447769\n",
      "step :  310 loss :  0.03917604312300682\n",
      "step :  311 loss :  0.03918314352631569\n",
      "step :  312 loss :  0.03918664902448654\n",
      "step :  313 loss :  0.039173491299152374\n",
      "step :  314 loss :  0.039173346012830734\n",
      "step :  315 loss :  0.0391809418797493\n",
      "step :  316 loss :  0.039180319756269455\n",
      "step :  317 loss :  0.039176858961582184\n",
      "step :  318 loss :  0.03917636722326279\n",
      "step :  319 loss :  0.039178185164928436\n",
      "step :  320 loss :  0.03917746990919113\n",
      "step :  321 loss :  0.03917263448238373\n",
      "step :  322 loss :  0.0391729474067688\n",
      "step :  323 loss :  0.039178453385829926\n",
      "step :  324 loss :  0.03917986899614334\n",
      "step :  325 loss :  0.03917528688907623\n",
      "step :  326 loss :  0.03917068988084793\n",
      "step :  327 loss :  0.039171915501356125\n",
      "step :  328 loss :  0.03917630389332771\n",
      "step :  329 loss :  0.03917761892080307\n",
      "step :  330 loss :  0.03917344659566879\n",
      "step :  331 loss :  0.03917117044329643\n",
      "step :  332 loss :  0.039174117147922516\n",
      "step :  333 loss :  0.03917573764920235\n",
      "step :  334 loss :  0.03917626291513443\n",
      "step :  335 loss :  0.03917446359992027\n",
      "step :  336 loss :  0.03917139396071434\n",
      "step :  337 loss :  0.03917074576020241\n",
      "step :  338 loss :  0.0391714833676815\n",
      "step :  339 loss :  0.03917301446199417\n",
      "step :  340 loss :  0.03917396813631058\n",
      "step :  341 loss :  0.039171118289232254\n",
      "step :  342 loss :  0.03916964307427406\n",
      "step :  343 loss :  0.03917209059000015\n",
      "step :  344 loss :  0.039173442870378494\n",
      "step :  345 loss :  0.03917110338807106\n",
      "step :  346 loss :  0.03916996344923973\n",
      "step :  347 loss :  0.03917194902896881\n",
      "step :  348 loss :  0.039173442870378494\n",
      "step :  349 loss :  0.03916970640420914\n",
      "step :  350 loss :  0.039167989045381546\n",
      "step :  351 loss :  0.03916976973414421\n",
      "step :  352 loss :  0.039171986281871796\n",
      "step :  353 loss :  0.03916969895362854\n",
      "step :  354 loss :  0.039167728275060654\n",
      "step :  355 loss :  0.03916756063699722\n",
      "step :  356 loss :  0.03916873782873154\n",
      "step :  357 loss :  0.039169248193502426\n",
      "step :  358 loss :  0.03916919603943825\n",
      "step :  359 loss :  0.03916860744357109\n",
      "step :  360 loss :  0.03916796296834946\n",
      "step :  361 loss :  0.03916844725608826\n",
      "step :  362 loss :  0.03916770592331886\n",
      "step :  363 loss :  0.03916820138692856\n",
      "step :  364 loss :  0.03916831687092781\n",
      "step :  365 loss :  0.03916797786951065\n",
      "step :  366 loss :  0.03916894644498825\n",
      "step :  367 loss :  0.0391673780977726\n",
      "step :  368 loss :  0.039167147129774094\n",
      "step :  369 loss :  0.03916877880692482\n",
      "step :  370 loss :  0.03916731849312782\n",
      "step :  371 loss :  0.039165887981653214\n",
      "step :  372 loss :  0.039166029542684555\n",
      "step :  373 loss :  0.039167389273643494\n",
      "step :  374 loss :  0.03916674852371216\n",
      "step :  375 loss :  0.03916678577661514\n",
      "step :  376 loss :  0.03916701674461365\n",
      "step :  377 loss :  0.03916585072875023\n",
      "step :  378 loss :  0.039166409522295\n",
      "step :  379 loss :  0.03916753828525543\n",
      "step :  380 loss :  0.03916582092642784\n",
      "step :  381 loss :  0.039165765047073364\n",
      "step :  382 loss :  0.03916682302951813\n",
      "step :  383 loss :  0.03916505351662636\n",
      "step :  384 loss :  0.039165198802948\n",
      "step :  385 loss :  0.039166755974292755\n",
      "step :  386 loss :  0.03916649892926216\n",
      "step :  387 loss :  0.039164792746305466\n",
      "step :  388 loss :  0.039164330810308456\n",
      "step :  389 loss :  0.03916534408926964\n",
      "step :  390 loss :  0.039166297763586044\n",
      "step :  391 loss :  0.03916459530591965\n",
      "step :  392 loss :  0.03916401416063309\n",
      "step :  393 loss :  0.03916490823030472\n",
      "step :  394 loss :  0.03916631266474724\n",
      "step :  395 loss :  0.03916524723172188\n",
      "step :  396 loss :  0.039163749665021896\n",
      "step :  397 loss :  0.039164479821920395\n",
      "step :  398 loss :  0.03916574642062187\n",
      "step :  399 loss :  0.039165034890174866\n",
      "step :  400 loss :  0.03916347399353981\n",
      "step :  401 loss :  0.039163440465927124\n",
      "step :  402 loss :  0.039164166897535324\n",
      "step :  403 loss :  0.03916466608643532\n",
      "step :  404 loss :  0.03916408494114876\n",
      "step :  405 loss :  0.03916417434811592\n",
      "step :  406 loss :  0.03916523978114128\n",
      "step :  407 loss :  0.03916453570127487\n",
      "step :  408 loss :  0.03916303813457489\n",
      "step :  409 loss :  0.03916344419121742\n",
      "step :  410 loss :  0.03916436806321144\n",
      "step :  411 loss :  0.039164766669273376\n",
      "step :  412 loss :  0.03916377201676369\n",
      "step :  413 loss :  0.03916361555457115\n",
      "step :  414 loss :  0.03916408494114876\n",
      "step :  415 loss :  0.039164505898952484\n",
      "step :  416 loss :  0.03916393592953682\n",
      "step :  417 loss :  0.03916379436850548\n",
      "step :  418 loss :  0.03916396200656891\n",
      "step :  419 loss :  0.03916399925947189\n",
      "step :  420 loss :  0.03916332498192787\n",
      "step :  421 loss :  0.03916363790631294\n",
      "step :  422 loss :  0.039163973182439804\n",
      "step :  423 loss :  0.03916299715638161\n",
      "step :  424 loss :  0.03916296362876892\n",
      "step :  425 loss :  0.03916378319263458\n",
      "step :  426 loss :  0.03916367143392563\n",
      "step :  427 loss :  0.039162904024124146\n",
      "step :  428 loss :  0.039163000881671906\n",
      "step :  429 loss :  0.039163507521152496\n",
      "step :  430 loss :  0.03916362300515175\n",
      "step :  431 loss :  0.039163410663604736\n",
      "step :  432 loss :  0.03916309401392937\n",
      "step :  433 loss :  0.03916291147470474\n",
      "step :  434 loss :  0.039163246750831604\n",
      "step :  435 loss :  0.03916354849934578\n",
      "step :  436 loss :  0.039162855595350266\n",
      "step :  437 loss :  0.03916289284825325\n",
      "step :  438 loss :  0.03916329890489578\n",
      "step :  439 loss :  0.03916310518980026\n",
      "step :  440 loss :  0.03916297107934952\n",
      "step :  441 loss :  0.03916308656334877\n",
      "step :  442 loss :  0.039162762463092804\n",
      "step :  443 loss :  0.03916282579302788\n",
      "step :  444 loss :  0.039163053035736084\n",
      "step :  445 loss :  0.0391627699136734\n",
      "step :  446 loss :  0.0391627699136734\n",
      "step :  447 loss :  0.039162907749414444\n",
      "step :  448 loss :  0.0391627661883831\n",
      "step :  449 loss :  0.039162710309028625\n",
      "step :  450 loss :  0.039162684231996536\n",
      "step :  451 loss :  0.03916236758232117\n",
      "step :  452 loss :  0.03916248306632042\n",
      "step :  453 loss :  0.03916278854012489\n",
      "step :  454 loss :  0.03916255384683609\n",
      "step :  455 loss :  0.03916259482502937\n",
      "step :  456 loss :  0.03916247561573982\n",
      "step :  457 loss :  0.03916255384683609\n",
      "step :  458 loss :  0.03916274011135101\n",
      "step :  459 loss :  0.03916246443986893\n",
      "step :  460 loss :  0.03916262835264206\n",
      "step :  461 loss :  0.03916238620877266\n",
      "step :  462 loss :  0.03916241228580475\n",
      "step :  463 loss :  0.039162494242191315\n",
      "step :  464 loss :  0.03916241601109505\n",
      "step :  465 loss :  0.03916214779019356\n",
      "step :  466 loss :  0.03916221484541893\n",
      "step :  467 loss :  0.03916250169277191\n",
      "step :  468 loss :  0.039162345230579376\n",
      "step :  469 loss :  0.03916219249367714\n",
      "step :  470 loss :  0.03916241601109505\n",
      "step :  471 loss :  0.039162296801805496\n",
      "step :  472 loss :  0.03916190564632416\n",
      "step :  473 loss :  0.039161909371614456\n",
      "step :  474 loss :  0.039162084460258484\n",
      "step :  475 loss :  0.03916230425238609\n",
      "step :  476 loss :  0.039162151515483856\n",
      "step :  477 loss :  0.039162199944257736\n",
      "step :  478 loss :  0.03916223719716072\n",
      "step :  479 loss :  0.03916207700967789\n",
      "step :  480 loss :  0.03916211053729057\n",
      "step :  481 loss :  0.039162199944257736\n",
      "step :  482 loss :  0.039162054657936096\n",
      "step :  483 loss :  0.03916201740503311\n",
      "step :  484 loss :  0.0391620434820652\n",
      "step :  485 loss :  0.039162155240774155\n",
      "step :  486 loss :  0.03916214033961296\n",
      "step :  487 loss :  0.03916206210851669\n",
      "step :  488 loss :  0.03916202858090401\n",
      "step :  489 loss :  0.039162084460258484\n",
      "step :  490 loss :  0.0391620472073555\n",
      "step :  491 loss :  0.03916206955909729\n",
      "step :  492 loss :  0.03916200250387192\n",
      "step :  493 loss :  0.03916185721755028\n",
      "step :  494 loss :  0.03916188329458237\n",
      "step :  495 loss :  0.039161913096904755\n",
      "step :  496 loss :  0.03916192427277565\n",
      "step :  497 loss :  0.03916202858090401\n",
      "step :  498 loss :  0.03916193172335625\n",
      "step :  499 loss :  0.039161719381809235\n",
      "Training model 10\n",
      "step :  0 loss :  0.5326704978942871\n",
      "step :  1 loss :  0.33376777172088623\n",
      "step :  2 loss :  0.17591635882854462\n",
      "step :  3 loss :  0.2787784934043884\n",
      "step :  4 loss :  0.17141473293304443\n",
      "step :  5 loss :  0.17712491750717163\n",
      "step :  6 loss :  0.1870592087507248\n",
      "step :  7 loss :  0.1637764722108841\n",
      "step :  8 loss :  0.1372835636138916\n",
      "step :  9 loss :  0.14205463230609894\n",
      "step :  10 loss :  0.12079078704118729\n",
      "step :  11 loss :  0.137851282954216\n",
      "step :  12 loss :  0.1398686319589615\n",
      "step :  13 loss :  0.13176995515823364\n",
      "step :  14 loss :  0.13405373692512512\n",
      "step :  15 loss :  0.14026692509651184\n",
      "step :  16 loss :  0.13043361902236938\n",
      "step :  17 loss :  0.12043176591396332\n",
      "step :  18 loss :  0.11509770154953003\n",
      "step :  19 loss :  0.11193495243787766\n",
      "step :  20 loss :  0.1098228394985199\n",
      "step :  21 loss :  0.10654210299253464\n",
      "step :  22 loss :  0.10517103970050812\n",
      "step :  23 loss :  0.10380994528532028\n",
      "step :  24 loss :  0.10139602422714233\n",
      "step :  25 loss :  0.09750016778707504\n",
      "step :  26 loss :  0.09866826236248016\n",
      "step :  27 loss :  0.09697448462247849\n",
      "step :  28 loss :  0.09021730720996857\n",
      "step :  29 loss :  0.08536529541015625\n",
      "step :  30 loss :  0.08289144188165665\n",
      "step :  31 loss :  0.0713161826133728\n",
      "step :  32 loss :  0.06601693481206894\n",
      "step :  33 loss :  0.05895868316292763\n",
      "step :  34 loss :  0.05716835707426071\n",
      "step :  35 loss :  0.056195102632045746\n",
      "step :  36 loss :  0.06606291234493256\n",
      "step :  37 loss :  0.06311170011758804\n",
      "step :  38 loss :  0.0605231411755085\n",
      "step :  39 loss :  0.05022403970360756\n",
      "step :  40 loss :  0.05134329944849014\n",
      "step :  41 loss :  0.05115579068660736\n",
      "step :  42 loss :  0.05748020485043526\n",
      "step :  43 loss :  0.06462487578392029\n",
      "step :  44 loss :  0.07102673500776291\n",
      "step :  45 loss :  0.06215779110789299\n",
      "step :  46 loss :  0.057042594999074936\n",
      "step :  47 loss :  0.050148963928222656\n",
      "step :  48 loss :  0.0501955971121788\n",
      "step :  49 loss :  0.05972056835889816\n",
      "step :  50 loss :  0.05023139342665672\n",
      "step :  51 loss :  0.050100166350603104\n",
      "step :  52 loss :  0.04703536629676819\n",
      "step :  53 loss :  0.04652322456240654\n",
      "step :  54 loss :  0.04582344368100166\n",
      "step :  55 loss :  0.04546050354838371\n",
      "step :  56 loss :  0.04523550346493721\n",
      "step :  57 loss :  0.047619156539440155\n",
      "step :  58 loss :  0.04854457825422287\n",
      "step :  59 loss :  0.05456903204321861\n",
      "step :  60 loss :  0.055881597101688385\n",
      "step :  61 loss :  0.052978262305259705\n",
      "step :  62 loss :  0.04558219388127327\n",
      "step :  63 loss :  0.046546995639801025\n",
      "step :  64 loss :  0.04365701973438263\n",
      "step :  65 loss :  0.0438913069665432\n",
      "step :  66 loss :  0.04375957325100899\n",
      "step :  67 loss :  0.04242340102791786\n",
      "step :  68 loss :  0.04210745170712471\n",
      "step :  69 loss :  0.04409516602754593\n",
      "step :  70 loss :  0.04650232195854187\n",
      "step :  71 loss :  0.05558618903160095\n",
      "step :  72 loss :  0.05327728018164635\n",
      "step :  73 loss :  0.054170895367860794\n",
      "step :  74 loss :  0.046696703881025314\n",
      "step :  75 loss :  0.042800240218639374\n",
      "step :  76 loss :  0.04173216596245766\n",
      "step :  77 loss :  0.04137471690773964\n",
      "step :  78 loss :  0.04073600843548775\n",
      "step :  79 loss :  0.04106730967760086\n",
      "step :  80 loss :  0.04174353554844856\n",
      "step :  81 loss :  0.04083515331149101\n",
      "step :  82 loss :  0.04297706484794617\n",
      "step :  83 loss :  0.04311252385377884\n",
      "step :  84 loss :  0.0511748306453228\n",
      "step :  85 loss :  0.04598899185657501\n",
      "step :  86 loss :  0.04793066531419754\n",
      "step :  87 loss :  0.042180098593235016\n",
      "step :  88 loss :  0.04163390398025513\n",
      "step :  89 loss :  0.039735544472932816\n",
      "step :  90 loss :  0.04080445319414139\n",
      "step :  91 loss :  0.03953471779823303\n",
      "step :  92 loss :  0.0407252237200737\n",
      "step :  93 loss :  0.03993551433086395\n",
      "step :  94 loss :  0.042697321623563766\n",
      "step :  95 loss :  0.039775263518095016\n",
      "step :  96 loss :  0.04404476284980774\n",
      "step :  97 loss :  0.040910422801971436\n",
      "step :  98 loss :  0.04495442658662796\n",
      "step :  99 loss :  0.040464241057634354\n",
      "step :  100 loss :  0.04125291854143143\n",
      "step :  101 loss :  0.039579540491104126\n",
      "step :  102 loss :  0.03940923884510994\n",
      "step :  103 loss :  0.04073140025138855\n",
      "step :  104 loss :  0.039309680461883545\n",
      "step :  105 loss :  0.041837748140096664\n",
      "step :  106 loss :  0.03958212211728096\n",
      "step :  107 loss :  0.04074746370315552\n",
      "step :  108 loss :  0.039457038044929504\n",
      "step :  109 loss :  0.039535991847515106\n",
      "step :  110 loss :  0.04046095907688141\n",
      "step :  111 loss :  0.03935424983501434\n",
      "step :  112 loss :  0.040595535188913345\n",
      "step :  113 loss :  0.03947526589035988\n",
      "step :  114 loss :  0.03950101137161255\n",
      "step :  115 loss :  0.039674919098615646\n",
      "step :  116 loss :  0.0394715890288353\n",
      "step :  117 loss :  0.03914337232708931\n",
      "step :  118 loss :  0.039724286645650864\n",
      "step :  119 loss :  0.03917984664440155\n",
      "step :  120 loss :  0.039093080908060074\n",
      "step :  121 loss :  0.039661258459091187\n",
      "step :  122 loss :  0.03923769295215607\n",
      "step :  123 loss :  0.039097581058740616\n",
      "step :  124 loss :  0.039572689682245255\n",
      "step :  125 loss :  0.039262402802705765\n",
      "step :  126 loss :  0.03946005553007126\n",
      "step :  127 loss :  0.03913869708776474\n",
      "step :  128 loss :  0.03945442661643028\n",
      "step :  129 loss :  0.039006128907203674\n",
      "step :  130 loss :  0.03952699527144432\n",
      "step :  131 loss :  0.03892327472567558\n",
      "step :  132 loss :  0.03915147855877876\n",
      "step :  133 loss :  0.039072923362255096\n",
      "step :  134 loss :  0.038974836468696594\n",
      "step :  135 loss :  0.03937586396932602\n",
      "step :  136 loss :  0.039151858538389206\n",
      "step :  137 loss :  0.03905726224184036\n",
      "step :  138 loss :  0.039004113525152206\n",
      "step :  139 loss :  0.03905835002660751\n",
      "step :  140 loss :  0.038950249552726746\n",
      "step :  141 loss :  0.03906978294253349\n",
      "step :  142 loss :  0.03893256187438965\n",
      "step :  143 loss :  0.03909105435013771\n",
      "step :  144 loss :  0.03877634182572365\n",
      "step :  145 loss :  0.039046432822942734\n",
      "step :  146 loss :  0.0388653427362442\n",
      "step :  147 loss :  0.03879081830382347\n",
      "step :  148 loss :  0.03927614167332649\n",
      "step :  149 loss :  0.038733188062906265\n",
      "step :  150 loss :  0.03892281651496887\n",
      "step :  151 loss :  0.038914136588573456\n",
      "step :  152 loss :  0.038774047046899796\n",
      "step :  153 loss :  0.038850490003824234\n",
      "step :  154 loss :  0.0388360321521759\n",
      "step :  155 loss :  0.03876815363764763\n",
      "step :  156 loss :  0.03887701407074928\n",
      "step :  157 loss :  0.03868774697184563\n",
      "step :  158 loss :  0.03886526823043823\n",
      "step :  159 loss :  0.03866191953420639\n",
      "step :  160 loss :  0.03883802145719528\n",
      "step :  161 loss :  0.03864189237356186\n",
      "step :  162 loss :  0.038715898990631104\n",
      "step :  163 loss :  0.038640402257442474\n",
      "step :  164 loss :  0.038704074919223785\n",
      "step :  165 loss :  0.038691915571689606\n",
      "step :  166 loss :  0.03868444263935089\n",
      "step :  167 loss :  0.03863091021776199\n",
      "step :  168 loss :  0.0387054868042469\n",
      "step :  169 loss :  0.03864879906177521\n",
      "step :  170 loss :  0.038715213537216187\n",
      "step :  171 loss :  0.038625460118055344\n",
      "step :  172 loss :  0.038648054003715515\n",
      "step :  173 loss :  0.038642216473817825\n",
      "step :  174 loss :  0.03863907605409622\n",
      "step :  175 loss :  0.038615576922893524\n",
      "step :  176 loss :  0.03861505165696144\n",
      "step :  177 loss :  0.03858102485537529\n",
      "step :  178 loss :  0.038613177835941315\n",
      "step :  179 loss :  0.03857339173555374\n",
      "step :  180 loss :  0.03860556334257126\n",
      "step :  181 loss :  0.038581691682338715\n",
      "step :  182 loss :  0.0387195460498333\n",
      "step :  183 loss :  0.038520149886608124\n",
      "step :  184 loss :  0.03857184946537018\n",
      "step :  185 loss :  0.03867587819695473\n",
      "step :  186 loss :  0.03853442147374153\n",
      "step :  187 loss :  0.03866858035326004\n",
      "step :  188 loss :  0.03850856423377991\n",
      "step :  189 loss :  0.03857722133398056\n",
      "step :  190 loss :  0.038532596081495285\n",
      "step :  191 loss :  0.038565315306186676\n",
      "step :  192 loss :  0.03852564096450806\n",
      "step :  193 loss :  0.03865331783890724\n",
      "step :  194 loss :  0.0385177880525589\n",
      "step :  195 loss :  0.03855496272444725\n",
      "step :  196 loss :  0.03852510452270508\n",
      "step :  197 loss :  0.038514453917741776\n",
      "step :  198 loss :  0.03855527564883232\n",
      "step :  199 loss :  0.03844098001718521\n",
      "step :  200 loss :  0.038477860391139984\n",
      "step :  201 loss :  0.03866346552968025\n",
      "step :  202 loss :  0.03846493735909462\n",
      "step :  203 loss :  0.03845861926674843\n",
      "step :  204 loss :  0.03863770142197609\n",
      "step :  205 loss :  0.03849896788597107\n",
      "step :  206 loss :  0.0384649932384491\n",
      "step :  207 loss :  0.03855570778250694\n",
      "step :  208 loss :  0.038465265184640884\n",
      "step :  209 loss :  0.03846510499715805\n",
      "step :  210 loss :  0.0384964644908905\n",
      "step :  211 loss :  0.03844883665442467\n",
      "step :  212 loss :  0.03847804665565491\n",
      "step :  213 loss :  0.03853805363178253\n",
      "step :  214 loss :  0.038433924317359924\n",
      "step :  215 loss :  0.03846083581447601\n",
      "step :  216 loss :  0.038549598306417465\n",
      "step :  217 loss :  0.03842359781265259\n",
      "step :  218 loss :  0.03841542825102806\n",
      "step :  219 loss :  0.03853478282690048\n",
      "step :  220 loss :  0.038462694734334946\n",
      "step :  221 loss :  0.038415830582380295\n",
      "step :  222 loss :  0.03846282884478569\n",
      "step :  223 loss :  0.03848708048462868\n",
      "step :  224 loss :  0.03840836510062218\n",
      "step :  225 loss :  0.038428228348493576\n",
      "step :  226 loss :  0.038515858352184296\n",
      "step :  227 loss :  0.03844185918569565\n",
      "step :  228 loss :  0.03841857612133026\n",
      "step :  229 loss :  0.038455720990896225\n",
      "step :  230 loss :  0.038424473255872726\n",
      "step :  231 loss :  0.03841014578938484\n",
      "step :  232 loss :  0.0384305939078331\n",
      "step :  233 loss :  0.03842218220233917\n",
      "step :  234 loss :  0.038412582129240036\n",
      "step :  235 loss :  0.038427650928497314\n",
      "step :  236 loss :  0.03841802850365639\n",
      "step :  237 loss :  0.0384129174053669\n",
      "step :  238 loss :  0.03840828686952591\n",
      "step :  239 loss :  0.03840140253305435\n",
      "step :  240 loss :  0.038420964032411575\n",
      "step :  241 loss :  0.0384187214076519\n",
      "step :  242 loss :  0.03840888291597366\n",
      "step :  243 loss :  0.03840295225381851\n",
      "step :  244 loss :  0.03839634358882904\n",
      "step :  245 loss :  0.03839631378650665\n",
      "step :  246 loss :  0.03840559348464012\n",
      "step :  247 loss :  0.03839875012636185\n",
      "step :  248 loss :  0.0383935421705246\n",
      "step :  249 loss :  0.03839746490120888\n",
      "step :  250 loss :  0.03840037062764168\n",
      "step :  251 loss :  0.03839413821697235\n",
      "step :  252 loss :  0.038389746099710464\n",
      "step :  253 loss :  0.03840387612581253\n",
      "step :  254 loss :  0.038390498608350754\n",
      "step :  255 loss :  0.03838425874710083\n",
      "step :  256 loss :  0.038387008011341095\n",
      "step :  257 loss :  0.03839363530278206\n",
      "step :  258 loss :  0.038387760519981384\n",
      "step :  259 loss :  0.03838755190372467\n",
      "step :  260 loss :  0.038384467363357544\n",
      "step :  261 loss :  0.03839120268821716\n",
      "step :  262 loss :  0.03838145360350609\n",
      "step :  263 loss :  0.038379423320293427\n",
      "step :  264 loss :  0.0383821502327919\n",
      "step :  265 loss :  0.038378436118364334\n",
      "step :  266 loss :  0.03837623819708824\n",
      "step :  267 loss :  0.038388196378946304\n",
      "step :  268 loss :  0.03837494179606438\n",
      "step :  269 loss :  0.03837776184082031\n",
      "step :  270 loss :  0.038373060524463654\n",
      "step :  271 loss :  0.03837188705801964\n",
      "step :  272 loss :  0.03838355839252472\n",
      "step :  273 loss :  0.038372114300727844\n",
      "step :  274 loss :  0.03836571425199509\n",
      "step :  275 loss :  0.0383772999048233\n",
      "step :  276 loss :  0.03837371990084648\n",
      "step :  277 loss :  0.03836363926529884\n",
      "step :  278 loss :  0.03837376832962036\n",
      "step :  279 loss :  0.038377564400434494\n",
      "step :  280 loss :  0.038369569927453995\n",
      "step :  281 loss :  0.038366977125406265\n",
      "step :  282 loss :  0.038366470485925674\n",
      "step :  283 loss :  0.03836780786514282\n",
      "step :  284 loss :  0.03836655989289284\n",
      "step :  285 loss :  0.03836582601070404\n",
      "step :  286 loss :  0.038374267518520355\n",
      "step :  287 loss :  0.03836468234658241\n",
      "step :  288 loss :  0.03835773468017578\n",
      "step :  289 loss :  0.038366157561540604\n",
      "step :  290 loss :  0.038366567343473434\n",
      "step :  291 loss :  0.03835737332701683\n",
      "step :  292 loss :  0.03835891932249069\n",
      "step :  293 loss :  0.03836413845419884\n",
      "step :  294 loss :  0.03835771977901459\n",
      "step :  295 loss :  0.038359515368938446\n",
      "step :  296 loss :  0.038358986377716064\n",
      "step :  297 loss :  0.03835546597838402\n",
      "step :  298 loss :  0.038359154015779495\n",
      "step :  299 loss :  0.03836340829730034\n",
      "step :  300 loss :  0.0383552648127079\n",
      "step :  301 loss :  0.03835181146860123\n",
      "step :  302 loss :  0.03835660591721535\n",
      "step :  303 loss :  0.03836079686880112\n",
      "step :  304 loss :  0.03835335001349449\n",
      "step :  305 loss :  0.03835674002766609\n",
      "step :  306 loss :  0.03835684806108475\n",
      "step :  307 loss :  0.03835320845246315\n",
      "step :  308 loss :  0.03835359588265419\n",
      "step :  309 loss :  0.038354597985744476\n",
      "step :  310 loss :  0.0383523553609848\n",
      "step :  311 loss :  0.03835121914744377\n",
      "step :  312 loss :  0.03835274651646614\n",
      "step :  313 loss :  0.038354288786649704\n",
      "step :  314 loss :  0.03834851086139679\n",
      "step :  315 loss :  0.038349274545907974\n",
      "step :  316 loss :  0.038351260125637054\n",
      "step :  317 loss :  0.03834792226552963\n",
      "step :  318 loss :  0.03835095465183258\n",
      "step :  319 loss :  0.038349028676748276\n",
      "step :  320 loss :  0.0383487194776535\n",
      "step :  321 loss :  0.03835095837712288\n",
      "step :  322 loss :  0.03834695741534233\n",
      "step :  323 loss :  0.03834995627403259\n",
      "step :  324 loss :  0.038347456604242325\n",
      "step :  325 loss :  0.038347817957401276\n",
      "step :  326 loss :  0.03834819793701172\n",
      "step :  327 loss :  0.0383467823266983\n",
      "step :  328 loss :  0.038348276168107986\n",
      "step :  329 loss :  0.03834424912929535\n",
      "step :  330 loss :  0.038348011672496796\n",
      "step :  331 loss :  0.03834923356771469\n",
      "step :  332 loss :  0.03834504261612892\n",
      "step :  333 loss :  0.03834490850567818\n",
      "step :  334 loss :  0.03834877535700798\n",
      "step :  335 loss :  0.03834511712193489\n",
      "step :  336 loss :  0.038342900574207306\n",
      "step :  337 loss :  0.038344305008649826\n",
      "step :  338 loss :  0.038345806300640106\n",
      "step :  339 loss :  0.038343172520399094\n",
      "step :  340 loss :  0.038343001157045364\n",
      "step :  341 loss :  0.0383463017642498\n",
      "step :  342 loss :  0.03834475204348564\n",
      "step :  343 loss :  0.03834182769060135\n",
      "step :  344 loss :  0.03834456950426102\n",
      "step :  345 loss :  0.038343437016010284\n",
      "step :  346 loss :  0.03834204375743866\n",
      "step :  347 loss :  0.038343872874975204\n",
      "step :  348 loss :  0.03834213688969612\n",
      "step :  349 loss :  0.038342565298080444\n",
      "step :  350 loss :  0.038342274725437164\n",
      "step :  351 loss :  0.03834248706698418\n",
      "step :  352 loss :  0.03834150731563568\n",
      "step :  353 loss :  0.03834240138530731\n",
      "step :  354 loss :  0.03834083676338196\n",
      "step :  355 loss :  0.0383421927690506\n",
      "step :  356 loss :  0.03834114596247673\n",
      "step :  357 loss :  0.03834153711795807\n",
      "step :  358 loss :  0.03834089636802673\n",
      "step :  359 loss :  0.03834123536944389\n",
      "step :  360 loss :  0.03834014758467674\n",
      "step :  361 loss :  0.0383414663374424\n",
      "step :  362 loss :  0.0383402518928051\n",
      "step :  363 loss :  0.03834026679396629\n",
      "step :  364 loss :  0.03834041208028793\n",
      "step :  365 loss :  0.03834006190299988\n",
      "step :  366 loss :  0.03833979368209839\n",
      "step :  367 loss :  0.03834039717912674\n",
      "step :  368 loss :  0.03833926096558571\n",
      "step :  369 loss :  0.03833974897861481\n",
      "step :  370 loss :  0.038339562714099884\n",
      "step :  371 loss :  0.038339242339134216\n",
      "step :  372 loss :  0.03833959996700287\n",
      "step :  373 loss :  0.03833908960223198\n",
      "step :  374 loss :  0.038339763879776\n",
      "step :  375 loss :  0.03833872079849243\n",
      "step :  376 loss :  0.038340017199516296\n",
      "step :  377 loss :  0.03833843767642975\n",
      "step :  378 loss :  0.038338158279657364\n",
      "step :  379 loss :  0.038340527564287186\n",
      "step :  380 loss :  0.03834012150764465\n",
      "step :  381 loss :  0.038338154554367065\n",
      "step :  382 loss :  0.03833945468068123\n",
      "step :  383 loss :  0.038338229060173035\n",
      "step :  384 loss :  0.03833742067217827\n",
      "step :  385 loss :  0.03833967447280884\n",
      "step :  386 loss :  0.03833959996700287\n",
      "step :  387 loss :  0.03833777457475662\n",
      "step :  388 loss :  0.03833771497011185\n",
      "step :  389 loss :  0.03833867609500885\n",
      "step :  390 loss :  0.03833790495991707\n",
      "step :  391 loss :  0.038337331265211105\n",
      "step :  392 loss :  0.03833891823887825\n",
      "step :  393 loss :  0.038339197635650635\n",
      "step :  394 loss :  0.038337551057338715\n",
      "step :  395 loss :  0.03833635523915291\n",
      "step :  396 loss :  0.03833722695708275\n",
      "step :  397 loss :  0.038339801132678986\n",
      "step :  398 loss :  0.03833961859345436\n",
      "step :  399 loss :  0.03833780437707901\n",
      "step :  400 loss :  0.03833620995283127\n",
      "step :  401 loss :  0.03833666443824768\n",
      "step :  402 loss :  0.03833828493952751\n",
      "step :  403 loss :  0.038337722420692444\n",
      "step :  404 loss :  0.038336317986249924\n",
      "step :  405 loss :  0.03833683952689171\n",
      "step :  406 loss :  0.03833784908056259\n",
      "step :  407 loss :  0.03833697363734245\n",
      "step :  408 loss :  0.038336049765348434\n",
      "step :  409 loss :  0.03833692520856857\n",
      "step :  410 loss :  0.0383380725979805\n",
      "step :  411 loss :  0.038337238132953644\n",
      "step :  412 loss :  0.03833599016070366\n",
      "step :  413 loss :  0.038336701691150665\n",
      "step :  414 loss :  0.038337964564561844\n",
      "step :  415 loss :  0.038337353616952896\n",
      "step :  416 loss :  0.03833601996302605\n",
      "step :  417 loss :  0.038335755467414856\n",
      "step :  418 loss :  0.03833676129579544\n",
      "step :  419 loss :  0.03833678364753723\n",
      "step :  420 loss :  0.03833577409386635\n",
      "step :  421 loss :  0.03833550587296486\n",
      "step :  422 loss :  0.03833623602986336\n",
      "step :  423 loss :  0.038336653262376785\n",
      "step :  424 loss :  0.03833632171154022\n",
      "step :  425 loss :  0.03833601996302605\n",
      "step :  426 loss :  0.03833594173192978\n",
      "step :  427 loss :  0.03833582624793053\n",
      "step :  428 loss :  0.03833586722612381\n",
      "step :  429 loss :  0.0383358970284462\n",
      "step :  430 loss :  0.03833572193980217\n",
      "step :  431 loss :  0.03833562880754471\n",
      "step :  432 loss :  0.03833572566509247\n",
      "step :  433 loss :  0.03833587467670441\n",
      "step :  434 loss :  0.03833547607064247\n",
      "step :  435 loss :  0.03833581507205963\n",
      "step :  436 loss :  0.03833562880754471\n",
      "step :  437 loss :  0.03833537548780441\n",
      "step :  438 loss :  0.03833584859967232\n",
      "step :  439 loss :  0.03833575174212456\n",
      "step :  440 loss :  0.03833546116948128\n",
      "step :  441 loss :  0.038335610181093216\n",
      "step :  442 loss :  0.03833547607064247\n",
      "step :  443 loss :  0.03833557665348053\n",
      "step :  444 loss :  0.03833558037877083\n",
      "step :  445 loss :  0.03833528980612755\n",
      "step :  446 loss :  0.038335319608449936\n",
      "step :  447 loss :  0.038335829973220825\n",
      "step :  448 loss :  0.03833579644560814\n",
      "step :  449 loss :  0.0383354090154171\n",
      "step :  450 loss :  0.03833519667387009\n",
      "step :  451 loss :  0.03833514451980591\n",
      "step :  452 loss :  0.03833539038896561\n",
      "step :  453 loss :  0.038335349410772324\n",
      "step :  454 loss :  0.03833521157503128\n",
      "step :  455 loss :  0.038335077464580536\n",
      "step :  456 loss :  0.038335446268320084\n",
      "step :  457 loss :  0.03833550959825516\n",
      "step :  458 loss :  0.03833514079451561\n",
      "step :  459 loss :  0.038334909826517105\n",
      "step :  460 loss :  0.0383351594209671\n",
      "step :  461 loss :  0.038335129618644714\n",
      "step :  462 loss :  0.038335077464580536\n",
      "step :  463 loss :  0.038334935903549194\n",
      "step :  464 loss :  0.03833535313606262\n",
      "step :  465 loss :  0.038335420191287994\n",
      "step :  466 loss :  0.038335129618644714\n",
      "step :  467 loss :  0.038334861397743225\n",
      "step :  468 loss :  0.03833511471748352\n",
      "step :  469 loss :  0.03833521530032158\n",
      "step :  470 loss :  0.03833508864045143\n",
      "step :  471 loss :  0.038334921002388\n",
      "step :  472 loss :  0.03833493962883949\n",
      "step :  473 loss :  0.03833487257361412\n",
      "step :  474 loss :  0.0383351594209671\n",
      "step :  475 loss :  0.03833514079451561\n",
      "step :  476 loss :  0.03833487257361412\n",
      "step :  477 loss :  0.038334786891937256\n",
      "step :  478 loss :  0.03833511471748352\n",
      "step :  479 loss :  0.03833508864045143\n",
      "step :  480 loss :  0.03833478316664696\n",
      "step :  481 loss :  0.03833471238613129\n",
      "step :  482 loss :  0.0383351668715477\n",
      "step :  483 loss :  0.03833523020148277\n",
      "step :  484 loss :  0.03833494335412979\n",
      "step :  485 loss :  0.03833466395735741\n",
      "step :  486 loss :  0.03833475708961487\n",
      "step :  487 loss :  0.03833489492535591\n",
      "step :  488 loss :  0.03833480179309845\n",
      "step :  489 loss :  0.03833465278148651\n",
      "step :  490 loss :  0.038334742188453674\n",
      "step :  491 loss :  0.038334716111421585\n",
      "step :  492 loss :  0.038334865123033524\n",
      "step :  493 loss :  0.03833488002419472\n",
      "step :  494 loss :  0.038334738463163376\n",
      "step :  495 loss :  0.03833463788032532\n",
      "step :  496 loss :  0.03833477944135666\n",
      "step :  497 loss :  0.03833470866084099\n",
      "step :  498 loss :  0.038334596902132034\n",
      "step :  499 loss :  0.03833472356200218\n",
      "Training model 1\n",
      "step :  0 loss :  0.4486604928970337\n",
      "step :  1 loss :  0.21525521576404572\n",
      "step :  2 loss :  0.22286197543144226\n",
      "step :  3 loss :  0.2516583204269409\n",
      "step :  4 loss :  0.22154544293880463\n",
      "step :  5 loss :  0.18391025066375732\n",
      "step :  6 loss :  0.24161596596240997\n",
      "step :  7 loss :  0.2125444859266281\n",
      "step :  8 loss :  0.2167472392320633\n",
      "step :  9 loss :  0.1927158087491989\n",
      "step :  10 loss :  0.22462768852710724\n",
      "step :  11 loss :  0.19898656010627747\n",
      "step :  12 loss :  0.17563709616661072\n",
      "step :  13 loss :  0.16930019855499268\n",
      "step :  14 loss :  0.16372725367546082\n",
      "step :  15 loss :  0.15080967545509338\n",
      "step :  16 loss :  0.14962424337863922\n",
      "step :  17 loss :  0.12650266289710999\n",
      "step :  18 loss :  0.15965601801872253\n",
      "step :  19 loss :  0.12288662791252136\n",
      "step :  20 loss :  0.3497232496738434\n",
      "step :  21 loss :  0.12603230774402618\n",
      "step :  22 loss :  0.1253928244113922\n",
      "step :  23 loss :  0.19189687073230743\n",
      "step :  24 loss :  0.12259803712368011\n",
      "step :  25 loss :  0.11933320760726929\n",
      "step :  26 loss :  0.14457792043685913\n",
      "step :  27 loss :  0.11937867850065231\n",
      "step :  28 loss :  0.12657582759857178\n",
      "step :  29 loss :  0.11925022304058075\n",
      "step :  30 loss :  0.10915862023830414\n",
      "step :  31 loss :  0.10916727781295776\n",
      "step :  32 loss :  0.1091403141617775\n",
      "step :  33 loss :  0.10826416313648224\n",
      "step :  34 loss :  0.11219291388988495\n",
      "step :  35 loss :  0.10695130378007889\n",
      "step :  36 loss :  0.11093004792928696\n",
      "step :  37 loss :  0.10832159966230392\n",
      "step :  38 loss :  0.10355298966169357\n",
      "step :  39 loss :  0.1250177025794983\n",
      "step :  40 loss :  0.1254548877477646\n",
      "step :  41 loss :  0.10153558850288391\n",
      "step :  42 loss :  0.10108932107686996\n",
      "step :  43 loss :  0.10947360843420029\n",
      "step :  44 loss :  0.10412135720252991\n",
      "step :  45 loss :  0.09922070801258087\n",
      "step :  46 loss :  0.10923238843679428\n",
      "step :  47 loss :  0.09558045864105225\n",
      "step :  48 loss :  0.09768547862768173\n",
      "step :  49 loss :  0.09566766768693924\n",
      "step :  50 loss :  0.09301812201738358\n",
      "step :  51 loss :  0.09555631875991821\n",
      "step :  52 loss :  0.0925232395529747\n",
      "step :  53 loss :  0.09717459976673126\n",
      "step :  54 loss :  0.0975656509399414\n",
      "step :  55 loss :  0.09655821323394775\n",
      "step :  56 loss :  0.09869519621133804\n",
      "step :  57 loss :  0.09424261748790741\n",
      "step :  58 loss :  0.10115759074687958\n",
      "step :  59 loss :  0.08843608945608139\n",
      "step :  60 loss :  0.09162587672472\n",
      "step :  61 loss :  0.09061772376298904\n",
      "step :  62 loss :  0.08669537305831909\n",
      "step :  63 loss :  0.08803915232419968\n",
      "step :  64 loss :  0.08856912702322006\n",
      "step :  65 loss :  0.10169728845357895\n",
      "step :  66 loss :  0.08548042178153992\n",
      "step :  67 loss :  0.08972010761499405\n",
      "step :  68 loss :  0.08458877354860306\n",
      "step :  69 loss :  0.08427651971578598\n",
      "step :  70 loss :  0.08746248483657837\n",
      "step :  71 loss :  0.08479445427656174\n",
      "step :  72 loss :  0.09815330058336258\n",
      "step :  73 loss :  0.08782393485307693\n",
      "step :  74 loss :  0.09359870105981827\n",
      "step :  75 loss :  0.08479581773281097\n",
      "step :  76 loss :  0.08548170328140259\n",
      "step :  77 loss :  0.08231261372566223\n",
      "step :  78 loss :  0.08922884613275528\n",
      "step :  79 loss :  0.08484510332345963\n",
      "step :  80 loss :  0.08987615257501602\n",
      "step :  81 loss :  0.09061956405639648\n",
      "step :  82 loss :  0.0829462930560112\n",
      "step :  83 loss :  0.08288560062646866\n",
      "step :  84 loss :  0.08283541351556778\n",
      "step :  85 loss :  0.08993309736251831\n",
      "step :  86 loss :  0.08541034162044525\n",
      "step :  87 loss :  0.08452733606100082\n",
      "step :  88 loss :  0.08380720764398575\n",
      "step :  89 loss :  0.08314650505781174\n",
      "step :  90 loss :  0.0840434804558754\n",
      "step :  91 loss :  0.08245155960321426\n",
      "step :  92 loss :  0.08567880094051361\n",
      "step :  93 loss :  0.08165188133716583\n",
      "step :  94 loss :  0.08116361498832703\n",
      "step :  95 loss :  0.08118219673633575\n",
      "step :  96 loss :  0.08073187619447708\n",
      "step :  97 loss :  0.08135423809289932\n",
      "step :  98 loss :  0.08186214417219162\n",
      "step :  99 loss :  0.08124592900276184\n",
      "step :  100 loss :  0.08262806385755539\n",
      "step :  101 loss :  0.08429113030433655\n",
      "step :  102 loss :  0.08193128556013107\n",
      "step :  103 loss :  0.0808623656630516\n",
      "step :  104 loss :  0.08066176623106003\n",
      "step :  105 loss :  0.08083339780569077\n",
      "step :  106 loss :  0.08024688065052032\n",
      "step :  107 loss :  0.08046060800552368\n",
      "step :  108 loss :  0.08006811141967773\n",
      "step :  109 loss :  0.0808238610625267\n",
      "step :  110 loss :  0.08021850883960724\n",
      "step :  111 loss :  0.08088900148868561\n",
      "step :  112 loss :  0.08004169911146164\n",
      "step :  113 loss :  0.08135109394788742\n",
      "step :  114 loss :  0.0801045298576355\n",
      "step :  115 loss :  0.07989463955163956\n",
      "step :  116 loss :  0.08061826974153519\n",
      "step :  117 loss :  0.07984139025211334\n",
      "step :  118 loss :  0.08001702278852463\n",
      "step :  119 loss :  0.07983258366584778\n",
      "step :  120 loss :  0.07967325299978256\n",
      "step :  121 loss :  0.08058218657970428\n",
      "step :  122 loss :  0.07963301241397858\n",
      "step :  123 loss :  0.07963285595178604\n",
      "step :  124 loss :  0.08007655292749405\n",
      "step :  125 loss :  0.08055663853883743\n",
      "step :  126 loss :  0.08010171353816986\n",
      "step :  127 loss :  0.07966989278793335\n",
      "step :  128 loss :  0.07948355376720428\n",
      "step :  129 loss :  0.07944910228252411\n",
      "step :  130 loss :  0.07988668233156204\n",
      "step :  131 loss :  0.07938788831233978\n",
      "step :  132 loss :  0.07956720143556595\n",
      "step :  133 loss :  0.07935260236263275\n",
      "step :  134 loss :  0.07933500409126282\n",
      "step :  135 loss :  0.08026237785816193\n",
      "step :  136 loss :  0.07959049195051193\n",
      "step :  137 loss :  0.08048810064792633\n",
      "step :  138 loss :  0.07972020655870438\n",
      "step :  139 loss :  0.07969152927398682\n",
      "step :  140 loss :  0.07956146448850632\n",
      "step :  141 loss :  0.07932039350271225\n",
      "step :  142 loss :  0.07927034050226212\n",
      "step :  143 loss :  0.07949855923652649\n",
      "step :  144 loss :  0.07937049120664597\n",
      "step :  145 loss :  0.07926778495311737\n",
      "step :  146 loss :  0.07917386293411255\n",
      "step :  147 loss :  0.07915576547384262\n",
      "step :  148 loss :  0.07929380983114243\n",
      "step :  149 loss :  0.07969360798597336\n",
      "step :  150 loss :  0.07918718457221985\n",
      "step :  151 loss :  0.07944953441619873\n",
      "step :  152 loss :  0.07916229963302612\n",
      "step :  153 loss :  0.07963687926530838\n",
      "step :  154 loss :  0.07924264669418335\n",
      "step :  155 loss :  0.07912720739841461\n",
      "step :  156 loss :  0.07914825528860092\n",
      "step :  157 loss :  0.0794392004609108\n",
      "step :  158 loss :  0.07948744297027588\n",
      "step :  159 loss :  0.07910718768835068\n",
      "step :  160 loss :  0.07927633076906204\n",
      "step :  161 loss :  0.0791451707482338\n",
      "step :  162 loss :  0.07903110980987549\n",
      "step :  163 loss :  0.07918772846460342\n",
      "step :  164 loss :  0.07918135076761246\n",
      "step :  165 loss :  0.07908155769109726\n",
      "step :  166 loss :  0.07904088497161865\n",
      "step :  167 loss :  0.07903587073087692\n",
      "step :  168 loss :  0.07906157523393631\n",
      "step :  169 loss :  0.0791124477982521\n",
      "step :  170 loss :  0.07900092005729675\n",
      "step :  171 loss :  0.07907657325267792\n",
      "step :  172 loss :  0.07899151742458344\n",
      "step :  173 loss :  0.07912901043891907\n",
      "step :  174 loss :  0.07898351550102234\n",
      "step :  175 loss :  0.07920515537261963\n",
      "step :  176 loss :  0.0789993479847908\n",
      "step :  177 loss :  0.07908929884433746\n",
      "step :  178 loss :  0.07897564023733139\n",
      "step :  179 loss :  0.07895848900079727\n",
      "step :  180 loss :  0.0789647176861763\n",
      "step :  181 loss :  0.07904437929391861\n",
      "step :  182 loss :  0.07903508841991425\n",
      "step :  183 loss :  0.07897858321666718\n",
      "step :  184 loss :  0.07895469665527344\n",
      "step :  185 loss :  0.07899894565343857\n",
      "step :  186 loss :  0.07897236943244934\n",
      "step :  187 loss :  0.07935300469398499\n",
      "step :  188 loss :  0.07899557054042816\n",
      "step :  189 loss :  0.07924208790063858\n",
      "step :  190 loss :  0.07906132936477661\n",
      "step :  191 loss :  0.07907134294509888\n",
      "step :  192 loss :  0.07897669076919556\n",
      "step :  193 loss :  0.07898015528917313\n",
      "step :  194 loss :  0.07895047962665558\n",
      "step :  195 loss :  0.07898145914077759\n",
      "step :  196 loss :  0.07897573709487915\n",
      "step :  197 loss :  0.0789899080991745\n",
      "step :  198 loss :  0.07895652204751968\n",
      "step :  199 loss :  0.07900390774011612\n",
      "step :  200 loss :  0.0789414793252945\n",
      "step :  201 loss :  0.07899166643619537\n",
      "step :  202 loss :  0.0789402425289154\n",
      "step :  203 loss :  0.07908037304878235\n",
      "step :  204 loss :  0.07896844297647476\n",
      "step :  205 loss :  0.07902135699987411\n",
      "step :  206 loss :  0.07892020791769028\n",
      "step :  207 loss :  0.07898709177970886\n",
      "step :  208 loss :  0.07893139123916626\n",
      "step :  209 loss :  0.07893231511116028\n",
      "step :  210 loss :  0.07891937345266342\n",
      "step :  211 loss :  0.07897676527500153\n",
      "step :  212 loss :  0.07892180234193802\n",
      "step :  213 loss :  0.07905413955450058\n",
      "step :  214 loss :  0.07891380786895752\n",
      "step :  215 loss :  0.07895362377166748\n",
      "step :  216 loss :  0.07892567664384842\n",
      "step :  217 loss :  0.07892048358917236\n",
      "step :  218 loss :  0.07899900525808334\n",
      "step :  219 loss :  0.07891476154327393\n",
      "step :  220 loss :  0.07901497185230255\n",
      "step :  221 loss :  0.0789545550942421\n",
      "step :  222 loss :  0.07892035692930222\n",
      "step :  223 loss :  0.07899805903434753\n",
      "step :  224 loss :  0.07891625910997391\n",
      "step :  225 loss :  0.0789276510477066\n",
      "step :  226 loss :  0.07891356945037842\n",
      "step :  227 loss :  0.07894827425479889\n",
      "step :  228 loss :  0.07890883833169937\n",
      "step :  229 loss :  0.07901062071323395\n",
      "step :  230 loss :  0.07893228530883789\n",
      "step :  231 loss :  0.07891107350587845\n",
      "step :  232 loss :  0.07910095155239105\n",
      "step :  233 loss :  0.07892604917287827\n",
      "step :  234 loss :  0.0789044052362442\n",
      "step :  235 loss :  0.07903389632701874\n",
      "step :  236 loss :  0.07893238961696625\n",
      "step :  237 loss :  0.07889897376298904\n",
      "step :  238 loss :  0.07901165634393692\n",
      "step :  239 loss :  0.07893574237823486\n",
      "step :  240 loss :  0.07889816910028458\n",
      "step :  241 loss :  0.07900561392307281\n",
      "step :  242 loss :  0.07893556356430054\n",
      "step :  243 loss :  0.07889501005411148\n",
      "step :  244 loss :  0.07900317758321762\n",
      "step :  245 loss :  0.07893453538417816\n",
      "step :  246 loss :  0.0788942351937294\n",
      "step :  247 loss :  0.07898645848035812\n",
      "step :  248 loss :  0.07893569767475128\n",
      "step :  249 loss :  0.07889372855424881\n",
      "step :  250 loss :  0.07901918143033981\n",
      "step :  251 loss :  0.07895378768444061\n",
      "step :  252 loss :  0.0789048969745636\n",
      "step :  253 loss :  0.07896807789802551\n",
      "step :  254 loss :  0.07894771546125412\n",
      "step :  255 loss :  0.07890091836452484\n",
      "step :  256 loss :  0.0789114236831665\n",
      "step :  257 loss :  0.07891079783439636\n",
      "step :  258 loss :  0.07891394942998886\n",
      "step :  259 loss :  0.07889989763498306\n",
      "step :  260 loss :  0.078908272087574\n",
      "step :  261 loss :  0.07890663295984268\n",
      "step :  262 loss :  0.07890328019857407\n",
      "step :  263 loss :  0.07891636341810226\n",
      "step :  264 loss :  0.07890117913484573\n",
      "step :  265 loss :  0.07890598475933075\n",
      "step :  266 loss :  0.07890696823596954\n",
      "step :  267 loss :  0.07890605926513672\n",
      "step :  268 loss :  0.07889612764120102\n",
      "step :  269 loss :  0.07895555347204208\n",
      "step :  270 loss :  0.07893842458724976\n",
      "step :  271 loss :  0.07890792936086655\n",
      "step :  272 loss :  0.07893697172403336\n",
      "step :  273 loss :  0.07892289012670517\n",
      "step :  274 loss :  0.07889554649591446\n",
      "step :  275 loss :  0.07893572747707367\n",
      "step :  276 loss :  0.0789140984416008\n",
      "step :  277 loss :  0.07889586687088013\n",
      "step :  278 loss :  0.07893850654363632\n",
      "step :  279 loss :  0.07892996817827225\n",
      "step :  280 loss :  0.07889971137046814\n",
      "step :  281 loss :  0.07891009747982025\n",
      "step :  282 loss :  0.07890596240758896\n",
      "step :  283 loss :  0.07890941947698593\n",
      "step :  284 loss :  0.07891031354665756\n",
      "step :  285 loss :  0.07890508323907852\n",
      "step :  286 loss :  0.07890881597995758\n",
      "step :  287 loss :  0.0789041593670845\n",
      "step :  288 loss :  0.07890539616346359\n",
      "step :  289 loss :  0.07890981435775757\n",
      "step :  290 loss :  0.07890136539936066\n",
      "step :  291 loss :  0.07890863716602325\n",
      "step :  292 loss :  0.07889821380376816\n",
      "step :  293 loss :  0.07890323549509048\n",
      "step :  294 loss :  0.0788993388414383\n",
      "step :  295 loss :  0.07890558987855911\n",
      "step :  296 loss :  0.07890059798955917\n",
      "step :  297 loss :  0.07889988273382187\n",
      "step :  298 loss :  0.07890917360782623\n",
      "step :  299 loss :  0.07889887690544128\n",
      "step :  300 loss :  0.07890419661998749\n",
      "step :  301 loss :  0.07890920341014862\n",
      "step :  302 loss :  0.07889603823423386\n",
      "step :  303 loss :  0.078901007771492\n",
      "step :  304 loss :  0.07891041785478592\n",
      "step :  305 loss :  0.07889513671398163\n",
      "step :  306 loss :  0.07890010625123978\n",
      "step :  307 loss :  0.07890313118696213\n",
      "step :  308 loss :  0.07889431715011597\n",
      "step :  309 loss :  0.07889872789382935\n",
      "step :  310 loss :  0.07889735698699951\n",
      "step :  311 loss :  0.07889453321695328\n",
      "step :  312 loss :  0.07889890670776367\n",
      "step :  313 loss :  0.07889508455991745\n",
      "step :  314 loss :  0.0788974016904831\n",
      "step :  315 loss :  0.07889803498983383\n",
      "step :  316 loss :  0.07889451831579208\n",
      "step :  317 loss :  0.07889843732118607\n",
      "step :  318 loss :  0.07889564335346222\n",
      "step :  319 loss :  0.07889661937952042\n",
      "step :  320 loss :  0.07890477031469345\n",
      "step :  321 loss :  0.07889849692583084\n",
      "step :  322 loss :  0.07889434695243835\n",
      "step :  323 loss :  0.07889443635940552\n",
      "step :  324 loss :  0.07889522612094879\n",
      "step :  325 loss :  0.0788969174027443\n",
      "step :  326 loss :  0.07890024036169052\n",
      "step :  327 loss :  0.07889868319034576\n",
      "step :  328 loss :  0.07889658957719803\n",
      "step :  329 loss :  0.07890394330024719\n",
      "step :  330 loss :  0.07889483124017715\n",
      "step :  331 loss :  0.07889334112405777\n",
      "step :  332 loss :  0.07889792323112488\n",
      "step :  333 loss :  0.07889313995838165\n",
      "step :  334 loss :  0.0788939818739891\n",
      "step :  335 loss :  0.07889281213283539\n",
      "step :  336 loss :  0.07889749854803085\n",
      "step :  337 loss :  0.07889556139707565\n",
      "step :  338 loss :  0.07889274507761002\n",
      "step :  339 loss :  0.07889401912689209\n",
      "step :  340 loss :  0.07889414578676224\n",
      "step :  341 loss :  0.0788930281996727\n",
      "step :  342 loss :  0.0788942500948906\n",
      "step :  343 loss :  0.07889527827501297\n",
      "step :  344 loss :  0.07889347523450851\n",
      "step :  345 loss :  0.07889149338006973\n",
      "step :  346 loss :  0.07889462262392044\n",
      "step :  347 loss :  0.07889271527528763\n",
      "step :  348 loss :  0.07889121770858765\n",
      "step :  349 loss :  0.0788925364613533\n",
      "step :  350 loss :  0.07889246195554733\n",
      "step :  351 loss :  0.07889292389154434\n",
      "step :  352 loss :  0.07889385521411896\n",
      "step :  353 loss :  0.07889169454574585\n",
      "step :  354 loss :  0.0788910835981369\n",
      "step :  355 loss :  0.07889626175165176\n",
      "step :  356 loss :  0.0788927674293518\n",
      "step :  357 loss :  0.07889123260974884\n",
      "step :  358 loss :  0.07889505475759506\n",
      "step :  359 loss :  0.07889369130134583\n",
      "step :  360 loss :  0.07889021188020706\n",
      "step :  361 loss :  0.07889208942651749\n",
      "step :  362 loss :  0.07889622449874878\n",
      "step :  363 loss :  0.07889211177825928\n",
      "step :  364 loss :  0.07888998091220856\n",
      "step :  365 loss :  0.07889164239168167\n",
      "step :  366 loss :  0.0788923054933548\n",
      "step :  367 loss :  0.07889179140329361\n",
      "step :  368 loss :  0.07889091223478317\n",
      "step :  369 loss :  0.07889214903116226\n",
      "step :  370 loss :  0.07889105379581451\n",
      "step :  371 loss :  0.07889058440923691\n",
      "step :  372 loss :  0.07889370620250702\n",
      "step :  373 loss :  0.07889270782470703\n",
      "step :  374 loss :  0.07889050990343094\n",
      "step :  375 loss :  0.07889069616794586\n",
      "step :  376 loss :  0.07889346778392792\n",
      "step :  377 loss :  0.0788930356502533\n",
      "step :  378 loss :  0.07889114320278168\n",
      "step :  379 loss :  0.07889048010110855\n",
      "step :  380 loss :  0.07889310270547867\n",
      "step :  381 loss :  0.07889226824045181\n",
      "step :  382 loss :  0.0788908377289772\n",
      "step :  383 loss :  0.07889082282781601\n",
      "step :  384 loss :  0.07889273017644882\n",
      "step :  385 loss :  0.07889244705438614\n",
      "step :  386 loss :  0.07889150083065033\n",
      "step :  387 loss :  0.07889042794704437\n",
      "step :  388 loss :  0.07889073342084885\n",
      "step :  389 loss :  0.0788915827870369\n",
      "step :  390 loss :  0.07889129966497421\n",
      "step :  391 loss :  0.07889066636562347\n",
      "step :  392 loss :  0.07889031618833542\n",
      "step :  393 loss :  0.07889077067375183\n",
      "step :  394 loss :  0.0788906142115593\n",
      "step :  395 loss :  0.07889033108949661\n",
      "step :  396 loss :  0.07889065891504288\n",
      "step :  397 loss :  0.07889164239168167\n",
      "step :  398 loss :  0.07889094203710556\n",
      "step :  399 loss :  0.07889000326395035\n",
      "step :  400 loss :  0.07889046519994736\n",
      "step :  401 loss :  0.0788905918598175\n",
      "step :  402 loss :  0.07889029383659363\n",
      "step :  403 loss :  0.07889065146446228\n",
      "step :  404 loss :  0.07889068871736526\n",
      "step :  405 loss :  0.0788901299238205\n",
      "step :  406 loss :  0.07889044284820557\n",
      "step :  407 loss :  0.07888959348201752\n",
      "step :  408 loss :  0.07888969779014587\n",
      "step :  409 loss :  0.07888970524072647\n",
      "step :  410 loss :  0.078891322016716\n",
      "step :  411 loss :  0.07889093458652496\n",
      "step :  412 loss :  0.07888984680175781\n",
      "step :  413 loss :  0.07888948172330856\n",
      "step :  414 loss :  0.07889153063297272\n",
      "step :  415 loss :  0.07889138907194138\n",
      "step :  416 loss :  0.07889064401388168\n",
      "step :  417 loss :  0.07889042049646378\n",
      "step :  418 loss :  0.07888985425233841\n",
      "step :  419 loss :  0.07888984680175781\n",
      "step :  420 loss :  0.07888956367969513\n",
      "step :  421 loss :  0.07888983935117722\n",
      "step :  422 loss :  0.07889086753129959\n",
      "step :  423 loss :  0.07889080047607422\n",
      "step :  424 loss :  0.07888981699943542\n",
      "step :  425 loss :  0.07888998091220856\n",
      "step :  426 loss :  0.07889080047607422\n",
      "step :  427 loss :  0.07889039814472198\n",
      "step :  428 loss :  0.07888991385698318\n",
      "step :  429 loss :  0.07888974994421005\n",
      "step :  430 loss :  0.07888999581336975\n",
      "step :  431 loss :  0.07888965308666229\n",
      "step :  432 loss :  0.07888934761285782\n",
      "step :  433 loss :  0.07889000326395035\n",
      "step :  434 loss :  0.07888980954885483\n",
      "step :  435 loss :  0.07888982445001602\n",
      "step :  436 loss :  0.07888992875814438\n",
      "step :  437 loss :  0.07889005541801453\n",
      "step :  438 loss :  0.07888980954885483\n",
      "step :  439 loss :  0.07888991385698318\n",
      "step :  440 loss :  0.07888970524072647\n",
      "step :  441 loss :  0.07888941466808319\n",
      "step :  442 loss :  0.07889018952846527\n",
      "step :  443 loss :  0.07889009267091751\n",
      "step :  444 loss :  0.07888941466808319\n",
      "step :  445 loss :  0.0788896381855011\n",
      "step :  446 loss :  0.07888995856046677\n",
      "step :  447 loss :  0.07889014482498169\n",
      "step :  448 loss :  0.07888966798782349\n",
      "step :  449 loss :  0.07888929545879364\n",
      "step :  450 loss :  0.07888971269130707\n",
      "step :  451 loss :  0.0788898766040802\n",
      "step :  452 loss :  0.07888984680175781\n",
      "step :  453 loss :  0.07888970524072647\n",
      "step :  454 loss :  0.07888957858085632\n",
      "step :  455 loss :  0.07888951152563095\n",
      "step :  456 loss :  0.07888932526111603\n",
      "step :  457 loss :  0.07888967543840408\n",
      "step :  458 loss :  0.07888941466808319\n",
      "step :  459 loss :  0.07888931035995483\n",
      "step :  460 loss :  0.07888926565647125\n",
      "step :  461 loss :  0.07888971269130707\n",
      "step :  462 loss :  0.07888958603143692\n",
      "step :  463 loss :  0.0788893774151802\n",
      "step :  464 loss :  0.07888960093259811\n",
      "step :  465 loss :  0.07888970524072647\n",
      "step :  466 loss :  0.07888991385698318\n",
      "step :  467 loss :  0.07888990640640259\n",
      "step :  468 loss :  0.0788896381855011\n",
      "step :  469 loss :  0.07888951152563095\n",
      "step :  470 loss :  0.07888955622911453\n",
      "step :  471 loss :  0.07888951152563095\n",
      "step :  472 loss :  0.07888948172330856\n",
      "step :  473 loss :  0.07888956367969513\n",
      "step :  474 loss :  0.07888933271169662\n",
      "step :  475 loss :  0.07888934761285782\n",
      "step :  476 loss :  0.07888980954885483\n",
      "step :  477 loss :  0.07888974249362946\n",
      "step :  478 loss :  0.07888934761285782\n",
      "step :  479 loss :  0.07888910174369812\n",
      "step :  480 loss :  0.07888932526111603\n",
      "step :  481 loss :  0.07888984680175781\n",
      "step :  482 loss :  0.07888998091220856\n",
      "step :  483 loss :  0.07888974994421005\n",
      "step :  484 loss :  0.0788893774151802\n",
      "step :  485 loss :  0.07888925075531006\n",
      "step :  486 loss :  0.07888942956924438\n",
      "step :  487 loss :  0.07888932526111603\n",
      "step :  488 loss :  0.07888925075531006\n",
      "step :  489 loss :  0.07888928800821304\n",
      "step :  490 loss :  0.07888923585414886\n",
      "step :  491 loss :  0.07888922095298767\n",
      "step :  492 loss :  0.0788896232843399\n",
      "step :  493 loss :  0.07888959348201752\n",
      "step :  494 loss :  0.07888933271169662\n",
      "step :  495 loss :  0.07888930290937424\n",
      "step :  496 loss :  0.07888951152563095\n",
      "step :  497 loss :  0.07888951152563095\n",
      "step :  498 loss :  0.07888929545879364\n",
      "step :  499 loss :  0.07888926565647125\n",
      "Training model 2\n",
      "step :  0 loss :  0.43673384189605713\n",
      "step :  1 loss :  0.3266723155975342\n",
      "step :  2 loss :  0.20464257895946503\n",
      "step :  3 loss :  0.23490585386753082\n",
      "step :  4 loss :  0.1957853138446808\n",
      "step :  5 loss :  0.18072834610939026\n",
      "step :  6 loss :  0.22818219661712646\n",
      "step :  7 loss :  0.1999833732843399\n",
      "step :  8 loss :  0.1948050707578659\n",
      "step :  9 loss :  0.19432801008224487\n",
      "step :  10 loss :  0.21919618546962738\n",
      "step :  11 loss :  0.18274404108524323\n",
      "step :  12 loss :  0.18200485408306122\n",
      "step :  13 loss :  0.1715523898601532\n",
      "step :  14 loss :  0.16369539499282837\n",
      "step :  15 loss :  0.15095210075378418\n",
      "step :  16 loss :  0.13588222861289978\n",
      "step :  17 loss :  0.12313541024923325\n",
      "step :  18 loss :  0.11211412400007248\n",
      "step :  19 loss :  0.1843315064907074\n",
      "step :  20 loss :  0.1587388813495636\n",
      "step :  21 loss :  0.11572661995887756\n",
      "step :  22 loss :  0.11720794439315796\n",
      "step :  23 loss :  0.14355194568634033\n",
      "step :  24 loss :  0.11250393092632294\n",
      "step :  25 loss :  0.11157548427581787\n",
      "step :  26 loss :  0.10757461935281754\n",
      "step :  27 loss :  0.10577433556318283\n",
      "step :  28 loss :  0.11240824311971664\n",
      "step :  29 loss :  0.10558013617992401\n",
      "step :  30 loss :  0.10373316705226898\n",
      "step :  31 loss :  0.18633154034614563\n",
      "step :  32 loss :  0.10909358412027359\n",
      "step :  33 loss :  0.14890654385089874\n",
      "step :  34 loss :  0.09689399600028992\n",
      "step :  35 loss :  0.1023004874587059\n",
      "step :  36 loss :  0.09550335258245468\n",
      "step :  37 loss :  0.10276927053928375\n",
      "step :  38 loss :  0.10266317427158356\n",
      "step :  39 loss :  0.18326516449451447\n",
      "step :  40 loss :  0.08970194309949875\n",
      "step :  41 loss :  0.092432402074337\n",
      "step :  42 loss :  0.0963737815618515\n",
      "step :  43 loss :  0.09936141967773438\n",
      "step :  44 loss :  0.12299800664186478\n",
      "step :  45 loss :  0.08856755495071411\n",
      "step :  46 loss :  0.09844406694173813\n",
      "step :  47 loss :  0.08528024703264236\n",
      "step :  48 loss :  0.09228145331144333\n",
      "step :  49 loss :  0.09758762270212173\n",
      "step :  50 loss :  0.08971840888261795\n",
      "step :  51 loss :  0.11140510439872742\n",
      "step :  52 loss :  0.08669393509626389\n",
      "step :  53 loss :  0.09590887278318405\n",
      "step :  54 loss :  0.08358022570610046\n",
      "step :  55 loss :  0.08791109919548035\n",
      "step :  56 loss :  0.09635517746210098\n",
      "step :  57 loss :  0.08525635302066803\n",
      "step :  58 loss :  0.09073073416948318\n",
      "step :  59 loss :  0.10670560598373413\n",
      "step :  60 loss :  0.09010567516088486\n",
      "step :  61 loss :  0.09063877910375595\n",
      "step :  62 loss :  0.08239644765853882\n",
      "step :  63 loss :  0.08630870282649994\n",
      "step :  64 loss :  0.08703015744686127\n",
      "step :  65 loss :  0.08605684340000153\n",
      "step :  66 loss :  0.09036877006292343\n",
      "step :  67 loss :  0.08794734627008438\n",
      "step :  68 loss :  0.09778609871864319\n",
      "step :  69 loss :  0.08300024271011353\n",
      "step :  70 loss :  0.08127227425575256\n",
      "step :  71 loss :  0.08218555152416229\n",
      "step :  72 loss :  0.08197542279958725\n",
      "step :  73 loss :  0.08175678551197052\n",
      "step :  74 loss :  0.08700554072856903\n",
      "step :  75 loss :  0.08809871226549149\n",
      "step :  76 loss :  0.09104644507169724\n",
      "step :  77 loss :  0.08656542003154755\n",
      "step :  78 loss :  0.08738210052251816\n",
      "step :  79 loss :  0.0816255733370781\n",
      "step :  80 loss :  0.08252488821744919\n",
      "step :  81 loss :  0.08164183050394058\n",
      "step :  82 loss :  0.08364453911781311\n",
      "step :  83 loss :  0.08454298228025436\n",
      "step :  84 loss :  0.0846998542547226\n",
      "step :  85 loss :  0.08380291610956192\n",
      "step :  86 loss :  0.08072344213724136\n",
      "step :  87 loss :  0.08419857919216156\n",
      "step :  88 loss :  0.0817023515701294\n",
      "step :  89 loss :  0.08115236461162567\n",
      "step :  90 loss :  0.08525102585554123\n",
      "step :  91 loss :  0.08028107136487961\n",
      "step :  92 loss :  0.08119518309831619\n",
      "step :  93 loss :  0.08060529083013535\n",
      "step :  94 loss :  0.08110550045967102\n",
      "step :  95 loss :  0.08030197769403458\n",
      "step :  96 loss :  0.08089579641819\n",
      "step :  97 loss :  0.08241274207830429\n",
      "step :  98 loss :  0.080205999314785\n",
      "step :  99 loss :  0.08146270364522934\n",
      "step :  100 loss :  0.07980470359325409\n",
      "step :  101 loss :  0.08327226340770721\n",
      "step :  102 loss :  0.0801917091012001\n",
      "step :  103 loss :  0.08068270981311798\n",
      "step :  104 loss :  0.08161545544862747\n",
      "step :  105 loss :  0.0803152322769165\n",
      "step :  106 loss :  0.07987800240516663\n",
      "step :  107 loss :  0.08029501885175705\n",
      "step :  108 loss :  0.08227834850549698\n",
      "step :  109 loss :  0.08001165091991425\n",
      "step :  110 loss :  0.08129061758518219\n",
      "step :  111 loss :  0.08310354501008987\n",
      "step :  112 loss :  0.08034273982048035\n",
      "step :  113 loss :  0.07947785407304764\n",
      "step :  114 loss :  0.08028054982423782\n",
      "step :  115 loss :  0.07939961552619934\n",
      "step :  116 loss :  0.07936076819896698\n",
      "step :  117 loss :  0.07930412143468857\n",
      "step :  118 loss :  0.08038397133350372\n",
      "step :  119 loss :  0.07991666346788406\n",
      "step :  120 loss :  0.0792713537812233\n",
      "step :  121 loss :  0.07917634397745132\n",
      "step :  122 loss :  0.07945053279399872\n",
      "step :  123 loss :  0.07967416197061539\n",
      "step :  124 loss :  0.07924182713031769\n",
      "step :  125 loss :  0.07908119261264801\n",
      "step :  126 loss :  0.07924528419971466\n",
      "step :  127 loss :  0.07901003211736679\n",
      "step :  128 loss :  0.07914286851882935\n",
      "step :  129 loss :  0.07909607142210007\n",
      "step :  130 loss :  0.07935523986816406\n",
      "step :  131 loss :  0.07896915823221207\n",
      "step :  132 loss :  0.07892002910375595\n",
      "step :  133 loss :  0.07894974201917648\n",
      "step :  134 loss :  0.07922878116369247\n",
      "step :  135 loss :  0.07937322556972504\n",
      "step :  136 loss :  0.07940599322319031\n",
      "step :  137 loss :  0.07910601049661636\n",
      "step :  138 loss :  0.07883920520544052\n",
      "step :  139 loss :  0.07882250100374222\n",
      "step :  140 loss :  0.07880918681621552\n",
      "step :  141 loss :  0.07905847579240799\n",
      "step :  142 loss :  0.07884315401315689\n",
      "step :  143 loss :  0.07888728380203247\n",
      "step :  144 loss :  0.07897832244634628\n",
      "step :  145 loss :  0.07875090092420578\n",
      "step :  146 loss :  0.07872794568538666\n",
      "step :  147 loss :  0.07878650724887848\n",
      "step :  148 loss :  0.07875896990299225\n",
      "step :  149 loss :  0.07883408665657043\n",
      "step :  150 loss :  0.07873237133026123\n",
      "step :  151 loss :  0.07866068184375763\n",
      "step :  152 loss :  0.07867622375488281\n",
      "step :  153 loss :  0.07872948050498962\n",
      "step :  154 loss :  0.07865316420793533\n",
      "step :  155 loss :  0.07861541211605072\n",
      "step :  156 loss :  0.0786639079451561\n",
      "step :  157 loss :  0.07867126166820526\n",
      "step :  158 loss :  0.07861002534627914\n",
      "step :  159 loss :  0.07857612520456314\n",
      "step :  160 loss :  0.07857261598110199\n",
      "step :  161 loss :  0.07874155044555664\n",
      "step :  162 loss :  0.0786258652806282\n",
      "step :  163 loss :  0.07859031111001968\n",
      "step :  164 loss :  0.07866451889276505\n",
      "step :  165 loss :  0.07858970016241074\n",
      "step :  166 loss :  0.07855454832315445\n",
      "step :  167 loss :  0.0785621628165245\n",
      "step :  168 loss :  0.0785529837012291\n",
      "step :  169 loss :  0.07856173813343048\n",
      "step :  170 loss :  0.07853428274393082\n",
      "step :  171 loss :  0.07854671031236649\n",
      "step :  172 loss :  0.07850008457899094\n",
      "step :  173 loss :  0.07849638909101486\n",
      "step :  174 loss :  0.07848289608955383\n",
      "step :  175 loss :  0.07856401056051254\n",
      "step :  176 loss :  0.0784689411520958\n",
      "step :  177 loss :  0.07845284789800644\n",
      "step :  178 loss :  0.07851497083902359\n",
      "step :  179 loss :  0.07850213348865509\n",
      "step :  180 loss :  0.0784495398402214\n",
      "step :  181 loss :  0.07846838980913162\n",
      "step :  182 loss :  0.07849767059087753\n",
      "step :  183 loss :  0.07842915505170822\n",
      "step :  184 loss :  0.07848376035690308\n",
      "step :  185 loss :  0.07844748347997665\n",
      "step :  186 loss :  0.0784924179315567\n",
      "step :  187 loss :  0.07842013239860535\n",
      "step :  188 loss :  0.0784243568778038\n",
      "step :  189 loss :  0.0784425362944603\n",
      "step :  190 loss :  0.07841551303863525\n",
      "step :  191 loss :  0.07841652631759644\n",
      "step :  192 loss :  0.07842616736888885\n",
      "step :  193 loss :  0.07844161987304688\n",
      "step :  194 loss :  0.07840291410684586\n",
      "step :  195 loss :  0.0784263014793396\n",
      "step :  196 loss :  0.07840612530708313\n",
      "step :  197 loss :  0.07839842885732651\n",
      "step :  198 loss :  0.07838473469018936\n",
      "step :  199 loss :  0.07840513437986374\n",
      "step :  200 loss :  0.07841195911169052\n",
      "step :  201 loss :  0.0783720389008522\n",
      "step :  202 loss :  0.07837007194757462\n",
      "step :  203 loss :  0.07842881977558136\n",
      "step :  204 loss :  0.0783717930316925\n",
      "step :  205 loss :  0.07835850119590759\n",
      "step :  206 loss :  0.07837178558111191\n",
      "step :  207 loss :  0.07835520803928375\n",
      "step :  208 loss :  0.07835039496421814\n",
      "step :  209 loss :  0.07836229354143143\n",
      "step :  210 loss :  0.07836276292800903\n",
      "step :  211 loss :  0.0783403217792511\n",
      "step :  212 loss :  0.0783616155385971\n",
      "step :  213 loss :  0.07833760976791382\n",
      "step :  214 loss :  0.07834441214799881\n",
      "step :  215 loss :  0.0783340260386467\n",
      "step :  216 loss :  0.07838372886180878\n",
      "step :  217 loss :  0.07834120094776154\n",
      "step :  218 loss :  0.07832822948694229\n",
      "step :  219 loss :  0.07832436263561249\n",
      "step :  220 loss :  0.07832689583301544\n",
      "step :  221 loss :  0.07832508534193039\n",
      "step :  222 loss :  0.07834906876087189\n",
      "step :  223 loss :  0.07833769917488098\n",
      "step :  224 loss :  0.07831230014562607\n",
      "step :  225 loss :  0.07832132279872894\n",
      "step :  226 loss :  0.07831250131130219\n",
      "step :  227 loss :  0.07831573486328125\n",
      "step :  228 loss :  0.07830928266048431\n",
      "step :  229 loss :  0.07830610126256943\n",
      "step :  230 loss :  0.07831143587827682\n",
      "step :  231 loss :  0.07830429822206497\n",
      "step :  232 loss :  0.07830468565225601\n",
      "step :  233 loss :  0.07831443846225739\n",
      "step :  234 loss :  0.07831968367099762\n",
      "step :  235 loss :  0.07830122113227844\n",
      "step :  236 loss :  0.07832872867584229\n",
      "step :  237 loss :  0.07829882949590683\n",
      "step :  238 loss :  0.07830844074487686\n",
      "step :  239 loss :  0.07832945138216019\n",
      "step :  240 loss :  0.0783359557390213\n",
      "step :  241 loss :  0.07832271605730057\n",
      "step :  242 loss :  0.07831170409917831\n",
      "step :  243 loss :  0.07829178869724274\n",
      "step :  244 loss :  0.07830516248941422\n",
      "step :  245 loss :  0.07828714698553085\n",
      "step :  246 loss :  0.07828964293003082\n",
      "step :  247 loss :  0.07829213887453079\n",
      "step :  248 loss :  0.07829312980175018\n",
      "step :  249 loss :  0.0782836377620697\n",
      "step :  250 loss :  0.07828396558761597\n",
      "step :  251 loss :  0.0782814770936966\n",
      "step :  252 loss :  0.07827840000391006\n",
      "step :  253 loss :  0.07829301804304123\n",
      "step :  254 loss :  0.07828729599714279\n",
      "step :  255 loss :  0.07828102260828018\n",
      "step :  256 loss :  0.0782819390296936\n",
      "step :  257 loss :  0.07827438414096832\n",
      "step :  258 loss :  0.0782790556550026\n",
      "step :  259 loss :  0.07827632874250412\n",
      "step :  260 loss :  0.07827135175466537\n",
      "step :  261 loss :  0.07826972007751465\n",
      "step :  262 loss :  0.07827631384134293\n",
      "step :  263 loss :  0.07827156782150269\n",
      "step :  264 loss :  0.07827945053577423\n",
      "step :  265 loss :  0.07827609032392502\n",
      "step :  266 loss :  0.07827088236808777\n",
      "step :  267 loss :  0.07827799767255783\n",
      "step :  268 loss :  0.07826466858386993\n",
      "step :  269 loss :  0.07826346904039383\n",
      "step :  270 loss :  0.07827553898096085\n",
      "step :  271 loss :  0.07826770842075348\n",
      "step :  272 loss :  0.07826368510723114\n",
      "step :  273 loss :  0.0782623365521431\n",
      "step :  274 loss :  0.07826142013072968\n",
      "step :  275 loss :  0.0782601535320282\n",
      "step :  276 loss :  0.07826452702283859\n",
      "step :  277 loss :  0.07826406508684158\n",
      "step :  278 loss :  0.0782601460814476\n",
      "step :  279 loss :  0.078258216381073\n",
      "step :  280 loss :  0.07825707644224167\n",
      "step :  281 loss :  0.0782686099410057\n",
      "step :  282 loss :  0.0782562717795372\n",
      "step :  283 loss :  0.07825275510549545\n",
      "step :  284 loss :  0.07825793325901031\n",
      "step :  285 loss :  0.07825557142496109\n",
      "step :  286 loss :  0.07825250923633575\n",
      "step :  287 loss :  0.07825276255607605\n",
      "step :  288 loss :  0.07825303822755814\n",
      "step :  289 loss :  0.07825399190187454\n",
      "step :  290 loss :  0.07825236022472382\n",
      "step :  291 loss :  0.07825155556201935\n",
      "step :  292 loss :  0.07825125008821487\n",
      "step :  293 loss :  0.0782511830329895\n",
      "step :  294 loss :  0.07825170457363129\n",
      "step :  295 loss :  0.07825140655040741\n",
      "step :  296 loss :  0.07824928313493729\n",
      "step :  297 loss :  0.07824791222810745\n",
      "step :  298 loss :  0.0782506912946701\n",
      "step :  299 loss :  0.07824788242578506\n",
      "step :  300 loss :  0.07824770361185074\n",
      "step :  301 loss :  0.07824692875146866\n",
      "step :  302 loss :  0.07824547588825226\n",
      "step :  303 loss :  0.07824643701314926\n",
      "step :  304 loss :  0.07824485749006271\n",
      "step :  305 loss :  0.07824713736772537\n",
      "step :  306 loss :  0.07824652642011642\n",
      "step :  307 loss :  0.07824496924877167\n",
      "step :  308 loss :  0.0782436728477478\n",
      "step :  309 loss :  0.07824520021677017\n",
      "step :  310 loss :  0.07824378460645676\n",
      "step :  311 loss :  0.07824471592903137\n",
      "step :  312 loss :  0.07824283093214035\n",
      "step :  313 loss :  0.0782424733042717\n",
      "step :  314 loss :  0.07824135571718216\n",
      "step :  315 loss :  0.07824255526065826\n",
      "step :  316 loss :  0.07824060320854187\n",
      "step :  317 loss :  0.07823959738016129\n",
      "step :  318 loss :  0.07824267446994781\n",
      "step :  319 loss :  0.07824015617370605\n",
      "step :  320 loss :  0.07823967188596725\n",
      "step :  321 loss :  0.07824026793241501\n",
      "step :  322 loss :  0.07823839783668518\n",
      "step :  323 loss :  0.07823876291513443\n",
      "step :  324 loss :  0.07824014127254486\n",
      "step :  325 loss :  0.07823904603719711\n",
      "step :  326 loss :  0.07824280858039856\n",
      "step :  327 loss :  0.07823831588029861\n",
      "step :  328 loss :  0.07823850959539413\n",
      "step :  329 loss :  0.07823626697063446\n",
      "step :  330 loss :  0.07823598384857178\n",
      "step :  331 loss :  0.07823652029037476\n",
      "step :  332 loss :  0.07823539525270462\n",
      "step :  333 loss :  0.07823862880468369\n",
      "step :  334 loss :  0.07823493331670761\n",
      "step :  335 loss :  0.07823432981967926\n",
      "step :  336 loss :  0.07823389023542404\n",
      "step :  337 loss :  0.07823438942432404\n",
      "step :  338 loss :  0.0782339796423912\n",
      "step :  339 loss :  0.0782332643866539\n",
      "step :  340 loss :  0.07823368906974792\n",
      "step :  341 loss :  0.07823379337787628\n",
      "step :  342 loss :  0.07823358476161957\n",
      "step :  343 loss :  0.07823262363672256\n",
      "step :  344 loss :  0.07823321223258972\n",
      "step :  345 loss :  0.07823365926742554\n",
      "step :  346 loss :  0.07823299616575241\n",
      "step :  347 loss :  0.07823330909013748\n",
      "step :  348 loss :  0.07823286205530167\n",
      "step :  349 loss :  0.07823227345943451\n",
      "step :  350 loss :  0.07823235541582108\n",
      "step :  351 loss :  0.07823178917169571\n",
      "step :  352 loss :  0.07823224365711212\n",
      "step :  353 loss :  0.07823124527931213\n",
      "step :  354 loss :  0.07823149114847183\n",
      "step :  355 loss :  0.07823118567466736\n",
      "step :  356 loss :  0.0782308354973793\n",
      "step :  357 loss :  0.07823101431131363\n",
      "step :  358 loss :  0.07823046296834946\n",
      "step :  359 loss :  0.0782301276922226\n",
      "step :  360 loss :  0.07823016494512558\n",
      "step :  361 loss :  0.07822991162538528\n",
      "step :  362 loss :  0.07823020964860916\n",
      "step :  363 loss :  0.07822984457015991\n",
      "step :  364 loss :  0.07822936028242111\n",
      "step :  365 loss :  0.07822955399751663\n",
      "step :  366 loss :  0.0782293975353241\n",
      "step :  367 loss :  0.07822945713996887\n",
      "step :  368 loss :  0.0782288983464241\n",
      "step :  369 loss :  0.07822862267494202\n",
      "step :  370 loss :  0.07822888344526291\n",
      "step :  371 loss :  0.07822874188423157\n",
      "step :  372 loss :  0.0782286673784256\n",
      "step :  373 loss :  0.07822868973016739\n",
      "step :  374 loss :  0.07822880148887634\n",
      "step :  375 loss :  0.07822886854410172\n",
      "step :  376 loss :  0.07822798937559128\n",
      "step :  377 loss :  0.07822831720113754\n",
      "step :  378 loss :  0.07822830229997635\n",
      "step :  379 loss :  0.07822784036397934\n",
      "step :  380 loss :  0.07822791486978531\n",
      "step :  381 loss :  0.07822807878255844\n",
      "step :  382 loss :  0.07822796702384949\n",
      "step :  383 loss :  0.0782276913523674\n",
      "step :  384 loss :  0.0782276913523674\n",
      "step :  385 loss :  0.0782277062535286\n",
      "step :  386 loss :  0.07822758704423904\n",
      "step :  387 loss :  0.07822693139314651\n",
      "step :  388 loss :  0.0782269760966301\n",
      "step :  389 loss :  0.0782269760966301\n",
      "step :  390 loss :  0.07822676002979279\n",
      "step :  391 loss :  0.07822682708501816\n",
      "step :  392 loss :  0.07822659611701965\n",
      "step :  393 loss :  0.07822737842798233\n",
      "step :  394 loss :  0.07822654396295547\n",
      "step :  395 loss :  0.07822635024785995\n",
      "step :  396 loss :  0.07822686433792114\n",
      "step :  397 loss :  0.07822630554437637\n",
      "step :  398 loss :  0.07822608947753906\n",
      "step :  399 loss :  0.07822662591934204\n",
      "step :  400 loss :  0.07822595536708832\n",
      "step :  401 loss :  0.07822580635547638\n",
      "step :  402 loss :  0.07822568714618683\n",
      "step :  403 loss :  0.07822597026824951\n",
      "step :  404 loss :  0.07822588086128235\n",
      "step :  405 loss :  0.07822569459676743\n",
      "step :  406 loss :  0.07822579890489578\n",
      "step :  407 loss :  0.07822577655315399\n",
      "step :  408 loss :  0.07822568714618683\n",
      "step :  409 loss :  0.07822546362876892\n",
      "step :  410 loss :  0.07822534441947937\n",
      "step :  411 loss :  0.07822541892528534\n",
      "step :  412 loss :  0.07822558283805847\n",
      "step :  413 loss :  0.07822585105895996\n",
      "step :  414 loss :  0.07822556793689728\n",
      "step :  415 loss :  0.0782255008816719\n",
      "step :  416 loss :  0.07822531461715698\n",
      "step :  417 loss :  0.07822522521018982\n",
      "step :  418 loss :  0.07822518050670624\n",
      "step :  419 loss :  0.0782250240445137\n",
      "step :  420 loss :  0.0782250314950943\n",
      "step :  421 loss :  0.07822483032941818\n",
      "step :  422 loss :  0.07822544127702713\n",
      "step :  423 loss :  0.07822465896606445\n",
      "step :  424 loss :  0.07822457700967789\n",
      "step :  425 loss :  0.07822451740503311\n",
      "step :  426 loss :  0.07822468876838684\n",
      "step :  427 loss :  0.07822452485561371\n",
      "step :  428 loss :  0.07822445034980774\n",
      "step :  429 loss :  0.07822475582361221\n",
      "step :  430 loss :  0.0782245546579361\n",
      "step :  431 loss :  0.07822465896606445\n",
      "step :  432 loss :  0.07822449505329132\n",
      "step :  433 loss :  0.07822436094284058\n",
      "step :  434 loss :  0.07822433859109879\n",
      "step :  435 loss :  0.07822436839342117\n",
      "step :  436 loss :  0.07822427898645401\n",
      "step :  437 loss :  0.07822459936141968\n",
      "step :  438 loss :  0.07822418212890625\n",
      "step :  439 loss :  0.07822418957948685\n",
      "step :  440 loss :  0.078224316239357\n",
      "step :  441 loss :  0.07822408527135849\n",
      "step :  442 loss :  0.0782240629196167\n",
      "step :  443 loss :  0.07822402566671371\n",
      "step :  444 loss :  0.07822395861148834\n",
      "step :  445 loss :  0.07822391390800476\n",
      "step :  446 loss :  0.07822389900684357\n",
      "step :  447 loss :  0.07822386175394058\n",
      "step :  448 loss :  0.07822384685277939\n",
      "step :  449 loss :  0.07822387665510178\n",
      "step :  450 loss :  0.0782238095998764\n",
      "step :  451 loss :  0.07822372764348984\n",
      "step :  452 loss :  0.07822377979755402\n",
      "step :  453 loss :  0.07822371274232864\n",
      "step :  454 loss :  0.07822367548942566\n",
      "step :  455 loss :  0.07822363823652267\n",
      "step :  456 loss :  0.07822364568710327\n",
      "step :  457 loss :  0.07822360843420029\n",
      "step :  458 loss :  0.07822376489639282\n",
      "step :  459 loss :  0.0782235711812973\n",
      "step :  460 loss :  0.07822354137897491\n",
      "step :  461 loss :  0.07822353392839432\n",
      "step :  462 loss :  0.0782235860824585\n",
      "step :  463 loss :  0.07822347432374954\n",
      "step :  464 loss :  0.07822362333536148\n",
      "step :  465 loss :  0.07822341471910477\n",
      "step :  466 loss :  0.07822340726852417\n",
      "step :  467 loss :  0.07822352647781372\n",
      "step :  468 loss :  0.07822345197200775\n",
      "step :  469 loss :  0.07822345197200775\n",
      "step :  470 loss :  0.07822336256504059\n",
      "step :  471 loss :  0.07822345942258835\n",
      "step :  472 loss :  0.0782233327627182\n",
      "step :  473 loss :  0.0782233476638794\n",
      "step :  474 loss :  0.07822343707084656\n",
      "step :  475 loss :  0.07822327315807343\n",
      "step :  476 loss :  0.07822319865226746\n",
      "step :  477 loss :  0.07822331041097641\n",
      "step :  478 loss :  0.07822319120168686\n",
      "step :  479 loss :  0.07822318375110626\n",
      "step :  480 loss :  0.07822315394878387\n",
      "step :  481 loss :  0.07822311669588089\n",
      "step :  482 loss :  0.07822314649820328\n",
      "step :  483 loss :  0.0782230868935585\n",
      "step :  484 loss :  0.07822311669588089\n",
      "step :  485 loss :  0.07822318375110626\n",
      "step :  486 loss :  0.07822312414646149\n",
      "step :  487 loss :  0.07822316139936447\n",
      "step :  488 loss :  0.07822304964065552\n",
      "step :  489 loss :  0.07822305709123611\n",
      "step :  490 loss :  0.07822301238775253\n",
      "step :  491 loss :  0.07822314649820328\n",
      "step :  492 loss :  0.07822301238775253\n",
      "step :  493 loss :  0.07822300493717194\n",
      "step :  494 loss :  0.07822304219007492\n",
      "step :  495 loss :  0.07822306454181671\n",
      "step :  496 loss :  0.07822294533252716\n",
      "step :  497 loss :  0.07822290807962418\n",
      "step :  498 loss :  0.07822298258543015\n",
      "step :  499 loss :  0.07822296023368835\n",
      "Training model 3\n",
      "step :  0 loss :  0.29429706931114197\n",
      "step :  1 loss :  0.2531120181083679\n",
      "step :  2 loss :  0.20978610217571259\n",
      "step :  3 loss :  0.2197209894657135\n",
      "step :  4 loss :  0.18939590454101562\n",
      "step :  5 loss :  0.22200386226177216\n",
      "step :  6 loss :  0.19876685738563538\n",
      "step :  7 loss :  0.2234814614057541\n",
      "step :  8 loss :  0.18718162178993225\n",
      "step :  9 loss :  0.205559641122818\n",
      "step :  10 loss :  0.17674235999584198\n",
      "step :  11 loss :  0.16820663213729858\n",
      "step :  12 loss :  0.15616311132907867\n",
      "step :  13 loss :  0.15437524020671844\n",
      "step :  14 loss :  0.15113411843776703\n",
      "step :  15 loss :  0.19554048776626587\n",
      "step :  16 loss :  0.209795743227005\n",
      "step :  17 loss :  0.11796281486749649\n",
      "step :  18 loss :  0.14035849273204803\n",
      "step :  19 loss :  0.11135294288396835\n",
      "step :  20 loss :  0.1586432009935379\n",
      "step :  21 loss :  0.2052527815103531\n",
      "step :  22 loss :  0.12226015329360962\n",
      "step :  23 loss :  0.11385942250490189\n",
      "step :  24 loss :  0.1305890679359436\n",
      "step :  25 loss :  0.11354564875364304\n",
      "step :  26 loss :  0.16535015404224396\n",
      "step :  27 loss :  0.11268766969442368\n",
      "step :  28 loss :  0.09943833947181702\n",
      "step :  29 loss :  0.09962763637304306\n",
      "step :  30 loss :  0.09680306911468506\n",
      "step :  31 loss :  0.10296088457107544\n",
      "step :  32 loss :  0.1103711947798729\n",
      "step :  33 loss :  0.1097913607954979\n",
      "step :  34 loss :  0.1296054571866989\n",
      "step :  35 loss :  0.09105455875396729\n",
      "step :  36 loss :  0.10705766826868057\n",
      "step :  37 loss :  0.12215611338615417\n",
      "step :  38 loss :  0.08826186507940292\n",
      "step :  39 loss :  0.09313733875751495\n",
      "step :  40 loss :  0.0909123346209526\n",
      "step :  41 loss :  0.1027538999915123\n",
      "step :  42 loss :  0.0909406915307045\n",
      "step :  43 loss :  0.08837629109621048\n",
      "step :  44 loss :  0.08572795242071152\n",
      "step :  45 loss :  0.09610580652952194\n",
      "step :  46 loss :  0.08835374563932419\n",
      "step :  47 loss :  0.09060001373291016\n",
      "step :  48 loss :  0.08633796125650406\n",
      "step :  49 loss :  0.08977054804563522\n",
      "step :  50 loss :  0.10203203558921814\n",
      "step :  51 loss :  0.08358291536569595\n",
      "step :  52 loss :  0.08470062911510468\n",
      "step :  53 loss :  0.08388004451990128\n",
      "step :  54 loss :  0.08465982228517532\n",
      "step :  55 loss :  0.0898546352982521\n",
      "step :  56 loss :  0.08302413672208786\n",
      "step :  57 loss :  0.09642945975065231\n",
      "step :  58 loss :  0.08663902431726456\n",
      "step :  59 loss :  0.09092594683170319\n",
      "step :  60 loss :  0.08510112017393112\n",
      "step :  61 loss :  0.08339017629623413\n",
      "step :  62 loss :  0.08708872646093369\n",
      "step :  63 loss :  0.09005890041589737\n",
      "step :  64 loss :  0.08220462501049042\n",
      "step :  65 loss :  0.08098778128623962\n",
      "step :  66 loss :  0.08258233964443207\n",
      "step :  67 loss :  0.08643551170825958\n",
      "step :  68 loss :  0.08154644817113876\n",
      "step :  69 loss :  0.08473069965839386\n",
      "step :  70 loss :  0.08240948617458344\n",
      "step :  71 loss :  0.08225888013839722\n",
      "step :  72 loss :  0.08093942701816559\n",
      "step :  73 loss :  0.0826474204659462\n",
      "step :  74 loss :  0.0855419859290123\n",
      "step :  75 loss :  0.0828426331281662\n",
      "step :  76 loss :  0.08871106803417206\n",
      "step :  77 loss :  0.08096662163734436\n",
      "step :  78 loss :  0.08149343729019165\n",
      "step :  79 loss :  0.08185115456581116\n",
      "step :  80 loss :  0.08503001183271408\n",
      "step :  81 loss :  0.08079244941473007\n",
      "step :  82 loss :  0.08729003369808197\n",
      "step :  83 loss :  0.08031988888978958\n",
      "step :  84 loss :  0.08060029149055481\n",
      "step :  85 loss :  0.08022894710302353\n",
      "step :  86 loss :  0.08110616356134415\n",
      "step :  87 loss :  0.08206526190042496\n",
      "step :  88 loss :  0.08192440122365952\n",
      "step :  89 loss :  0.08317313343286514\n",
      "step :  90 loss :  0.08137568086385727\n",
      "step :  91 loss :  0.07978537678718567\n",
      "step :  92 loss :  0.08255737274885178\n",
      "step :  93 loss :  0.08029131591320038\n",
      "step :  94 loss :  0.08043073117733002\n",
      "step :  95 loss :  0.07974296808242798\n",
      "step :  96 loss :  0.07961617410182953\n",
      "step :  97 loss :  0.07980722188949585\n",
      "step :  98 loss :  0.08099672943353653\n",
      "step :  99 loss :  0.08011934161186218\n",
      "step :  100 loss :  0.08072277158498764\n",
      "step :  101 loss :  0.08338693529367447\n",
      "step :  102 loss :  0.0799311026930809\n",
      "step :  103 loss :  0.08065624535083771\n",
      "step :  104 loss :  0.07956798374652863\n",
      "step :  105 loss :  0.08123022317886353\n",
      "step :  106 loss :  0.08325161039829254\n",
      "step :  107 loss :  0.08006617426872253\n",
      "step :  108 loss :  0.08016861230134964\n",
      "step :  109 loss :  0.08023133873939514\n",
      "step :  110 loss :  0.08109792321920395\n",
      "step :  111 loss :  0.07991600036621094\n",
      "step :  112 loss :  0.07975896447896957\n",
      "step :  113 loss :  0.07976595312356949\n",
      "step :  114 loss :  0.07944465428590775\n",
      "step :  115 loss :  0.0803208276629448\n",
      "step :  116 loss :  0.079485684633255\n",
      "step :  117 loss :  0.0792032778263092\n",
      "step :  118 loss :  0.07916375994682312\n",
      "step :  119 loss :  0.07927355170249939\n",
      "step :  120 loss :  0.07923669368028641\n",
      "step :  121 loss :  0.0794031172990799\n",
      "step :  122 loss :  0.07968274503946304\n",
      "step :  123 loss :  0.07959947735071182\n",
      "step :  124 loss :  0.07927484065294266\n",
      "step :  125 loss :  0.079208143055439\n",
      "step :  126 loss :  0.07984008640050888\n",
      "step :  127 loss :  0.07919250428676605\n",
      "step :  128 loss :  0.0790746659040451\n",
      "step :  129 loss :  0.07912485301494598\n",
      "step :  130 loss :  0.07907526195049286\n",
      "step :  131 loss :  0.07934031635522842\n",
      "step :  132 loss :  0.07899525761604309\n",
      "step :  133 loss :  0.07890790700912476\n",
      "step :  134 loss :  0.07907763868570328\n",
      "step :  135 loss :  0.07890544831752777\n",
      "step :  136 loss :  0.07935799658298492\n",
      "step :  137 loss :  0.0789405107498169\n",
      "step :  138 loss :  0.07897081226110458\n",
      "step :  139 loss :  0.07905453443527222\n",
      "step :  140 loss :  0.07901729643344879\n",
      "step :  141 loss :  0.0793377012014389\n",
      "step :  142 loss :  0.07897254824638367\n",
      "step :  143 loss :  0.07899677008390427\n",
      "step :  144 loss :  0.079180046916008\n",
      "step :  145 loss :  0.07897768169641495\n",
      "step :  146 loss :  0.07904407382011414\n",
      "step :  147 loss :  0.0790286511182785\n",
      "step :  148 loss :  0.07882782816886902\n",
      "step :  149 loss :  0.07890915125608444\n",
      "step :  150 loss :  0.07889794558286667\n",
      "step :  151 loss :  0.0787465050816536\n",
      "step :  152 loss :  0.07883251458406448\n",
      "step :  153 loss :  0.07879705727100372\n",
      "step :  154 loss :  0.07882290333509445\n",
      "step :  155 loss :  0.07896565645933151\n",
      "step :  156 loss :  0.07900697737932205\n",
      "step :  157 loss :  0.07871896773576736\n",
      "step :  158 loss :  0.07876957207918167\n",
      "step :  159 loss :  0.0787554383277893\n",
      "step :  160 loss :  0.07878270000219345\n",
      "step :  161 loss :  0.07877176254987717\n",
      "step :  162 loss :  0.07871058583259583\n",
      "step :  163 loss :  0.07869548350572586\n",
      "step :  164 loss :  0.07863648980855942\n",
      "step :  165 loss :  0.07878551632165909\n",
      "step :  166 loss :  0.07865097373723984\n",
      "step :  167 loss :  0.07890298962593079\n",
      "step :  168 loss :  0.07869589328765869\n",
      "step :  169 loss :  0.07883129268884659\n",
      "step :  170 loss :  0.07875564694404602\n",
      "step :  171 loss :  0.07885359972715378\n",
      "step :  172 loss :  0.078658327460289\n",
      "step :  173 loss :  0.07871904224157333\n",
      "step :  174 loss :  0.07861492037773132\n",
      "step :  175 loss :  0.07857821881771088\n",
      "step :  176 loss :  0.07867935299873352\n",
      "step :  177 loss :  0.07859098166227341\n",
      "step :  178 loss :  0.0786929726600647\n",
      "step :  179 loss :  0.07866424322128296\n",
      "step :  180 loss :  0.07856708765029907\n",
      "step :  181 loss :  0.07868469506502151\n",
      "step :  182 loss :  0.07864191383123398\n",
      "step :  183 loss :  0.07863878458738327\n",
      "step :  184 loss :  0.07857892662286758\n",
      "step :  185 loss :  0.07853221148252487\n",
      "step :  186 loss :  0.07872045785188675\n",
      "step :  187 loss :  0.07863597571849823\n",
      "step :  188 loss :  0.07855682075023651\n",
      "step :  189 loss :  0.07871231436729431\n",
      "step :  190 loss :  0.07857274264097214\n",
      "step :  191 loss :  0.0785689577460289\n",
      "step :  192 loss :  0.07867851108312607\n",
      "step :  193 loss :  0.07854284346103668\n",
      "step :  194 loss :  0.0786365494132042\n",
      "step :  195 loss :  0.0785767212510109\n",
      "step :  196 loss :  0.07853033393621445\n",
      "step :  197 loss :  0.0786418467760086\n",
      "step :  198 loss :  0.07860572636127472\n",
      "step :  199 loss :  0.0785268247127533\n",
      "step :  200 loss :  0.07861974835395813\n",
      "step :  201 loss :  0.07857935130596161\n",
      "step :  202 loss :  0.07851211726665497\n",
      "step :  203 loss :  0.0785503014922142\n",
      "step :  204 loss :  0.07853328436613083\n",
      "step :  205 loss :  0.07849083840847015\n",
      "step :  206 loss :  0.07855510711669922\n",
      "step :  207 loss :  0.07849496603012085\n",
      "step :  208 loss :  0.07849299907684326\n",
      "step :  209 loss :  0.07866643369197845\n",
      "step :  210 loss :  0.07848557084798813\n",
      "step :  211 loss :  0.07850824296474457\n",
      "step :  212 loss :  0.0785127803683281\n",
      "step :  213 loss :  0.07847674936056137\n",
      "step :  214 loss :  0.07849245518445969\n",
      "step :  215 loss :  0.07846064865589142\n",
      "step :  216 loss :  0.07846523821353912\n",
      "step :  217 loss :  0.07848090678453445\n",
      "step :  218 loss :  0.07845579087734222\n",
      "step :  219 loss :  0.07847612351179123\n",
      "step :  220 loss :  0.07845241576433182\n",
      "step :  221 loss :  0.07847243547439575\n",
      "step :  222 loss :  0.07845230400562286\n",
      "step :  223 loss :  0.07844790071249008\n",
      "step :  224 loss :  0.07850506901741028\n",
      "step :  225 loss :  0.07846028357744217\n",
      "step :  226 loss :  0.07845598459243774\n",
      "step :  227 loss :  0.07845889031887054\n",
      "step :  228 loss :  0.07845746725797653\n",
      "step :  229 loss :  0.07844363152980804\n",
      "step :  230 loss :  0.07845636457204819\n",
      "step :  231 loss :  0.07845347374677658\n",
      "step :  232 loss :  0.07843049615621567\n",
      "step :  233 loss :  0.07846485078334808\n",
      "step :  234 loss :  0.07846307754516602\n",
      "step :  235 loss :  0.0784418061375618\n",
      "step :  236 loss :  0.07843083888292313\n",
      "step :  237 loss :  0.07845636457204819\n",
      "step :  238 loss :  0.07847381383180618\n",
      "step :  239 loss :  0.07843174785375595\n",
      "step :  240 loss :  0.07842283695936203\n",
      "step :  241 loss :  0.07844190299510956\n",
      "step :  242 loss :  0.07842344045639038\n",
      "step :  243 loss :  0.0784316286444664\n",
      "step :  244 loss :  0.07842497527599335\n",
      "step :  245 loss :  0.07843256741762161\n",
      "step :  246 loss :  0.078428253531456\n",
      "step :  247 loss :  0.07842258363962173\n",
      "step :  248 loss :  0.07842643558979034\n",
      "step :  249 loss :  0.07842191308736801\n",
      "step :  250 loss :  0.07841259241104126\n",
      "step :  251 loss :  0.07841096818447113\n",
      "step :  252 loss :  0.07841205596923828\n",
      "step :  253 loss :  0.07840700447559357\n",
      "step :  254 loss :  0.07842599600553513\n",
      "step :  255 loss :  0.07841456681489944\n",
      "step :  256 loss :  0.07839823514223099\n",
      "step :  257 loss :  0.07842569798231125\n",
      "step :  258 loss :  0.0784081369638443\n",
      "step :  259 loss :  0.07839655131101608\n",
      "step :  260 loss :  0.0784163698554039\n",
      "step :  261 loss :  0.07840059697628021\n",
      "step :  262 loss :  0.07838934659957886\n",
      "step :  263 loss :  0.07839632779359818\n",
      "step :  264 loss :  0.07840480655431747\n",
      "step :  265 loss :  0.07838195562362671\n",
      "step :  266 loss :  0.07840243726968765\n",
      "step :  267 loss :  0.07839450240135193\n",
      "step :  268 loss :  0.07838349044322968\n",
      "step :  269 loss :  0.07839168608188629\n",
      "step :  270 loss :  0.07837902009487152\n",
      "step :  271 loss :  0.07838191837072372\n",
      "step :  272 loss :  0.07839004695415497\n",
      "step :  273 loss :  0.07838691771030426\n",
      "step :  274 loss :  0.07838429510593414\n",
      "step :  275 loss :  0.07837611436843872\n",
      "step :  276 loss :  0.07838061451911926\n",
      "step :  277 loss :  0.07837533205747604\n",
      "step :  278 loss :  0.07837443798780441\n",
      "step :  279 loss :  0.07838062196969986\n",
      "step :  280 loss :  0.07838602364063263\n",
      "step :  281 loss :  0.0783734992146492\n",
      "step :  282 loss :  0.07837019860744476\n",
      "step :  283 loss :  0.07838816940784454\n",
      "step :  284 loss :  0.07837937772274017\n",
      "step :  285 loss :  0.07836811989545822\n",
      "step :  286 loss :  0.07837130129337311\n",
      "step :  287 loss :  0.07837314903736115\n",
      "step :  288 loss :  0.07836322486400604\n",
      "step :  289 loss :  0.07837353646755219\n",
      "step :  290 loss :  0.07836752384901047\n",
      "step :  291 loss :  0.0783638209104538\n",
      "step :  292 loss :  0.07837642729282379\n",
      "step :  293 loss :  0.07836822420358658\n",
      "step :  294 loss :  0.07836056500673294\n",
      "step :  295 loss :  0.07836263626813889\n",
      "step :  296 loss :  0.07836943119764328\n",
      "step :  297 loss :  0.07836053520441055\n",
      "step :  298 loss :  0.07836182415485382\n",
      "step :  299 loss :  0.07836488634347916\n",
      "step :  300 loss :  0.07836191356182098\n",
      "step :  301 loss :  0.07836590707302094\n",
      "step :  302 loss :  0.07835548371076584\n",
      "step :  303 loss :  0.07835660874843597\n",
      "step :  304 loss :  0.07835537940263748\n",
      "step :  305 loss :  0.0783509761095047\n",
      "step :  306 loss :  0.07836464792490005\n",
      "step :  307 loss :  0.07835835218429565\n",
      "step :  308 loss :  0.07834929972887039\n",
      "step :  309 loss :  0.07835943251848221\n",
      "step :  310 loss :  0.07835688441991806\n",
      "step :  311 loss :  0.07834992557764053\n",
      "step :  312 loss :  0.07835056632757187\n",
      "step :  313 loss :  0.07835384458303452\n",
      "step :  314 loss :  0.07834585011005402\n",
      "step :  315 loss :  0.07835132628679276\n",
      "step :  316 loss :  0.07835293561220169\n",
      "step :  317 loss :  0.07834497839212418\n",
      "step :  318 loss :  0.07834790647029877\n",
      "step :  319 loss :  0.0783461481332779\n",
      "step :  320 loss :  0.07834620028734207\n",
      "step :  321 loss :  0.07834352552890778\n",
      "step :  322 loss :  0.07834555208683014\n",
      "step :  323 loss :  0.078342005610466\n",
      "step :  324 loss :  0.07834704965353012\n",
      "step :  325 loss :  0.07834131270647049\n",
      "step :  326 loss :  0.07833883911371231\n",
      "step :  327 loss :  0.07834841310977936\n",
      "step :  328 loss :  0.07834283262491226\n",
      "step :  329 loss :  0.07834018766880035\n",
      "step :  330 loss :  0.07834143936634064\n",
      "step :  331 loss :  0.0783391073346138\n",
      "step :  332 loss :  0.07834000885486603\n",
      "step :  333 loss :  0.07834407687187195\n",
      "step :  334 loss :  0.0783383920788765\n",
      "step :  335 loss :  0.07833744585514069\n",
      "step :  336 loss :  0.0783379077911377\n",
      "step :  337 loss :  0.07834000885486603\n",
      "step :  338 loss :  0.07833606749773026\n",
      "step :  339 loss :  0.07833965867757797\n",
      "step :  340 loss :  0.07833770662546158\n",
      "step :  341 loss :  0.07833579927682877\n",
      "step :  342 loss :  0.0783386379480362\n",
      "step :  343 loss :  0.07833773642778397\n",
      "step :  344 loss :  0.07833672314882278\n",
      "step :  345 loss :  0.07834024727344513\n",
      "step :  346 loss :  0.07833458483219147\n",
      "step :  347 loss :  0.0783364474773407\n",
      "step :  348 loss :  0.07833674550056458\n",
      "step :  349 loss :  0.07833489775657654\n",
      "step :  350 loss :  0.07833606004714966\n",
      "step :  351 loss :  0.07833781093358994\n",
      "step :  352 loss :  0.0783345103263855\n",
      "step :  353 loss :  0.07833581417798996\n",
      "step :  354 loss :  0.07833480089902878\n",
      "step :  355 loss :  0.0783354640007019\n",
      "step :  356 loss :  0.07833461463451385\n",
      "step :  357 loss :  0.0783323347568512\n",
      "step :  358 loss :  0.07833609730005264\n",
      "step :  359 loss :  0.07833301275968552\n",
      "step :  360 loss :  0.07833180576562881\n",
      "step :  361 loss :  0.07833138853311539\n",
      "step :  362 loss :  0.07833299785852432\n",
      "step :  363 loss :  0.0783306285738945\n",
      "step :  364 loss :  0.0783318504691124\n",
      "step :  365 loss :  0.07833447307348251\n",
      "step :  366 loss :  0.0783325582742691\n",
      "step :  367 loss :  0.07833058387041092\n",
      "step :  368 loss :  0.07833179086446762\n",
      "step :  369 loss :  0.07833024859428406\n",
      "step :  370 loss :  0.07832906395196915\n",
      "step :  371 loss :  0.07833153754472733\n",
      "step :  372 loss :  0.07833032310009003\n",
      "step :  373 loss :  0.07832874357700348\n",
      "step :  374 loss :  0.07833106070756912\n",
      "step :  375 loss :  0.07833167910575867\n",
      "step :  376 loss :  0.07833008468151093\n",
      "step :  377 loss :  0.07832872122526169\n",
      "step :  378 loss :  0.07833078503608704\n",
      "step :  379 loss :  0.07832847535610199\n",
      "step :  380 loss :  0.0783303827047348\n",
      "step :  381 loss :  0.07832971960306168\n",
      "step :  382 loss :  0.07832808792591095\n",
      "step :  383 loss :  0.07832934707403183\n",
      "step :  384 loss :  0.0783301591873169\n",
      "step :  385 loss :  0.07832790166139603\n",
      "step :  386 loss :  0.07832799106836319\n",
      "step :  387 loss :  0.07832921296358109\n",
      "step :  388 loss :  0.07832715660333633\n",
      "step :  389 loss :  0.07832710444927216\n",
      "step :  390 loss :  0.07832879573106766\n",
      "step :  391 loss :  0.07832640409469604\n",
      "step :  392 loss :  0.0783287063241005\n",
      "step :  393 loss :  0.07832875847816467\n",
      "step :  394 loss :  0.07832685858011246\n",
      "step :  395 loss :  0.07832863181829453\n",
      "step :  396 loss :  0.0783277302980423\n",
      "step :  397 loss :  0.07832596451044083\n",
      "step :  398 loss :  0.07832653075456619\n",
      "step :  399 loss :  0.07832829654216766\n",
      "step :  400 loss :  0.0783272385597229\n",
      "step :  401 loss :  0.07832605391740799\n",
      "step :  402 loss :  0.07832609117031097\n",
      "step :  403 loss :  0.07832732051610947\n",
      "step :  404 loss :  0.07832535356283188\n",
      "step :  405 loss :  0.07832515239715576\n",
      "step :  406 loss :  0.07832702994346619\n",
      "step :  407 loss :  0.07832599431276321\n",
      "step :  408 loss :  0.07832495123147964\n",
      "step :  409 loss :  0.07832589745521545\n",
      "step :  410 loss :  0.07832540571689606\n",
      "step :  411 loss :  0.07832540571689606\n",
      "step :  412 loss :  0.07832594215869904\n",
      "step :  413 loss :  0.07832624763250351\n",
      "step :  414 loss :  0.07832545042037964\n",
      "step :  415 loss :  0.0783248171210289\n",
      "step :  416 loss :  0.07832518965005875\n",
      "step :  417 loss :  0.07832460850477219\n",
      "step :  418 loss :  0.07832524180412292\n",
      "step :  419 loss :  0.07832486182451248\n",
      "step :  420 loss :  0.07832422107458115\n",
      "step :  421 loss :  0.07832460105419159\n",
      "step :  422 loss :  0.07832532376050949\n",
      "step :  423 loss :  0.07832436263561249\n",
      "step :  424 loss :  0.07832416146993637\n",
      "step :  425 loss :  0.07832567393779755\n",
      "step :  426 loss :  0.07832439243793488\n",
      "step :  427 loss :  0.07832387089729309\n",
      "step :  428 loss :  0.07832496613264084\n",
      "step :  429 loss :  0.0783245787024498\n",
      "step :  430 loss :  0.07832401990890503\n",
      "step :  431 loss :  0.07832415401935577\n",
      "step :  432 loss :  0.07832450419664383\n",
      "step :  433 loss :  0.07832366973161697\n",
      "step :  434 loss :  0.07832380384206772\n",
      "step :  435 loss :  0.07832352072000504\n",
      "step :  436 loss :  0.07832411676645279\n",
      "step :  437 loss :  0.07832390069961548\n",
      "step :  438 loss :  0.07832347601652145\n",
      "step :  439 loss :  0.07832439243793488\n",
      "step :  440 loss :  0.07832416146993637\n",
      "step :  441 loss :  0.07832342386245728\n",
      "step :  442 loss :  0.07832354307174683\n",
      "step :  443 loss :  0.07832391560077667\n",
      "step :  444 loss :  0.07832344621419907\n",
      "step :  445 loss :  0.07832328230142593\n",
      "step :  446 loss :  0.0783240795135498\n",
      "step :  447 loss :  0.07832346111536026\n",
      "step :  448 loss :  0.07832357287406921\n",
      "step :  449 loss :  0.0783233493566513\n",
      "step :  450 loss :  0.07832305878400803\n",
      "step :  451 loss :  0.07832306623458862\n",
      "step :  452 loss :  0.07832353562116623\n",
      "step :  453 loss :  0.07832304388284683\n",
      "step :  454 loss :  0.07832285016775131\n",
      "step :  455 loss :  0.07832343876361847\n",
      "step :  456 loss :  0.07832323014736176\n",
      "step :  457 loss :  0.07832303643226624\n",
      "step :  458 loss :  0.07832294702529907\n",
      "step :  459 loss :  0.07832310348749161\n",
      "step :  460 loss :  0.07832321524620056\n",
      "step :  461 loss :  0.07832298427820206\n",
      "step :  462 loss :  0.07832269370555878\n",
      "step :  463 loss :  0.07832310348749161\n",
      "step :  464 loss :  0.07832269370555878\n",
      "step :  465 loss :  0.07832244038581848\n",
      "step :  466 loss :  0.07832299917936325\n",
      "step :  467 loss :  0.07832273095846176\n",
      "step :  468 loss :  0.07832290977239609\n",
      "step :  469 loss :  0.07832247018814087\n",
      "step :  470 loss :  0.07832258939743042\n",
      "step :  471 loss :  0.07832273840904236\n",
      "step :  472 loss :  0.0783228799700737\n",
      "step :  473 loss :  0.0783223882317543\n",
      "step :  474 loss :  0.0783226266503334\n",
      "step :  475 loss :  0.07832227647304535\n",
      "step :  476 loss :  0.0783226266503334\n",
      "step :  477 loss :  0.078322634100914\n",
      "step :  478 loss :  0.07832241803407669\n",
      "step :  479 loss :  0.07832226902246475\n",
      "step :  480 loss :  0.07832241803407669\n",
      "step :  481 loss :  0.0783223882317543\n",
      "step :  482 loss :  0.07832256704568863\n",
      "step :  483 loss :  0.07832252234220505\n",
      "step :  484 loss :  0.07832225412130356\n",
      "step :  485 loss :  0.07832223922014236\n",
      "step :  486 loss :  0.0783224031329155\n",
      "step :  487 loss :  0.07832212001085281\n",
      "step :  488 loss :  0.07832232117652893\n",
      "step :  489 loss :  0.07832230627536774\n",
      "step :  490 loss :  0.07832203060388565\n",
      "step :  491 loss :  0.07832211256027222\n",
      "step :  492 loss :  0.07832212746143341\n",
      "step :  493 loss :  0.07832201570272446\n",
      "step :  494 loss :  0.07832217961549759\n",
      "step :  495 loss :  0.07832205295562744\n",
      "step :  496 loss :  0.07832213491201401\n",
      "step :  497 loss :  0.07832194119691849\n",
      "step :  498 loss :  0.07832213491201401\n",
      "step :  499 loss :  0.07832212746143341\n",
      "Training model 4\n",
      "step :  0 loss :  0.36404949426651\n",
      "step :  1 loss :  0.2111763060092926\n",
      "step :  2 loss :  0.20080652832984924\n",
      "step :  3 loss :  0.2437771111726761\n",
      "step :  4 loss :  0.2169862687587738\n",
      "step :  5 loss :  0.1922372579574585\n",
      "step :  6 loss :  0.217172309756279\n",
      "step :  7 loss :  0.23127445578575134\n",
      "step :  8 loss :  0.21283318102359772\n",
      "step :  9 loss :  0.19713401794433594\n",
      "step :  10 loss :  0.20342083275318146\n",
      "step :  11 loss :  0.1752931773662567\n",
      "step :  12 loss :  0.1648741364479065\n",
      "step :  13 loss :  0.15720146894454956\n",
      "step :  14 loss :  0.1408052295446396\n",
      "step :  15 loss :  0.13579116761684418\n",
      "step :  16 loss :  0.12240497767925262\n",
      "step :  17 loss :  0.3813205361366272\n",
      "step :  18 loss :  0.12163731455802917\n",
      "step :  19 loss :  0.12430260330438614\n",
      "step :  20 loss :  0.1799994856119156\n",
      "step :  21 loss :  0.1382894665002823\n",
      "step :  22 loss :  0.1383723020553589\n",
      "step :  23 loss :  0.12799008190631866\n",
      "step :  24 loss :  0.1178097277879715\n",
      "step :  25 loss :  0.11014118045568466\n",
      "step :  26 loss :  0.10624159872531891\n",
      "step :  27 loss :  0.1049753949046135\n",
      "step :  28 loss :  0.10408827662467957\n",
      "step :  29 loss :  0.10971330106258392\n",
      "step :  30 loss :  0.10668956488370895\n",
      "step :  31 loss :  0.10560491681098938\n",
      "step :  32 loss :  0.10003675520420074\n",
      "step :  33 loss :  0.10609201341867447\n",
      "step :  34 loss :  0.09985725581645966\n",
      "step :  35 loss :  0.12815611064434052\n",
      "step :  36 loss :  0.11249060928821564\n",
      "step :  37 loss :  0.09599312394857407\n",
      "step :  38 loss :  0.11604790389537811\n",
      "step :  39 loss :  0.09996820986270905\n",
      "step :  40 loss :  0.09653971344232559\n",
      "step :  41 loss :  0.09482031315565109\n",
      "step :  42 loss :  0.09177540987730026\n",
      "step :  43 loss :  0.10668586194515228\n",
      "step :  44 loss :  0.08995567262172699\n",
      "step :  45 loss :  0.10301679372787476\n",
      "step :  46 loss :  0.11447646468877792\n",
      "step :  47 loss :  0.0915253683924675\n",
      "step :  48 loss :  0.10041382908821106\n",
      "step :  49 loss :  0.09887541830539703\n",
      "step :  50 loss :  0.08933155983686447\n",
      "step :  51 loss :  0.09282071143388748\n",
      "step :  52 loss :  0.08779855072498322\n",
      "step :  53 loss :  0.0890827476978302\n",
      "step :  54 loss :  0.0857938900589943\n",
      "step :  55 loss :  0.08488401770591736\n",
      "step :  56 loss :  0.08646433800458908\n",
      "step :  57 loss :  0.08951320499181747\n",
      "step :  58 loss :  0.09090141206979752\n",
      "step :  59 loss :  0.09199285507202148\n",
      "step :  60 loss :  0.09048066288232803\n",
      "step :  61 loss :  0.0925721749663353\n",
      "step :  62 loss :  0.08599438518285751\n",
      "step :  63 loss :  0.08652272075414658\n",
      "step :  64 loss :  0.08350386470556259\n",
      "step :  65 loss :  0.09234996140003204\n",
      "step :  66 loss :  0.08979151397943497\n",
      "step :  67 loss :  0.08356549590826035\n",
      "step :  68 loss :  0.087983138859272\n",
      "step :  69 loss :  0.08632590621709824\n",
      "step :  70 loss :  0.08505766838788986\n",
      "step :  71 loss :  0.08349582552909851\n",
      "step :  72 loss :  0.08702579885721207\n",
      "step :  73 loss :  0.08271396905183792\n",
      "step :  74 loss :  0.08894333243370056\n",
      "step :  75 loss :  0.08561526238918304\n",
      "step :  76 loss :  0.0889207050204277\n",
      "step :  77 loss :  0.0847773551940918\n",
      "step :  78 loss :  0.08336862176656723\n",
      "step :  79 loss :  0.08259619027376175\n",
      "step :  80 loss :  0.08355515450239182\n",
      "step :  81 loss :  0.08503884077072144\n",
      "step :  82 loss :  0.08464138209819794\n",
      "step :  83 loss :  0.08606317639350891\n",
      "step :  84 loss :  0.08651243150234222\n",
      "step :  85 loss :  0.08241043984889984\n",
      "step :  86 loss :  0.08210057020187378\n",
      "step :  87 loss :  0.08215252310037613\n",
      "step :  88 loss :  0.08168422430753708\n",
      "step :  89 loss :  0.08214709907770157\n",
      "step :  90 loss :  0.08246234804391861\n",
      "step :  91 loss :  0.08226070553064346\n",
      "step :  92 loss :  0.08453864604234695\n",
      "step :  93 loss :  0.08535865694284439\n",
      "step :  94 loss :  0.0818273201584816\n",
      "step :  95 loss :  0.08204954117536545\n",
      "step :  96 loss :  0.08332794904708862\n",
      "step :  97 loss :  0.08169553428888321\n",
      "step :  98 loss :  0.08165289461612701\n",
      "step :  99 loss :  0.08213500678539276\n",
      "step :  100 loss :  0.08224949240684509\n",
      "step :  101 loss :  0.08233290910720825\n",
      "step :  102 loss :  0.08126603811979294\n",
      "step :  103 loss :  0.08167297393083572\n",
      "step :  104 loss :  0.08199968189001083\n",
      "step :  105 loss :  0.0810864120721817\n",
      "step :  106 loss :  0.08286160975694656\n",
      "step :  107 loss :  0.08278609067201614\n",
      "step :  108 loss :  0.08129981905221939\n",
      "step :  109 loss :  0.08177001029253006\n",
      "step :  110 loss :  0.08106832206249237\n",
      "step :  111 loss :  0.08169350028038025\n",
      "step :  112 loss :  0.08097974210977554\n",
      "step :  113 loss :  0.08105781674385071\n",
      "step :  114 loss :  0.08107316493988037\n",
      "step :  115 loss :  0.0811014249920845\n",
      "step :  116 loss :  0.08149085193872452\n",
      "step :  117 loss :  0.080827996134758\n",
      "step :  118 loss :  0.08082187175750732\n",
      "step :  119 loss :  0.08132211118936539\n",
      "step :  120 loss :  0.08085726946592331\n",
      "step :  121 loss :  0.0808257982134819\n",
      "step :  122 loss :  0.08072385936975479\n",
      "step :  123 loss :  0.08074525743722916\n",
      "step :  124 loss :  0.08236966282129288\n",
      "step :  125 loss :  0.08112343400716782\n",
      "step :  126 loss :  0.08152610808610916\n",
      "step :  127 loss :  0.08090420067310333\n",
      "step :  128 loss :  0.08066661655902863\n",
      "step :  129 loss :  0.08147092163562775\n",
      "step :  130 loss :  0.08056922256946564\n",
      "step :  131 loss :  0.08056149631738663\n",
      "step :  132 loss :  0.08106958866119385\n",
      "step :  133 loss :  0.08073043823242188\n",
      "step :  134 loss :  0.08110898733139038\n",
      "step :  135 loss :  0.08055155724287033\n",
      "step :  136 loss :  0.08054337650537491\n",
      "step :  137 loss :  0.08052226901054382\n",
      "step :  138 loss :  0.08037183433771133\n",
      "step :  139 loss :  0.08076182752847672\n",
      "step :  140 loss :  0.08042873442173004\n",
      "step :  141 loss :  0.08092674612998962\n",
      "step :  142 loss :  0.08038165420293808\n",
      "step :  143 loss :  0.08117225021123886\n",
      "step :  144 loss :  0.08033374696969986\n",
      "step :  145 loss :  0.08031977713108063\n",
      "step :  146 loss :  0.08030503243207932\n",
      "step :  147 loss :  0.08028526604175568\n",
      "step :  148 loss :  0.08040443807840347\n",
      "step :  149 loss :  0.08038708567619324\n",
      "step :  150 loss :  0.08029484003782272\n",
      "step :  151 loss :  0.08032310009002686\n",
      "step :  152 loss :  0.08019707351922989\n",
      "step :  153 loss :  0.08038616925477982\n",
      "step :  154 loss :  0.08019685745239258\n",
      "step :  155 loss :  0.0805259421467781\n",
      "step :  156 loss :  0.08036218583583832\n",
      "step :  157 loss :  0.08076424896717072\n",
      "step :  158 loss :  0.08018888533115387\n",
      "step :  159 loss :  0.08101744949817657\n",
      "step :  160 loss :  0.08023384213447571\n",
      "step :  161 loss :  0.08012006431818008\n",
      "step :  162 loss :  0.08025111258029938\n",
      "step :  163 loss :  0.08011069148778915\n",
      "step :  164 loss :  0.08013281226158142\n",
      "step :  165 loss :  0.08022220432758331\n",
      "step :  166 loss :  0.08006572723388672\n",
      "step :  167 loss :  0.08010754734277725\n",
      "step :  168 loss :  0.07999017834663391\n",
      "step :  169 loss :  0.0801251158118248\n",
      "step :  170 loss :  0.08001143485307693\n",
      "step :  171 loss :  0.08010446280241013\n",
      "step :  172 loss :  0.08001016825437546\n",
      "step :  173 loss :  0.08008609712123871\n",
      "step :  174 loss :  0.07995180040597916\n",
      "step :  175 loss :  0.07994472235441208\n",
      "step :  176 loss :  0.0799432024359703\n",
      "step :  177 loss :  0.07994440197944641\n",
      "step :  178 loss :  0.07996269315481186\n",
      "step :  179 loss :  0.07991691678762436\n",
      "step :  180 loss :  0.07999785989522934\n",
      "step :  181 loss :  0.07991760969161987\n",
      "step :  182 loss :  0.07990599423646927\n",
      "step :  183 loss :  0.07993809133768082\n",
      "step :  184 loss :  0.07988462597131729\n",
      "step :  185 loss :  0.08000282198190689\n",
      "step :  186 loss :  0.08000525087118149\n",
      "step :  187 loss :  0.07999647408723831\n",
      "step :  188 loss :  0.07997971773147583\n",
      "step :  189 loss :  0.07996433228254318\n",
      "step :  190 loss :  0.07986801862716675\n",
      "step :  191 loss :  0.0799282118678093\n",
      "step :  192 loss :  0.07995574921369553\n",
      "step :  193 loss :  0.07985913753509521\n",
      "step :  194 loss :  0.07988811284303665\n",
      "step :  195 loss :  0.0798392966389656\n",
      "step :  196 loss :  0.07993645966053009\n",
      "step :  197 loss :  0.079836905002594\n",
      "step :  198 loss :  0.0799628347158432\n",
      "step :  199 loss :  0.07993514090776443\n",
      "step :  200 loss :  0.07993148267269135\n",
      "step :  201 loss :  0.07990249991416931\n",
      "step :  202 loss :  0.07987503707408905\n",
      "step :  203 loss :  0.07986856251955032\n",
      "step :  204 loss :  0.0798550620675087\n",
      "step :  205 loss :  0.0798506811261177\n",
      "step :  206 loss :  0.0798392966389656\n",
      "step :  207 loss :  0.07986310869455338\n",
      "step :  208 loss :  0.07982829958200455\n",
      "step :  209 loss :  0.07984726876020432\n",
      "step :  210 loss :  0.0799211785197258\n",
      "step :  211 loss :  0.07978891581296921\n",
      "step :  212 loss :  0.07982928305864334\n",
      "step :  213 loss :  0.0798250213265419\n",
      "step :  214 loss :  0.07977919280529022\n",
      "step :  215 loss :  0.07984715700149536\n",
      "step :  216 loss :  0.07979834079742432\n",
      "step :  217 loss :  0.07990077883005142\n",
      "step :  218 loss :  0.07987413555383682\n",
      "step :  219 loss :  0.07983465492725372\n",
      "step :  220 loss :  0.07984145730733871\n",
      "step :  221 loss :  0.07982181012630463\n",
      "step :  222 loss :  0.0797797441482544\n",
      "step :  223 loss :  0.07977790385484695\n",
      "step :  224 loss :  0.07980739325284958\n",
      "step :  225 loss :  0.0797792300581932\n",
      "step :  226 loss :  0.0798158198595047\n",
      "step :  227 loss :  0.07975703477859497\n",
      "step :  228 loss :  0.07982394844293594\n",
      "step :  229 loss :  0.07978150993585587\n",
      "step :  230 loss :  0.07980145514011383\n",
      "step :  231 loss :  0.07978696376085281\n",
      "step :  232 loss :  0.07976090908050537\n",
      "step :  233 loss :  0.07976364344358444\n",
      "step :  234 loss :  0.07975604385137558\n",
      "step :  235 loss :  0.07974687963724136\n",
      "step :  236 loss :  0.07975515723228455\n",
      "step :  237 loss :  0.07974172383546829\n",
      "step :  238 loss :  0.07979749143123627\n",
      "step :  239 loss :  0.0797843486070633\n",
      "step :  240 loss :  0.07981420308351517\n",
      "step :  241 loss :  0.07979180663824081\n",
      "step :  242 loss :  0.07978302240371704\n",
      "step :  243 loss :  0.07977412641048431\n",
      "step :  244 loss :  0.07975458353757858\n",
      "step :  245 loss :  0.07976631075143814\n",
      "step :  246 loss :  0.07973998039960861\n",
      "step :  247 loss :  0.07977294921875\n",
      "step :  248 loss :  0.07973790913820267\n",
      "step :  249 loss :  0.0797814354300499\n",
      "step :  250 loss :  0.07980801165103912\n",
      "step :  251 loss :  0.0797591283917427\n",
      "step :  252 loss :  0.07974646240472794\n",
      "step :  253 loss :  0.0797804445028305\n",
      "step :  254 loss :  0.07972368597984314\n",
      "step :  255 loss :  0.07974536716938019\n",
      "step :  256 loss :  0.07974356412887573\n",
      "step :  257 loss :  0.07973653823137283\n",
      "step :  258 loss :  0.07975203543901443\n",
      "step :  259 loss :  0.07972732186317444\n",
      "step :  260 loss :  0.07971888035535812\n",
      "step :  261 loss :  0.07973901927471161\n",
      "step :  262 loss :  0.07972420752048492\n",
      "step :  263 loss :  0.07974256575107574\n",
      "step :  264 loss :  0.07972688227891922\n",
      "step :  265 loss :  0.07973302900791168\n",
      "step :  266 loss :  0.07975298166275024\n",
      "step :  267 loss :  0.0797279104590416\n",
      "step :  268 loss :  0.07973755896091461\n",
      "step :  269 loss :  0.07972511649131775\n",
      "step :  270 loss :  0.07972203940153122\n",
      "step :  271 loss :  0.07971794158220291\n",
      "step :  272 loss :  0.07971102744340897\n",
      "step :  273 loss :  0.07972991466522217\n",
      "step :  274 loss :  0.07971271127462387\n",
      "step :  275 loss :  0.07971131801605225\n",
      "step :  276 loss :  0.07971630245447159\n",
      "step :  277 loss :  0.07971258461475372\n",
      "step :  278 loss :  0.07972967624664307\n",
      "step :  279 loss :  0.0797228217124939\n",
      "step :  280 loss :  0.07971421629190445\n",
      "step :  281 loss :  0.07973264902830124\n",
      "step :  282 loss :  0.0797111764550209\n",
      "step :  283 loss :  0.07971670478582382\n",
      "step :  284 loss :  0.07974162697792053\n",
      "step :  285 loss :  0.07971936464309692\n",
      "step :  286 loss :  0.07972011715173721\n",
      "step :  287 loss :  0.07973024249076843\n",
      "step :  288 loss :  0.07971205562353134\n",
      "step :  289 loss :  0.079714834690094\n",
      "step :  290 loss :  0.07972335815429688\n",
      "step :  291 loss :  0.0797068253159523\n",
      "step :  292 loss :  0.07970871031284332\n",
      "step :  293 loss :  0.07970645278692245\n",
      "step :  294 loss :  0.0797126516699791\n",
      "step :  295 loss :  0.07970007508993149\n",
      "step :  296 loss :  0.07971808314323425\n",
      "step :  297 loss :  0.07970760017633438\n",
      "step :  298 loss :  0.0797085091471672\n",
      "step :  299 loss :  0.07972555607557297\n",
      "step :  300 loss :  0.07971397787332535\n",
      "step :  301 loss :  0.07969926297664642\n",
      "step :  302 loss :  0.07970929890871048\n",
      "step :  303 loss :  0.07971225678920746\n",
      "step :  304 loss :  0.07970685511827469\n",
      "step :  305 loss :  0.07971180230379105\n",
      "step :  306 loss :  0.07969937473535538\n",
      "step :  307 loss :  0.07970374077558517\n",
      "step :  308 loss :  0.07970476895570755\n",
      "step :  309 loss :  0.07969915866851807\n",
      "step :  310 loss :  0.0797029361128807\n",
      "step :  311 loss :  0.07969845831394196\n",
      "step :  312 loss :  0.0797031968832016\n",
      "step :  313 loss :  0.07969801127910614\n",
      "step :  314 loss :  0.07970868796110153\n",
      "step :  315 loss :  0.07970215380191803\n",
      "step :  316 loss :  0.07969684153795242\n",
      "step :  317 loss :  0.07971183955669403\n",
      "step :  318 loss :  0.07970384508371353\n",
      "step :  319 loss :  0.07970006763935089\n",
      "step :  320 loss :  0.07970893383026123\n",
      "step :  321 loss :  0.07971147447824478\n",
      "step :  322 loss :  0.07969829440116882\n",
      "step :  323 loss :  0.0796961858868599\n",
      "step :  324 loss :  0.07969930768013\n",
      "step :  325 loss :  0.07969459146261215\n",
      "step :  326 loss :  0.07969611883163452\n",
      "step :  327 loss :  0.07969747483730316\n",
      "step :  328 loss :  0.0796995460987091\n",
      "step :  329 loss :  0.07969510555267334\n",
      "step :  330 loss :  0.0797056034207344\n",
      "step :  331 loss :  0.07969647645950317\n",
      "step :  332 loss :  0.07970140129327774\n",
      "step :  333 loss :  0.07969878613948822\n",
      "step :  334 loss :  0.07969918847084045\n",
      "step :  335 loss :  0.07969918847084045\n",
      "step :  336 loss :  0.07969599217176437\n",
      "step :  337 loss :  0.07969485223293304\n",
      "step :  338 loss :  0.07969577610492706\n",
      "step :  339 loss :  0.07969339191913605\n",
      "step :  340 loss :  0.07969631999731064\n",
      "step :  341 loss :  0.0796947032213211\n",
      "step :  342 loss :  0.07969345152378082\n",
      "step :  343 loss :  0.07969635725021362\n",
      "step :  344 loss :  0.07969295233488083\n",
      "step :  345 loss :  0.07969121634960175\n",
      "step :  346 loss :  0.07970137894153595\n",
      "step :  347 loss :  0.07969816029071808\n",
      "step :  348 loss :  0.07969222217798233\n",
      "step :  349 loss :  0.07969430088996887\n",
      "step :  350 loss :  0.07969745248556137\n",
      "step :  351 loss :  0.079692542552948\n",
      "step :  352 loss :  0.07969191670417786\n",
      "step :  353 loss :  0.07969317585229874\n",
      "step :  354 loss :  0.0796981081366539\n",
      "step :  355 loss :  0.07969387620687485\n",
      "step :  356 loss :  0.07969100773334503\n",
      "step :  357 loss :  0.07969356328248978\n",
      "step :  358 loss :  0.07969138771295547\n",
      "step :  359 loss :  0.07969143986701965\n",
      "step :  360 loss :  0.07969554513692856\n",
      "step :  361 loss :  0.07969393581151962\n",
      "step :  362 loss :  0.07969067245721817\n",
      "step :  363 loss :  0.07969358563423157\n",
      "step :  364 loss :  0.07969095557928085\n",
      "step :  365 loss :  0.07969145476818085\n",
      "step :  366 loss :  0.07969343662261963\n",
      "step :  367 loss :  0.07969032227993011\n",
      "step :  368 loss :  0.07969002425670624\n",
      "step :  369 loss :  0.07969149202108383\n",
      "step :  370 loss :  0.079691082239151\n",
      "step :  371 loss :  0.07969086617231369\n",
      "step :  372 loss :  0.07969049364328384\n",
      "step :  373 loss :  0.07969265431165695\n",
      "step :  374 loss :  0.07969294488430023\n",
      "step :  375 loss :  0.07969019562005997\n",
      "step :  376 loss :  0.07969295978546143\n",
      "step :  377 loss :  0.07969175279140472\n",
      "step :  378 loss :  0.07968954741954803\n",
      "step :  379 loss :  0.07969100028276443\n",
      "step :  380 loss :  0.07968927174806595\n",
      "step :  381 loss :  0.0796896442770958\n",
      "step :  382 loss :  0.07969155162572861\n",
      "step :  383 loss :  0.07968929409980774\n",
      "step :  384 loss :  0.07969056814908981\n",
      "step :  385 loss :  0.07969050109386444\n",
      "step :  386 loss :  0.0796888917684555\n",
      "step :  387 loss :  0.07968863099813461\n",
      "step :  388 loss :  0.07969159632921219\n",
      "step :  389 loss :  0.07968980818986893\n",
      "step :  390 loss :  0.07969015091657639\n",
      "step :  391 loss :  0.0796922892332077\n",
      "step :  392 loss :  0.07969053834676743\n",
      "step :  393 loss :  0.07968837767839432\n",
      "step :  394 loss :  0.0796893835067749\n",
      "step :  395 loss :  0.07968971878290176\n",
      "step :  396 loss :  0.07968927174806595\n",
      "step :  397 loss :  0.07968892902135849\n",
      "step :  398 loss :  0.07968993484973907\n",
      "step :  399 loss :  0.07968977093696594\n",
      "step :  400 loss :  0.07968872040510178\n",
      "step :  401 loss :  0.07968932390213013\n",
      "step :  402 loss :  0.07968917489051819\n",
      "step :  403 loss :  0.0796889066696167\n",
      "step :  404 loss :  0.07968968152999878\n",
      "step :  405 loss :  0.07968976348638535\n",
      "step :  406 loss :  0.07968872785568237\n",
      "step :  407 loss :  0.07968969643115997\n",
      "step :  408 loss :  0.07968933135271072\n",
      "step :  409 loss :  0.07968839257955551\n",
      "step :  410 loss :  0.07968845218420029\n",
      "step :  411 loss :  0.07968921959400177\n",
      "step :  412 loss :  0.07968820631504059\n",
      "step :  413 loss :  0.0796891301870346\n",
      "step :  414 loss :  0.07968892902135849\n",
      "step :  415 loss :  0.07968782633543015\n",
      "step :  416 loss :  0.07968837022781372\n",
      "step :  417 loss :  0.07968925684690475\n",
      "step :  418 loss :  0.07968833297491074\n",
      "step :  419 loss :  0.07968799024820328\n",
      "step :  420 loss :  0.07968806475400925\n",
      "step :  421 loss :  0.07968824356794357\n",
      "step :  422 loss :  0.07968836277723312\n",
      "step :  423 loss :  0.07968832552433014\n",
      "step :  424 loss :  0.07968804240226746\n",
      "step :  425 loss :  0.07968873530626297\n",
      "step :  426 loss :  0.07968810945749283\n",
      "step :  427 loss :  0.07968847453594208\n",
      "step :  428 loss :  0.07968854159116745\n",
      "step :  429 loss :  0.07968826591968536\n",
      "step :  430 loss :  0.07968810200691223\n",
      "step :  431 loss :  0.0796881839632988\n",
      "step :  432 loss :  0.07968785613775253\n",
      "step :  433 loss :  0.07968835532665253\n",
      "step :  434 loss :  0.07968780398368835\n",
      "step :  435 loss :  0.0796879231929779\n",
      "step :  436 loss :  0.07968775182962418\n",
      "step :  437 loss :  0.07968807220458984\n",
      "step :  438 loss :  0.0796874389052391\n",
      "step :  439 loss :  0.07968766987323761\n",
      "step :  440 loss :  0.07968780398368835\n",
      "step :  441 loss :  0.07968801259994507\n",
      "step :  442 loss :  0.07968749850988388\n",
      "step :  443 loss :  0.07968760281801224\n",
      "step :  444 loss :  0.07968755066394806\n",
      "step :  445 loss :  0.07968753576278687\n",
      "step :  446 loss :  0.07968756556510925\n",
      "step :  447 loss :  0.07968780398368835\n",
      "step :  448 loss :  0.07968762516975403\n",
      "step :  449 loss :  0.07968790084123611\n",
      "step :  450 loss :  0.07968741655349731\n",
      "step :  451 loss :  0.07968781143426895\n",
      "step :  452 loss :  0.07968747615814209\n",
      "step :  453 loss :  0.07968736439943314\n",
      "step :  454 loss :  0.07968811690807343\n",
      "step :  455 loss :  0.07968755811452866\n",
      "step :  456 loss :  0.07968728989362717\n",
      "step :  457 loss :  0.07968761771917343\n",
      "step :  458 loss :  0.07968738675117493\n",
      "step :  459 loss :  0.07968749105930328\n",
      "step :  460 loss :  0.07968726009130478\n",
      "step :  461 loss :  0.07968725264072418\n",
      "step :  462 loss :  0.07968720048666\n",
      "step :  463 loss :  0.0796874538064003\n",
      "step :  464 loss :  0.0796872153878212\n",
      "step :  465 loss :  0.07968737930059433\n",
      "step :  466 loss :  0.07968733459711075\n",
      "step :  467 loss :  0.07968708872795105\n",
      "step :  468 loss :  0.07968723773956299\n",
      "step :  469 loss :  0.07968704402446747\n",
      "step :  470 loss :  0.07968714088201523\n",
      "step :  471 loss :  0.07968705892562866\n",
      "step :  472 loss :  0.07968705147504807\n",
      "step :  473 loss :  0.07968718558549881\n",
      "step :  474 loss :  0.07968726754188538\n",
      "step :  475 loss :  0.0796872228384018\n",
      "step :  476 loss :  0.07968711853027344\n",
      "step :  477 loss :  0.07968716323375702\n",
      "step :  478 loss :  0.07968702167272568\n",
      "step :  479 loss :  0.07968693226575851\n",
      "step :  480 loss :  0.07968705892562866\n",
      "step :  481 loss :  0.0796869769692421\n",
      "step :  482 loss :  0.07968685030937195\n",
      "step :  483 loss :  0.07968702167272568\n",
      "step :  484 loss :  0.07968704402446747\n",
      "step :  485 loss :  0.07968689501285553\n",
      "step :  486 loss :  0.07968716323375702\n",
      "step :  487 loss :  0.0796869695186615\n",
      "step :  488 loss :  0.07968683540821075\n",
      "step :  489 loss :  0.07968705147504807\n",
      "step :  490 loss :  0.07968685775995255\n",
      "step :  491 loss :  0.07968688756227493\n",
      "step :  492 loss :  0.07968682795763016\n",
      "step :  493 loss :  0.07968689501285553\n",
      "step :  494 loss :  0.07968683540821075\n",
      "step :  495 loss :  0.07968682795763016\n",
      "step :  496 loss :  0.07968685030937195\n",
      "step :  497 loss :  0.07968692481517792\n",
      "step :  498 loss :  0.07968690991401672\n",
      "step :  499 loss :  0.07968685775995255\n",
      "Training model 5\n",
      "step :  0 loss :  0.3513351082801819\n",
      "step :  1 loss :  0.255074143409729\n",
      "step :  2 loss :  0.2019203007221222\n",
      "step :  3 loss :  0.22901806235313416\n",
      "step :  4 loss :  0.1908859759569168\n",
      "step :  5 loss :  0.21385538578033447\n",
      "step :  6 loss :  0.20396524667739868\n",
      "step :  7 loss :  0.23964165151119232\n",
      "step :  8 loss :  0.19215703010559082\n",
      "step :  9 loss :  0.18355503678321838\n",
      "step :  10 loss :  0.18342959880828857\n",
      "step :  11 loss :  0.1734676957130432\n",
      "step :  12 loss :  0.16403643786907196\n",
      "step :  13 loss :  0.15876811742782593\n",
      "step :  14 loss :  0.14624066650867462\n",
      "step :  15 loss :  0.12980836629867554\n",
      "step :  16 loss :  0.11392676830291748\n",
      "step :  17 loss :  0.10947831720113754\n",
      "step :  18 loss :  0.11201778054237366\n",
      "step :  19 loss :  0.22177687287330627\n",
      "step :  20 loss :  0.24113038182258606\n",
      "step :  21 loss :  0.1258663684129715\n",
      "step :  22 loss :  0.12891168892383575\n",
      "step :  23 loss :  0.1512191742658615\n",
      "step :  24 loss :  0.12751665711402893\n",
      "step :  25 loss :  0.1167546734213829\n",
      "step :  26 loss :  0.116461381316185\n",
      "step :  27 loss :  0.10429499298334122\n",
      "step :  28 loss :  0.10160364210605621\n",
      "step :  29 loss :  0.1006729006767273\n",
      "step :  30 loss :  0.11263593286275864\n",
      "step :  31 loss :  0.10366763174533844\n",
      "step :  32 loss :  0.11089833825826645\n",
      "step :  33 loss :  0.12302292883396149\n",
      "step :  34 loss :  0.0981036052107811\n",
      "step :  35 loss :  0.09809689223766327\n",
      "step :  36 loss :  0.09618306905031204\n",
      "step :  37 loss :  0.09187669306993484\n",
      "step :  38 loss :  0.11790579557418823\n",
      "step :  39 loss :  0.09923054277896881\n",
      "step :  40 loss :  0.0904502421617508\n",
      "step :  41 loss :  0.08802024275064468\n",
      "step :  42 loss :  0.08660139888525009\n",
      "step :  43 loss :  0.09844111651182175\n",
      "step :  44 loss :  0.08454813063144684\n",
      "step :  45 loss :  0.11320171505212784\n",
      "step :  46 loss :  0.09065037220716476\n",
      "step :  47 loss :  0.08718637377023697\n",
      "step :  48 loss :  0.0878724679350853\n",
      "step :  49 loss :  0.0831558033823967\n",
      "step :  50 loss :  0.09040965884923935\n",
      "step :  51 loss :  0.09088189154863358\n",
      "step :  52 loss :  0.0953499972820282\n",
      "step :  53 loss :  0.09064527601003647\n",
      "step :  54 loss :  0.08501752465963364\n",
      "step :  55 loss :  0.08261386305093765\n",
      "step :  56 loss :  0.08224482834339142\n",
      "step :  57 loss :  0.0837898924946785\n",
      "step :  58 loss :  0.08440208435058594\n",
      "step :  59 loss :  0.0870194211602211\n",
      "step :  60 loss :  0.08521418273448944\n",
      "step :  61 loss :  0.09769026190042496\n",
      "step :  62 loss :  0.08433951437473297\n",
      "step :  63 loss :  0.08207745850086212\n",
      "step :  64 loss :  0.08177486062049866\n",
      "step :  65 loss :  0.08203083276748657\n",
      "step :  66 loss :  0.08204390853643417\n",
      "step :  67 loss :  0.08490937948226929\n",
      "step :  68 loss :  0.08459165692329407\n",
      "step :  69 loss :  0.08791478723287582\n",
      "step :  70 loss :  0.08256501704454422\n",
      "step :  71 loss :  0.0840403214097023\n",
      "step :  72 loss :  0.08431188762187958\n",
      "step :  73 loss :  0.08226177096366882\n",
      "step :  74 loss :  0.08431293815374374\n",
      "step :  75 loss :  0.08610336482524872\n",
      "step :  76 loss :  0.08282150328159332\n",
      "step :  77 loss :  0.08245901763439178\n",
      "step :  78 loss :  0.08349869400262833\n",
      "step :  79 loss :  0.08064130693674088\n",
      "step :  80 loss :  0.08816178888082504\n",
      "step :  81 loss :  0.08303757011890411\n",
      "step :  82 loss :  0.08384740352630615\n",
      "step :  83 loss :  0.08251053839921951\n",
      "step :  84 loss :  0.08107408881187439\n",
      "step :  85 loss :  0.08102240413427353\n",
      "step :  86 loss :  0.08453107625246048\n",
      "step :  87 loss :  0.08126389980316162\n",
      "step :  88 loss :  0.08343495428562164\n",
      "step :  89 loss :  0.08556396514177322\n",
      "step :  90 loss :  0.08318910747766495\n",
      "step :  91 loss :  0.08290144056081772\n",
      "step :  92 loss :  0.08095822483301163\n",
      "step :  93 loss :  0.08063795417547226\n",
      "step :  94 loss :  0.08120038360357285\n",
      "step :  95 loss :  0.08052869141101837\n",
      "step :  96 loss :  0.08052971214056015\n",
      "step :  97 loss :  0.08104858547449112\n",
      "step :  98 loss :  0.08042512834072113\n",
      "step :  99 loss :  0.08048878610134125\n",
      "step :  100 loss :  0.08074009418487549\n",
      "step :  101 loss :  0.08367351442575455\n",
      "step :  102 loss :  0.08064184337854385\n",
      "step :  103 loss :  0.08021843433380127\n",
      "step :  104 loss :  0.08252660930156708\n",
      "step :  105 loss :  0.08247249573469162\n",
      "step :  106 loss :  0.08085881918668747\n",
      "step :  107 loss :  0.0802447497844696\n",
      "step :  108 loss :  0.0803271010518074\n",
      "step :  109 loss :  0.08304907381534576\n",
      "step :  110 loss :  0.08369185030460358\n",
      "step :  111 loss :  0.08109277486801147\n",
      "step :  112 loss :  0.08316684514284134\n",
      "step :  113 loss :  0.0814289078116417\n",
      "step :  114 loss :  0.08004654198884964\n",
      "step :  115 loss :  0.08146532624959946\n",
      "step :  116 loss :  0.08049774169921875\n",
      "step :  117 loss :  0.08007713407278061\n",
      "step :  118 loss :  0.08024099469184875\n",
      "step :  119 loss :  0.08057821542024612\n",
      "step :  120 loss :  0.07994415611028671\n",
      "step :  121 loss :  0.08040585368871689\n",
      "step :  122 loss :  0.08005639910697937\n",
      "step :  123 loss :  0.08004464954137802\n",
      "step :  124 loss :  0.07982364296913147\n",
      "step :  125 loss :  0.0803200751543045\n",
      "step :  126 loss :  0.07991732656955719\n",
      "step :  127 loss :  0.08043412864208221\n",
      "step :  128 loss :  0.08076201379299164\n",
      "step :  129 loss :  0.08031672984361649\n",
      "step :  130 loss :  0.07976242899894714\n",
      "step :  131 loss :  0.0797019675374031\n",
      "step :  132 loss :  0.07975206524133682\n",
      "step :  133 loss :  0.07950399070978165\n",
      "step :  134 loss :  0.07960590720176697\n",
      "step :  135 loss :  0.08017416298389435\n",
      "step :  136 loss :  0.08012716472148895\n",
      "step :  137 loss :  0.07966429740190506\n",
      "step :  138 loss :  0.07992248982191086\n",
      "step :  139 loss :  0.08000809699296951\n",
      "step :  140 loss :  0.08044955879449844\n",
      "step :  141 loss :  0.07953371852636337\n",
      "step :  142 loss :  0.08012881875038147\n",
      "step :  143 loss :  0.07986649125814438\n",
      "step :  144 loss :  0.0802263468503952\n",
      "step :  145 loss :  0.07986573874950409\n",
      "step :  146 loss :  0.08016692847013474\n",
      "step :  147 loss :  0.07948827743530273\n",
      "step :  148 loss :  0.08005828410387039\n",
      "step :  149 loss :  0.07973208278417587\n",
      "step :  150 loss :  0.08004143089056015\n",
      "step :  151 loss :  0.07960279285907745\n",
      "step :  152 loss :  0.07977800071239471\n",
      "step :  153 loss :  0.07940682023763657\n",
      "step :  154 loss :  0.07964347302913666\n",
      "step :  155 loss :  0.0794074609875679\n",
      "step :  156 loss :  0.07950764149427414\n",
      "step :  157 loss :  0.07933778315782547\n",
      "step :  158 loss :  0.07959255576133728\n",
      "step :  159 loss :  0.079286128282547\n",
      "step :  160 loss :  0.07927284389734268\n",
      "step :  161 loss :  0.07931259274482727\n",
      "step :  162 loss :  0.07932772487401962\n",
      "step :  163 loss :  0.07936684042215347\n",
      "step :  164 loss :  0.07930833101272583\n",
      "step :  165 loss :  0.07932227104902267\n",
      "step :  166 loss :  0.07927350699901581\n",
      "step :  167 loss :  0.0792199969291687\n",
      "step :  168 loss :  0.07923126220703125\n",
      "step :  169 loss :  0.07920816540718079\n",
      "step :  170 loss :  0.07922188192605972\n",
      "step :  171 loss :  0.07945986092090607\n",
      "step :  172 loss :  0.0792962908744812\n",
      "step :  173 loss :  0.07921963185071945\n",
      "step :  174 loss :  0.07927817106246948\n",
      "step :  175 loss :  0.07937684655189514\n",
      "step :  176 loss :  0.07922114431858063\n",
      "step :  177 loss :  0.07941466569900513\n",
      "step :  178 loss :  0.07922090590000153\n",
      "step :  179 loss :  0.07947640120983124\n",
      "step :  180 loss :  0.07936812192201614\n",
      "step :  181 loss :  0.0794026330113411\n",
      "step :  182 loss :  0.079292893409729\n",
      "step :  183 loss :  0.07940490543842316\n",
      "step :  184 loss :  0.07928398251533508\n",
      "step :  185 loss :  0.0793774202466011\n",
      "step :  186 loss :  0.07928060740232468\n",
      "step :  187 loss :  0.07945442199707031\n",
      "step :  188 loss :  0.07913314551115036\n",
      "step :  189 loss :  0.07933159172534943\n",
      "step :  190 loss :  0.07913045585155487\n",
      "step :  191 loss :  0.07928448170423508\n",
      "step :  192 loss :  0.07921472936868668\n",
      "step :  193 loss :  0.07921192049980164\n",
      "step :  194 loss :  0.07929211109876633\n",
      "step :  195 loss :  0.0793953537940979\n",
      "step :  196 loss :  0.07912380248308182\n",
      "step :  197 loss :  0.07926991581916809\n",
      "step :  198 loss :  0.07911767810583115\n",
      "step :  199 loss :  0.07926354557275772\n",
      "step :  200 loss :  0.0791710689663887\n",
      "step :  201 loss :  0.07917199283838272\n",
      "step :  202 loss :  0.07910877466201782\n",
      "step :  203 loss :  0.07923712581396103\n",
      "step :  204 loss :  0.07913920283317566\n",
      "step :  205 loss :  0.07930649816989899\n",
      "step :  206 loss :  0.07913544774055481\n",
      "step :  207 loss :  0.07926733046770096\n",
      "step :  208 loss :  0.07911274582147598\n",
      "step :  209 loss :  0.07926372438669205\n",
      "step :  210 loss :  0.079086534678936\n",
      "step :  211 loss :  0.0792214646935463\n",
      "step :  212 loss :  0.07910716533660889\n",
      "step :  213 loss :  0.07922182232141495\n",
      "step :  214 loss :  0.07909669727087021\n",
      "step :  215 loss :  0.07915395498275757\n",
      "step :  216 loss :  0.07908802479505539\n",
      "step :  217 loss :  0.07918258011341095\n",
      "step :  218 loss :  0.07906598597764969\n",
      "step :  219 loss :  0.07915016263723373\n",
      "step :  220 loss :  0.07906202226877213\n",
      "step :  221 loss :  0.07913707196712494\n",
      "step :  222 loss :  0.07906762510538101\n",
      "step :  223 loss :  0.07914641499519348\n",
      "step :  224 loss :  0.07907303422689438\n",
      "step :  225 loss :  0.07914751768112183\n",
      "step :  226 loss :  0.07906799018383026\n",
      "step :  227 loss :  0.07914730906486511\n",
      "step :  228 loss :  0.07905329763889313\n",
      "step :  229 loss :  0.07907167077064514\n",
      "step :  230 loss :  0.07904928922653198\n",
      "step :  231 loss :  0.07909881323575974\n",
      "step :  232 loss :  0.0790766254067421\n",
      "step :  233 loss :  0.07917274534702301\n",
      "step :  234 loss :  0.07904676347970963\n",
      "step :  235 loss :  0.07905995845794678\n",
      "step :  236 loss :  0.07905101776123047\n",
      "step :  237 loss :  0.07905108481645584\n",
      "step :  238 loss :  0.07905091345310211\n",
      "step :  239 loss :  0.07904525846242905\n",
      "step :  240 loss :  0.07905226945877075\n",
      "step :  241 loss :  0.07903844863176346\n",
      "step :  242 loss :  0.07906517386436462\n",
      "step :  243 loss :  0.0790431797504425\n",
      "step :  244 loss :  0.0790955126285553\n",
      "step :  245 loss :  0.079036645591259\n",
      "step :  246 loss :  0.07908446341753006\n",
      "step :  247 loss :  0.07904195785522461\n",
      "step :  248 loss :  0.07904276996850967\n",
      "step :  249 loss :  0.07903356105089188\n",
      "step :  250 loss :  0.07903182506561279\n",
      "step :  251 loss :  0.07903330028057098\n",
      "step :  252 loss :  0.07903194427490234\n",
      "step :  253 loss :  0.07903303951025009\n",
      "step :  254 loss :  0.07902652770280838\n",
      "step :  255 loss :  0.07903392612934113\n",
      "step :  256 loss :  0.07902824878692627\n",
      "step :  257 loss :  0.07902921736240387\n",
      "step :  258 loss :  0.07902435958385468\n",
      "step :  259 loss :  0.07901895046234131\n",
      "step :  260 loss :  0.07903846353292465\n",
      "step :  261 loss :  0.07902980595827103\n",
      "step :  262 loss :  0.07907184958457947\n",
      "step :  263 loss :  0.07901723682880402\n",
      "step :  264 loss :  0.07902079820632935\n",
      "step :  265 loss :  0.07902370393276215\n",
      "step :  266 loss :  0.07901657372713089\n",
      "step :  267 loss :  0.07903026789426804\n",
      "step :  268 loss :  0.0790143758058548\n",
      "step :  269 loss :  0.07901973277330399\n",
      "step :  270 loss :  0.07901395857334137\n",
      "step :  271 loss :  0.07901312410831451\n",
      "step :  272 loss :  0.07900917530059814\n",
      "step :  273 loss :  0.07901620119810104\n",
      "step :  274 loss :  0.07900924980640411\n",
      "step :  275 loss :  0.07904056459665298\n",
      "step :  276 loss :  0.07901178300380707\n",
      "step :  277 loss :  0.07902269810438156\n",
      "step :  278 loss :  0.07900843024253845\n",
      "step :  279 loss :  0.07901158928871155\n",
      "step :  280 loss :  0.07901595532894135\n",
      "step :  281 loss :  0.07900503277778625\n",
      "step :  282 loss :  0.07900913059711456\n",
      "step :  283 loss :  0.07900388538837433\n",
      "step :  284 loss :  0.07900449633598328\n",
      "step :  285 loss :  0.07900333404541016\n",
      "step :  286 loss :  0.07901918888092041\n",
      "step :  287 loss :  0.07901282608509064\n",
      "step :  288 loss :  0.07900729030370712\n",
      "step :  289 loss :  0.07900387048721313\n",
      "step :  290 loss :  0.07900508493185043\n",
      "step :  291 loss :  0.07900216430425644\n",
      "step :  292 loss :  0.0790204182267189\n",
      "step :  293 loss :  0.0790046975016594\n",
      "step :  294 loss :  0.07901434600353241\n",
      "step :  295 loss :  0.07899997383356094\n",
      "step :  296 loss :  0.07900272309780121\n",
      "step :  297 loss :  0.0790080800652504\n",
      "step :  298 loss :  0.07899980247020721\n",
      "step :  299 loss :  0.0790042132139206\n",
      "step :  300 loss :  0.07900302857160568\n",
      "step :  301 loss :  0.07900051772594452\n",
      "step :  302 loss :  0.07901845872402191\n",
      "step :  303 loss :  0.07899805158376694\n",
      "step :  304 loss :  0.0790071114897728\n",
      "step :  305 loss :  0.07900157570838928\n",
      "step :  306 loss :  0.07900028675794601\n",
      "step :  307 loss :  0.07900502532720566\n",
      "step :  308 loss :  0.07899606227874756\n",
      "step :  309 loss :  0.07899662852287292\n",
      "step :  310 loss :  0.07900425791740417\n",
      "step :  311 loss :  0.07899673283100128\n",
      "step :  312 loss :  0.0790032371878624\n",
      "step :  313 loss :  0.0789957344532013\n",
      "step :  314 loss :  0.07899507880210876\n",
      "step :  315 loss :  0.07900750637054443\n",
      "step :  316 loss :  0.07899390161037445\n",
      "step :  317 loss :  0.07899453490972519\n",
      "step :  318 loss :  0.07899415493011475\n",
      "step :  319 loss :  0.07899411767721176\n",
      "step :  320 loss :  0.07899178564548492\n",
      "step :  321 loss :  0.07899569720029831\n",
      "step :  322 loss :  0.0789913684129715\n",
      "step :  323 loss :  0.07899079471826553\n",
      "step :  324 loss :  0.07899002730846405\n",
      "step :  325 loss :  0.07899154722690582\n",
      "step :  326 loss :  0.07899170368909836\n",
      "step :  327 loss :  0.07899775356054306\n",
      "step :  328 loss :  0.07899178564548492\n",
      "step :  329 loss :  0.07898950576782227\n",
      "step :  330 loss :  0.0789993479847908\n",
      "step :  331 loss :  0.07899115979671478\n",
      "step :  332 loss :  0.07899114489555359\n",
      "step :  333 loss :  0.07899057120084763\n",
      "step :  334 loss :  0.07899133116006851\n",
      "step :  335 loss :  0.0789887011051178\n",
      "step :  336 loss :  0.07899125665426254\n",
      "step :  337 loss :  0.07898984849452972\n",
      "step :  338 loss :  0.07898999005556107\n",
      "step :  339 loss :  0.07898960262537003\n",
      "step :  340 loss :  0.07899229973554611\n",
      "step :  341 loss :  0.07898861914873123\n",
      "step :  342 loss :  0.07898854464292526\n",
      "step :  343 loss :  0.07898851484060287\n",
      "step :  344 loss :  0.07898765802383423\n",
      "step :  345 loss :  0.07898837327957153\n",
      "step :  346 loss :  0.07898978888988495\n",
      "step :  347 loss :  0.07898828387260437\n",
      "step :  348 loss :  0.07899332791566849\n",
      "step :  349 loss :  0.07898677885532379\n",
      "step :  350 loss :  0.07898678630590439\n",
      "step :  351 loss :  0.07899082452058792\n",
      "step :  352 loss :  0.07898814976215363\n",
      "step :  353 loss :  0.07898832112550735\n",
      "step :  354 loss :  0.07898993790149689\n",
      "step :  355 loss :  0.07898689806461334\n",
      "step :  356 loss :  0.07898899167776108\n",
      "step :  357 loss :  0.07898689061403275\n",
      "step :  358 loss :  0.0789869874715805\n",
      "step :  359 loss :  0.07898841798305511\n",
      "step :  360 loss :  0.07898689806461334\n",
      "step :  361 loss :  0.07898571342229843\n",
      "step :  362 loss :  0.07898806780576706\n",
      "step :  363 loss :  0.07898654043674469\n",
      "step :  364 loss :  0.07898833602666855\n",
      "step :  365 loss :  0.07898672670125961\n",
      "step :  366 loss :  0.07898771017789841\n",
      "step :  367 loss :  0.07898661494255066\n",
      "step :  368 loss :  0.07898513227701187\n",
      "step :  369 loss :  0.07898572087287903\n",
      "step :  370 loss :  0.07898537069559097\n",
      "step :  371 loss :  0.07898474484682083\n",
      "step :  372 loss :  0.07898689061403275\n",
      "step :  373 loss :  0.07898670434951782\n",
      "step :  374 loss :  0.07898469269275665\n",
      "step :  375 loss :  0.0789843425154686\n",
      "step :  376 loss :  0.07898610085248947\n",
      "step :  377 loss :  0.07898759841918945\n",
      "step :  378 loss :  0.07898416370153427\n",
      "step :  379 loss :  0.07898419350385666\n",
      "step :  380 loss :  0.07898523658514023\n",
      "step :  381 loss :  0.0789853110909462\n",
      "step :  382 loss :  0.07898403704166412\n",
      "step :  383 loss :  0.07898607105016708\n",
      "step :  384 loss :  0.0789840817451477\n",
      "step :  385 loss :  0.0789836049079895\n",
      "step :  386 loss :  0.07898467034101486\n",
      "step :  387 loss :  0.07898436486721039\n",
      "step :  388 loss :  0.07898378372192383\n",
      "step :  389 loss :  0.07898584753274918\n",
      "step :  390 loss :  0.07898371666669846\n",
      "step :  391 loss :  0.07898367196321487\n",
      "step :  392 loss :  0.07898364216089249\n",
      "step :  393 loss :  0.07898341864347458\n",
      "step :  394 loss :  0.07898350059986115\n",
      "step :  395 loss :  0.0789833664894104\n",
      "step :  396 loss :  0.07898319512605667\n",
      "step :  397 loss :  0.07898449152708054\n",
      "step :  398 loss :  0.07898330688476562\n",
      "step :  399 loss :  0.07898341864347458\n",
      "step :  400 loss :  0.0789833515882492\n",
      "step :  401 loss :  0.07898320257663727\n",
      "step :  402 loss :  0.07898393273353577\n",
      "step :  403 loss :  0.07898294925689697\n",
      "step :  404 loss :  0.07898307591676712\n",
      "step :  405 loss :  0.0789833813905716\n",
      "step :  406 loss :  0.07898322492837906\n",
      "step :  407 loss :  0.07898318767547607\n",
      "step :  408 loss :  0.07898296415805817\n",
      "step :  409 loss :  0.07898319512605667\n",
      "step :  410 loss :  0.07898344099521637\n",
      "step :  411 loss :  0.07898261398077011\n",
      "step :  412 loss :  0.0789831206202507\n",
      "step :  413 loss :  0.07898258417844772\n",
      "step :  414 loss :  0.07898274809122086\n",
      "step :  415 loss :  0.07898294180631638\n",
      "step :  416 loss :  0.0789823830127716\n",
      "step :  417 loss :  0.07898297160863876\n",
      "step :  418 loss :  0.07898309826850891\n",
      "step :  419 loss :  0.07898260653018951\n",
      "step :  420 loss :  0.07898217439651489\n",
      "step :  421 loss :  0.07898220419883728\n",
      "step :  422 loss :  0.07898471504449844\n",
      "step :  423 loss :  0.07898257672786713\n",
      "step :  424 loss :  0.07898198813199997\n",
      "step :  425 loss :  0.07898198068141937\n",
      "step :  426 loss :  0.0789828673005104\n",
      "step :  427 loss :  0.0789831131696701\n",
      "step :  428 loss :  0.07898242026567459\n",
      "step :  429 loss :  0.07898212969303131\n",
      "step :  430 loss :  0.07898242026567459\n",
      "step :  431 loss :  0.07898396998643875\n",
      "step :  432 loss :  0.0789823830127716\n",
      "step :  433 loss :  0.07898198068141937\n",
      "step :  434 loss :  0.07898261398077011\n",
      "step :  435 loss :  0.07898199558258057\n",
      "step :  436 loss :  0.07898225635290146\n",
      "step :  437 loss :  0.07898218184709549\n",
      "step :  438 loss :  0.07898301631212234\n",
      "step :  439 loss :  0.07898177206516266\n",
      "step :  440 loss :  0.07898207008838654\n",
      "step :  441 loss :  0.0789821669459343\n",
      "step :  442 loss :  0.07898203283548355\n",
      "step :  443 loss :  0.07898154854774475\n",
      "step :  444 loss :  0.07898155599832535\n",
      "step :  445 loss :  0.07898271083831787\n",
      "step :  446 loss :  0.07898209244012833\n",
      "step :  447 loss :  0.07898137718439102\n",
      "step :  448 loss :  0.0789814442396164\n",
      "step :  449 loss :  0.07898209244012833\n",
      "step :  450 loss :  0.0789819061756134\n",
      "step :  451 loss :  0.07898149639368057\n",
      "step :  452 loss :  0.07898145914077759\n",
      "step :  453 loss :  0.07898169010877609\n",
      "step :  454 loss :  0.07898154109716415\n",
      "step :  455 loss :  0.0789814218878746\n",
      "step :  456 loss :  0.07898175716400146\n",
      "step :  457 loss :  0.07898139208555222\n",
      "step :  458 loss :  0.0789814293384552\n",
      "step :  459 loss :  0.07898125797510147\n",
      "step :  460 loss :  0.07898150384426117\n",
      "step :  461 loss :  0.07898137718439102\n",
      "step :  462 loss :  0.07898147404193878\n",
      "step :  463 loss :  0.07898122817277908\n",
      "step :  464 loss :  0.07898154854774475\n",
      "step :  465 loss :  0.07898128777742386\n",
      "step :  466 loss :  0.07898122072219849\n",
      "step :  467 loss :  0.07898149639368057\n",
      "step :  468 loss :  0.07898125797510147\n",
      "step :  469 loss :  0.07898106426000595\n",
      "step :  470 loss :  0.07898113876581192\n",
      "step :  471 loss :  0.07898139953613281\n",
      "step :  472 loss :  0.07898116856813431\n",
      "step :  473 loss :  0.07898122072219849\n",
      "step :  474 loss :  0.07898120582103729\n",
      "step :  475 loss :  0.07898115366697311\n",
      "step :  476 loss :  0.07898105680942535\n",
      "step :  477 loss :  0.07898114621639252\n",
      "step :  478 loss :  0.07898111641407013\n",
      "step :  479 loss :  0.07898109406232834\n",
      "step :  480 loss :  0.07898102700710297\n",
      "step :  481 loss :  0.07898115366697311\n",
      "step :  482 loss :  0.07898106426000595\n",
      "step :  483 loss :  0.07898106426000595\n",
      "step :  484 loss :  0.07898125797510147\n",
      "step :  485 loss :  0.07898098975419998\n",
      "step :  486 loss :  0.07898110151290894\n",
      "step :  487 loss :  0.07898107916116714\n",
      "step :  488 loss :  0.07898111641407013\n",
      "step :  489 loss :  0.078980952501297\n",
      "step :  490 loss :  0.07898104190826416\n",
      "step :  491 loss :  0.07898100465536118\n",
      "step :  492 loss :  0.07898104190826416\n",
      "step :  493 loss :  0.07898133993148804\n",
      "step :  494 loss :  0.07898089289665222\n",
      "step :  495 loss :  0.07898092269897461\n",
      "step :  496 loss :  0.07898101955652237\n",
      "step :  497 loss :  0.0789809599518776\n",
      "step :  498 loss :  0.07898084074258804\n",
      "step :  499 loss :  0.07898109406232834\n",
      "Training model 6\n",
      "step :  0 loss :  0.39100882411003113\n",
      "step :  1 loss :  0.3806602656841278\n",
      "step :  2 loss :  0.201361283659935\n",
      "step :  3 loss :  0.22291982173919678\n",
      "step :  4 loss :  0.1840744912624359\n",
      "step :  5 loss :  0.1848187893629074\n",
      "step :  6 loss :  0.2238898128271103\n",
      "step :  7 loss :  0.21648573875427246\n",
      "step :  8 loss :  0.19603653252124786\n",
      "step :  9 loss :  0.19261065125465393\n",
      "step :  10 loss :  0.1939413845539093\n",
      "step :  11 loss :  0.17459669709205627\n",
      "step :  12 loss :  0.16848410665988922\n",
      "step :  13 loss :  0.16549716889858246\n",
      "step :  14 loss :  0.16235053539276123\n",
      "step :  15 loss :  0.15419453382492065\n",
      "step :  16 loss :  0.14163751900196075\n",
      "step :  17 loss :  0.1282832771539688\n",
      "step :  18 loss :  0.15074975788593292\n",
      "step :  19 loss :  0.18990002572536469\n",
      "step :  20 loss :  0.1987488716840744\n",
      "step :  21 loss :  0.1105460375547409\n",
      "step :  22 loss :  0.11539845168590546\n",
      "step :  23 loss :  0.11431000381708145\n",
      "step :  24 loss :  0.11266472935676575\n",
      "step :  25 loss :  0.11187996715307236\n",
      "step :  26 loss :  0.11446138471364975\n",
      "step :  27 loss :  0.10355088859796524\n",
      "step :  28 loss :  0.11263514310121536\n",
      "step :  29 loss :  0.12570640444755554\n",
      "step :  30 loss :  0.18363353610038757\n",
      "step :  31 loss :  0.10687814652919769\n",
      "step :  32 loss :  0.09220447391271591\n",
      "step :  33 loss :  0.09351297467947006\n",
      "step :  34 loss :  0.09681172668933868\n",
      "step :  35 loss :  0.09624729305505753\n",
      "step :  36 loss :  0.10019034147262573\n",
      "step :  37 loss :  0.12989085912704468\n",
      "step :  38 loss :  0.0876113697886467\n",
      "step :  39 loss :  0.08941258490085602\n",
      "step :  40 loss :  0.09046898782253265\n",
      "step :  41 loss :  0.11706782132387161\n",
      "step :  42 loss :  0.10309112817049026\n",
      "step :  43 loss :  0.08734772354364395\n",
      "step :  44 loss :  0.0901184156537056\n",
      "step :  45 loss :  0.08521398156881332\n",
      "step :  46 loss :  0.11951051652431488\n",
      "step :  47 loss :  0.0968870297074318\n",
      "step :  48 loss :  0.08629745244979858\n",
      "step :  49 loss :  0.08555140346288681\n",
      "step :  50 loss :  0.08975138515233994\n",
      "step :  51 loss :  0.08740575611591339\n",
      "step :  52 loss :  0.08465413749217987\n",
      "step :  53 loss :  0.09109470248222351\n",
      "step :  54 loss :  0.08515485376119614\n",
      "step :  55 loss :  0.10003549605607986\n",
      "step :  56 loss :  0.08402702957391739\n",
      "step :  57 loss :  0.08488970249891281\n",
      "step :  58 loss :  0.08715669810771942\n",
      "step :  59 loss :  0.08304321765899658\n",
      "step :  60 loss :  0.08730766177177429\n",
      "step :  61 loss :  0.09054327011108398\n",
      "step :  62 loss :  0.09264220297336578\n",
      "step :  63 loss :  0.09120085090398788\n",
      "step :  64 loss :  0.08585306257009506\n",
      "step :  65 loss :  0.08530784398317337\n",
      "step :  66 loss :  0.08502693474292755\n",
      "step :  67 loss :  0.08419822156429291\n",
      "step :  68 loss :  0.08263953775167465\n",
      "step :  69 loss :  0.08621829748153687\n",
      "step :  70 loss :  0.08490583300590515\n",
      "step :  71 loss :  0.08476055413484573\n",
      "step :  72 loss :  0.08540192991495132\n",
      "step :  73 loss :  0.08392501622438431\n",
      "step :  74 loss :  0.08676691353321075\n",
      "step :  75 loss :  0.08299266546964645\n",
      "step :  76 loss :  0.08323971182107925\n",
      "step :  77 loss :  0.08440565317869186\n",
      "step :  78 loss :  0.08195597678422928\n",
      "step :  79 loss :  0.08311786502599716\n",
      "step :  80 loss :  0.08199373632669449\n",
      "step :  81 loss :  0.08758310228586197\n",
      "step :  82 loss :  0.08377258479595184\n",
      "step :  83 loss :  0.08435296267271042\n",
      "step :  84 loss :  0.08806243538856506\n",
      "step :  85 loss :  0.08777683973312378\n",
      "step :  86 loss :  0.0843319296836853\n",
      "step :  87 loss :  0.08276623487472534\n",
      "step :  88 loss :  0.08190067857503891\n",
      "step :  89 loss :  0.08367442339658737\n",
      "step :  90 loss :  0.08373774588108063\n",
      "step :  91 loss :  0.08234617859125137\n",
      "step :  92 loss :  0.08584645390510559\n",
      "step :  93 loss :  0.08404640853404999\n",
      "step :  94 loss :  0.08324125409126282\n",
      "step :  95 loss :  0.08228391408920288\n",
      "step :  96 loss :  0.08329643309116364\n",
      "step :  97 loss :  0.08456943184137344\n",
      "step :  98 loss :  0.08264312893152237\n",
      "step :  99 loss :  0.08233669400215149\n",
      "step :  100 loss :  0.08169292658567429\n",
      "step :  101 loss :  0.08181635290384293\n",
      "step :  102 loss :  0.08145661652088165\n",
      "step :  103 loss :  0.08145610243082047\n",
      "step :  104 loss :  0.08140302449464798\n",
      "step :  105 loss :  0.0814264789223671\n",
      "step :  106 loss :  0.08234206587076187\n",
      "step :  107 loss :  0.08143435418605804\n",
      "step :  108 loss :  0.08133529871702194\n",
      "step :  109 loss :  0.08135490119457245\n",
      "step :  110 loss :  0.08105999231338501\n",
      "step :  111 loss :  0.08116292953491211\n",
      "step :  112 loss :  0.08109470456838608\n",
      "step :  113 loss :  0.0812058374285698\n",
      "step :  114 loss :  0.08118809759616852\n",
      "step :  115 loss :  0.08100873976945877\n",
      "step :  116 loss :  0.08211558312177658\n",
      "step :  117 loss :  0.08155524730682373\n",
      "step :  118 loss :  0.08107534795999527\n",
      "step :  119 loss :  0.08128156512975693\n",
      "step :  120 loss :  0.0809168890118599\n",
      "step :  121 loss :  0.08182815462350845\n",
      "step :  122 loss :  0.0813429057598114\n",
      "step :  123 loss :  0.0808144360780716\n",
      "step :  124 loss :  0.08102355152368546\n",
      "step :  125 loss :  0.08106821030378342\n",
      "step :  126 loss :  0.08159906417131424\n",
      "step :  127 loss :  0.08092436194419861\n",
      "step :  128 loss :  0.08094397932291031\n",
      "step :  129 loss :  0.08095228672027588\n",
      "step :  130 loss :  0.08089196681976318\n",
      "step :  131 loss :  0.08148231357336044\n",
      "step :  132 loss :  0.08165512979030609\n",
      "step :  133 loss :  0.08093581348657608\n",
      "step :  134 loss :  0.08086443692445755\n",
      "step :  135 loss :  0.08071843534708023\n",
      "step :  136 loss :  0.08085112273693085\n",
      "step :  137 loss :  0.08063071966171265\n",
      "step :  138 loss :  0.08073005825281143\n",
      "step :  139 loss :  0.08071643114089966\n",
      "step :  140 loss :  0.08076319098472595\n",
      "step :  141 loss :  0.0807390809059143\n",
      "step :  142 loss :  0.08083564043045044\n",
      "step :  143 loss :  0.08063209056854248\n",
      "step :  144 loss :  0.08075137436389923\n",
      "step :  145 loss :  0.08054734021425247\n",
      "step :  146 loss :  0.08064108341932297\n",
      "step :  147 loss :  0.08055174350738525\n",
      "step :  148 loss :  0.08055330812931061\n",
      "step :  149 loss :  0.08056046813726425\n",
      "step :  150 loss :  0.08052719384431839\n",
      "step :  151 loss :  0.08068425208330154\n",
      "step :  152 loss :  0.08048677444458008\n",
      "step :  153 loss :  0.08093268424272537\n",
      "step :  154 loss :  0.08058026432991028\n",
      "step :  155 loss :  0.08054114133119583\n",
      "step :  156 loss :  0.0806257426738739\n",
      "step :  157 loss :  0.080511674284935\n",
      "step :  158 loss :  0.08062590658664703\n",
      "step :  159 loss :  0.08056680113077164\n",
      "step :  160 loss :  0.08054053783416748\n",
      "step :  161 loss :  0.08050363510847092\n",
      "step :  162 loss :  0.08046860992908478\n",
      "step :  163 loss :  0.08042574673891068\n",
      "step :  164 loss :  0.08053925633430481\n",
      "step :  165 loss :  0.08048414438962936\n",
      "step :  166 loss :  0.08048746734857559\n",
      "step :  167 loss :  0.0804242491722107\n",
      "step :  168 loss :  0.08040377497673035\n",
      "step :  169 loss :  0.08033823221921921\n",
      "step :  170 loss :  0.08036194741725922\n",
      "step :  171 loss :  0.08036311715841293\n",
      "step :  172 loss :  0.08040735125541687\n",
      "step :  173 loss :  0.08033159375190735\n",
      "step :  174 loss :  0.08032135665416718\n",
      "step :  175 loss :  0.08046509325504303\n",
      "step :  176 loss :  0.0803619846701622\n",
      "step :  177 loss :  0.08033810555934906\n",
      "step :  178 loss :  0.08038177341222763\n",
      "step :  179 loss :  0.08029452711343765\n",
      "step :  180 loss :  0.08033142238855362\n",
      "step :  181 loss :  0.08025041967630386\n",
      "step :  182 loss :  0.0803089514374733\n",
      "step :  183 loss :  0.0802544578909874\n",
      "step :  184 loss :  0.08034797757863998\n",
      "step :  185 loss :  0.08031933754682541\n",
      "step :  186 loss :  0.0803229957818985\n",
      "step :  187 loss :  0.08035114407539368\n",
      "step :  188 loss :  0.08031028509140015\n",
      "step :  189 loss :  0.08028977364301682\n",
      "step :  190 loss :  0.080282062292099\n",
      "step :  191 loss :  0.08037614077329636\n",
      "step :  192 loss :  0.0802832692861557\n",
      "step :  193 loss :  0.08020151406526566\n",
      "step :  194 loss :  0.08031034469604492\n",
      "step :  195 loss :  0.08025703579187393\n",
      "step :  196 loss :  0.0801892951130867\n",
      "step :  197 loss :  0.08024141192436218\n",
      "step :  198 loss :  0.0801667869091034\n",
      "step :  199 loss :  0.08021125942468643\n",
      "step :  200 loss :  0.08022601902484894\n",
      "step :  201 loss :  0.08020041882991791\n",
      "step :  202 loss :  0.08022084832191467\n",
      "step :  203 loss :  0.0801813155412674\n",
      "step :  204 loss :  0.08018884807825089\n",
      "step :  205 loss :  0.08024042099714279\n",
      "step :  206 loss :  0.08018627017736435\n",
      "step :  207 loss :  0.08017012476921082\n",
      "step :  208 loss :  0.08022104203701019\n",
      "step :  209 loss :  0.08022264391183853\n",
      "step :  210 loss :  0.08017603307962418\n",
      "step :  211 loss :  0.08019201457500458\n",
      "step :  212 loss :  0.08017003536224365\n",
      "step :  213 loss :  0.08018970489501953\n",
      "step :  214 loss :  0.08022113144397736\n",
      "step :  215 loss :  0.08019525557756424\n",
      "step :  216 loss :  0.0801897794008255\n",
      "step :  217 loss :  0.08019663393497467\n",
      "step :  218 loss :  0.08016851544380188\n",
      "step :  219 loss :  0.08017869293689728\n",
      "step :  220 loss :  0.08017386496067047\n",
      "step :  221 loss :  0.08016040921211243\n",
      "step :  222 loss :  0.08016369491815567\n",
      "step :  223 loss :  0.08021970093250275\n",
      "step :  224 loss :  0.08020425587892532\n",
      "step :  225 loss :  0.08015496283769608\n",
      "step :  226 loss :  0.08018920570611954\n",
      "step :  227 loss :  0.08019884675741196\n",
      "step :  228 loss :  0.080128513276577\n",
      "step :  229 loss :  0.0801403596997261\n",
      "step :  230 loss :  0.08015713095664978\n",
      "step :  231 loss :  0.08014058321714401\n",
      "step :  232 loss :  0.08012305945158005\n",
      "step :  233 loss :  0.0801413506269455\n",
      "step :  234 loss :  0.08014344424009323\n",
      "step :  235 loss :  0.08015020936727524\n",
      "step :  236 loss :  0.08014886826276779\n",
      "step :  237 loss :  0.08015307039022446\n",
      "step :  238 loss :  0.0801486149430275\n",
      "step :  239 loss :  0.08014669269323349\n",
      "step :  240 loss :  0.0801478773355484\n",
      "step :  241 loss :  0.08015159517526627\n",
      "step :  242 loss :  0.0801468938589096\n",
      "step :  243 loss :  0.08014564961194992\n",
      "step :  244 loss :  0.08014286309480667\n",
      "step :  245 loss :  0.08015163987874985\n",
      "step :  246 loss :  0.08014623075723648\n",
      "step :  247 loss :  0.08015408366918564\n",
      "step :  248 loss :  0.08015400916337967\n",
      "step :  249 loss :  0.0801449567079544\n",
      "step :  250 loss :  0.08014363050460815\n",
      "step :  251 loss :  0.08014652878046036\n",
      "step :  252 loss :  0.08013574779033661\n",
      "step :  253 loss :  0.08014112710952759\n",
      "step :  254 loss :  0.0801536813378334\n",
      "step :  255 loss :  0.08014349639415741\n",
      "step :  256 loss :  0.08013677597045898\n",
      "step :  257 loss :  0.08014608174562454\n",
      "step :  258 loss :  0.08015654981136322\n",
      "step :  259 loss :  0.08014114201068878\n",
      "step :  260 loss :  0.08014832437038422\n",
      "step :  261 loss :  0.08014169335365295\n",
      "step :  262 loss :  0.08014854788780212\n",
      "step :  263 loss :  0.08014290034770966\n",
      "step :  264 loss :  0.08014026284217834\n",
      "step :  265 loss :  0.08014819771051407\n",
      "step :  266 loss :  0.08014260977506638\n",
      "step :  267 loss :  0.08014080673456192\n",
      "step :  268 loss :  0.08013834059238434\n",
      "step :  269 loss :  0.08013898134231567\n",
      "step :  270 loss :  0.08013298362493515\n",
      "step :  271 loss :  0.08012401312589645\n",
      "step :  272 loss :  0.0801323652267456\n",
      "step :  273 loss :  0.0801311656832695\n",
      "step :  274 loss :  0.08012352138757706\n",
      "step :  275 loss :  0.08013975620269775\n",
      "step :  276 loss :  0.08013007789850235\n",
      "step :  277 loss :  0.08013039082288742\n",
      "step :  278 loss :  0.08012749254703522\n",
      "step :  279 loss :  0.08013086020946503\n",
      "step :  280 loss :  0.08012723922729492\n",
      "step :  281 loss :  0.08013193309307098\n",
      "step :  282 loss :  0.08013128489255905\n",
      "step :  283 loss :  0.08012548089027405\n",
      "step :  284 loss :  0.08012169599533081\n",
      "step :  285 loss :  0.08012992888689041\n",
      "step :  286 loss :  0.08012401312589645\n",
      "step :  287 loss :  0.0801219493150711\n",
      "step :  288 loss :  0.08012748509645462\n",
      "step :  289 loss :  0.08012615144252777\n",
      "step :  290 loss :  0.08012232929468155\n",
      "step :  291 loss :  0.08012206852436066\n",
      "step :  292 loss :  0.08012240380048752\n",
      "step :  293 loss :  0.08012651652097702\n",
      "step :  294 loss :  0.08012066036462784\n",
      "step :  295 loss :  0.08012477308511734\n",
      "step :  296 loss :  0.08012193441390991\n",
      "step :  297 loss :  0.08012484759092331\n",
      "step :  298 loss :  0.08011582493782043\n",
      "step :  299 loss :  0.08011563867330551\n",
      "step :  300 loss :  0.08012300729751587\n",
      "step :  301 loss :  0.08011604845523834\n",
      "step :  302 loss :  0.08011264353990555\n",
      "step :  303 loss :  0.08012299239635468\n",
      "step :  304 loss :  0.08011935651302338\n",
      "step :  305 loss :  0.08011740446090698\n",
      "step :  306 loss :  0.08011554926633835\n",
      "step :  307 loss :  0.08011438697576523\n",
      "step :  308 loss :  0.08011803776025772\n",
      "step :  309 loss :  0.08011418581008911\n",
      "step :  310 loss :  0.08011534810066223\n",
      "step :  311 loss :  0.08011443167924881\n",
      "step :  312 loss :  0.08011321723461151\n",
      "step :  313 loss :  0.08011549711227417\n",
      "step :  314 loss :  0.08011315017938614\n",
      "step :  315 loss :  0.0801103264093399\n",
      "step :  316 loss :  0.08011295646429062\n",
      "step :  317 loss :  0.08010978996753693\n",
      "step :  318 loss :  0.08011162281036377\n",
      "step :  319 loss :  0.08011280000209808\n",
      "step :  320 loss :  0.08011271804571152\n",
      "step :  321 loss :  0.08011285960674286\n",
      "step :  322 loss :  0.0801137313246727\n",
      "step :  323 loss :  0.0801122710108757\n",
      "step :  324 loss :  0.08011076599359512\n",
      "step :  325 loss :  0.08011116832494736\n",
      "step :  326 loss :  0.0801093652844429\n",
      "step :  327 loss :  0.0801108181476593\n",
      "step :  328 loss :  0.080112524330616\n",
      "step :  329 loss :  0.08011128008365631\n",
      "step :  330 loss :  0.08011018484830856\n",
      "step :  331 loss :  0.080112025141716\n",
      "step :  332 loss :  0.08011028170585632\n",
      "step :  333 loss :  0.08010709285736084\n",
      "step :  334 loss :  0.08011060953140259\n",
      "step :  335 loss :  0.08011072874069214\n",
      "step :  336 loss :  0.08010844886302948\n",
      "step :  337 loss :  0.08011022210121155\n",
      "step :  338 loss :  0.08010852336883545\n",
      "step :  339 loss :  0.08010919392108917\n",
      "step :  340 loss :  0.08010747283697128\n",
      "step :  341 loss :  0.08010762929916382\n",
      "step :  342 loss :  0.08010856807231903\n",
      "step :  343 loss :  0.08010908961296082\n",
      "step :  344 loss :  0.08010775595903397\n",
      "step :  345 loss :  0.08010656386613846\n",
      "step :  346 loss :  0.08010877668857574\n",
      "step :  347 loss :  0.08010806888341904\n",
      "step :  348 loss :  0.0801079124212265\n",
      "step :  349 loss :  0.08011013269424438\n",
      "step :  350 loss :  0.08010981976985931\n",
      "step :  351 loss :  0.08010777086019516\n",
      "step :  352 loss :  0.08010762184858322\n",
      "step :  353 loss :  0.08010617643594742\n",
      "step :  354 loss :  0.08010666072368622\n",
      "step :  355 loss :  0.08010643720626831\n",
      "step :  356 loss :  0.08010610193014145\n",
      "step :  357 loss :  0.08010709285736084\n",
      "step :  358 loss :  0.08010739088058472\n",
      "step :  359 loss :  0.08010513335466385\n",
      "step :  360 loss :  0.0801050066947937\n",
      "step :  361 loss :  0.08010459691286087\n",
      "step :  362 loss :  0.08010613918304443\n",
      "step :  363 loss :  0.0801052376627922\n",
      "step :  364 loss :  0.080104760825634\n",
      "step :  365 loss :  0.08010604977607727\n",
      "step :  366 loss :  0.08010505139827728\n",
      "step :  367 loss :  0.08010482788085938\n",
      "step :  368 loss :  0.08010391145944595\n",
      "step :  369 loss :  0.08010414242744446\n",
      "step :  370 loss :  0.080103799700737\n",
      "step :  371 loss :  0.08010382950305939\n",
      "step :  372 loss :  0.080104760825634\n",
      "step :  373 loss :  0.08010420948266983\n",
      "step :  374 loss :  0.08010408282279968\n",
      "step :  375 loss :  0.08010335266590118\n",
      "step :  376 loss :  0.08010424673557281\n",
      "step :  377 loss :  0.08010480552911758\n",
      "step :  378 loss :  0.08010357618331909\n",
      "step :  379 loss :  0.08010391145944595\n",
      "step :  380 loss :  0.08010473102331161\n",
      "step :  381 loss :  0.08010375499725342\n",
      "step :  382 loss :  0.08010317385196686\n",
      "step :  383 loss :  0.08010262250900269\n",
      "step :  384 loss :  0.08010255545377731\n",
      "step :  385 loss :  0.08010309934616089\n",
      "step :  386 loss :  0.08010395616292953\n",
      "step :  387 loss :  0.08010254800319672\n",
      "step :  388 loss :  0.08010406792163849\n",
      "step :  389 loss :  0.0801047533750534\n",
      "step :  390 loss :  0.08010303974151611\n",
      "step :  391 loss :  0.080101378262043\n",
      "step :  392 loss :  0.08010270446538925\n",
      "step :  393 loss :  0.08010309934616089\n",
      "step :  394 loss :  0.0801033154129982\n",
      "step :  395 loss :  0.08010288327932358\n",
      "step :  396 loss :  0.08010247349739075\n",
      "step :  397 loss :  0.08010208606719971\n",
      "step :  398 loss :  0.08010239899158478\n",
      "step :  399 loss :  0.08010298013687134\n",
      "step :  400 loss :  0.08010252565145493\n",
      "step :  401 loss :  0.08010193705558777\n",
      "step :  402 loss :  0.08010194450616837\n",
      "step :  403 loss :  0.08010216057300568\n",
      "step :  404 loss :  0.08010202646255493\n",
      "step :  405 loss :  0.08010183274745941\n",
      "step :  406 loss :  0.08010241389274597\n",
      "step :  407 loss :  0.08010170608758926\n",
      "step :  408 loss :  0.08010203391313553\n",
      "step :  409 loss :  0.08010169863700867\n",
      "step :  410 loss :  0.08010181039571762\n",
      "step :  411 loss :  0.08010190725326538\n",
      "step :  412 loss :  0.08010154962539673\n",
      "step :  413 loss :  0.0801013708114624\n",
      "step :  414 loss :  0.08010146021842957\n",
      "step :  415 loss :  0.08010166883468628\n",
      "step :  416 loss :  0.08010149747133255\n",
      "step :  417 loss :  0.08010100573301315\n",
      "step :  418 loss :  0.08010156452655792\n",
      "step :  419 loss :  0.0801011323928833\n",
      "step :  420 loss :  0.0801011174917221\n",
      "step :  421 loss :  0.08010148257017136\n",
      "step :  422 loss :  0.08010105043649673\n",
      "step :  423 loss :  0.08010099083185196\n",
      "step :  424 loss :  0.08010119944810867\n",
      "step :  425 loss :  0.08010086417198181\n",
      "step :  426 loss :  0.08010067045688629\n",
      "step :  427 loss :  0.08010075241327286\n",
      "step :  428 loss :  0.08010076731443405\n",
      "step :  429 loss :  0.08010043948888779\n",
      "step :  430 loss :  0.08010078966617584\n",
      "step :  431 loss :  0.08010116219520569\n",
      "step :  432 loss :  0.08010076731443405\n",
      "step :  433 loss :  0.08010080456733704\n",
      "step :  434 loss :  0.08010049909353256\n",
      "step :  435 loss :  0.08010071516036987\n",
      "step :  436 loss :  0.08010037988424301\n",
      "step :  437 loss :  0.08010058104991913\n",
      "step :  438 loss :  0.0801006481051445\n",
      "step :  439 loss :  0.08010079711675644\n",
      "step :  440 loss :  0.08010049164295197\n",
      "step :  441 loss :  0.08010042458772659\n",
      "step :  442 loss :  0.0801001638174057\n",
      "step :  443 loss :  0.08010014146566391\n",
      "step :  444 loss :  0.0800999104976654\n",
      "step :  445 loss :  0.08010028302669525\n",
      "step :  446 loss :  0.08010020852088928\n",
      "step :  447 loss :  0.08010026812553406\n",
      "step :  448 loss :  0.08009987324476242\n",
      "step :  449 loss :  0.0801001712679863\n",
      "step :  450 loss :  0.08010008931159973\n",
      "step :  451 loss :  0.08009981364011765\n",
      "step :  452 loss :  0.08010001480579376\n",
      "step :  453 loss :  0.08009998500347137\n",
      "step :  454 loss :  0.0800999253988266\n",
      "step :  455 loss :  0.08009984344244003\n",
      "step :  456 loss :  0.08009995520114899\n",
      "step :  457 loss :  0.08009951561689377\n",
      "step :  458 loss :  0.0800996646285057\n",
      "step :  459 loss :  0.08009983599185944\n",
      "step :  460 loss :  0.08009973168373108\n",
      "step :  461 loss :  0.08009980618953705\n",
      "step :  462 loss :  0.08009959012269974\n",
      "step :  463 loss :  0.08009977638721466\n",
      "step :  464 loss :  0.08009961247444153\n",
      "step :  465 loss :  0.0800996944308281\n",
      "step :  466 loss :  0.08009975403547287\n",
      "step :  467 loss :  0.08009951561689377\n",
      "step :  468 loss :  0.08009964972734451\n",
      "step :  469 loss :  0.08009971678256989\n",
      "step :  470 loss :  0.08009948581457138\n",
      "step :  471 loss :  0.08009961992502213\n",
      "step :  472 loss :  0.08009961247444153\n",
      "step :  473 loss :  0.08009953796863556\n",
      "step :  474 loss :  0.08009948581457138\n",
      "step :  475 loss :  0.08009933680295944\n",
      "step :  476 loss :  0.08009941130876541\n",
      "step :  477 loss :  0.08009938895702362\n",
      "step :  478 loss :  0.08009931445121765\n",
      "step :  479 loss :  0.08009951561689377\n",
      "step :  480 loss :  0.08009950816631317\n",
      "step :  481 loss :  0.08009929955005646\n",
      "step :  482 loss :  0.08009951561689377\n",
      "step :  483 loss :  0.08009957522153854\n",
      "step :  484 loss :  0.08009927719831467\n",
      "step :  485 loss :  0.08009924739599228\n",
      "step :  486 loss :  0.08009929955005646\n",
      "step :  487 loss :  0.08009935915470123\n",
      "step :  488 loss :  0.08009929209947586\n",
      "step :  489 loss :  0.08009912818670273\n",
      "step :  490 loss :  0.0800991877913475\n",
      "step :  491 loss :  0.0800991877913475\n",
      "step :  492 loss :  0.08009931445121765\n",
      "step :  493 loss :  0.0800992026925087\n",
      "step :  494 loss :  0.08009924739599228\n",
      "step :  495 loss :  0.08009926229715347\n",
      "step :  496 loss :  0.08009914308786392\n",
      "step :  497 loss :  0.0800991877913475\n",
      "step :  498 loss :  0.08009928464889526\n",
      "step :  499 loss :  0.08009923994541168\n",
      "Training model 7\n",
      "step :  0 loss :  0.32592639327049255\n",
      "step :  1 loss :  0.24466177821159363\n",
      "step :  2 loss :  0.20778919756412506\n",
      "step :  3 loss :  0.23436053097248077\n",
      "step :  4 loss :  0.19529806077480316\n",
      "step :  5 loss :  0.22538816928863525\n",
      "step :  6 loss :  0.1957336813211441\n",
      "step :  7 loss :  0.2207290381193161\n",
      "step :  8 loss :  0.18775171041488647\n",
      "step :  9 loss :  0.21154455840587616\n",
      "step :  10 loss :  0.18157784640789032\n",
      "step :  11 loss :  0.1733413189649582\n",
      "step :  12 loss :  0.1618233323097229\n",
      "step :  13 loss :  0.15935973823070526\n",
      "step :  14 loss :  0.14146004617214203\n",
      "step :  15 loss :  0.15035805106163025\n",
      "step :  16 loss :  0.14988058805465698\n",
      "step :  17 loss :  0.22101761400699615\n",
      "step :  18 loss :  0.13843925297260284\n",
      "step :  19 loss :  0.1444815844297409\n",
      "step :  20 loss :  0.14082729816436768\n",
      "step :  21 loss :  0.1172400489449501\n",
      "step :  22 loss :  0.12605442106723785\n",
      "step :  23 loss :  0.11438856273889542\n",
      "step :  24 loss :  0.13202492892742157\n",
      "step :  25 loss :  0.10768242180347443\n",
      "step :  26 loss :  0.10593116283416748\n",
      "step :  27 loss :  0.11042114347219467\n",
      "step :  28 loss :  0.10553377121686935\n",
      "step :  29 loss :  0.11714699119329453\n",
      "step :  30 loss :  0.108282670378685\n",
      "step :  31 loss :  0.12894856929779053\n",
      "step :  32 loss :  0.13362553715705872\n",
      "step :  33 loss :  0.1175825297832489\n",
      "step :  34 loss :  0.11523497104644775\n",
      "step :  35 loss :  0.11456253379583359\n",
      "step :  36 loss :  0.101160429418087\n",
      "step :  37 loss :  0.09623944759368896\n",
      "step :  38 loss :  0.10674147307872772\n",
      "step :  39 loss :  0.10142579674720764\n",
      "step :  40 loss :  0.10295355319976807\n",
      "step :  41 loss :  0.10758256167173386\n",
      "step :  42 loss :  0.09360326826572418\n",
      "step :  43 loss :  0.09201302379369736\n",
      "step :  44 loss :  0.08961322903633118\n",
      "step :  45 loss :  0.08947984874248505\n",
      "step :  46 loss :  0.08726146072149277\n",
      "step :  47 loss :  0.09550967812538147\n",
      "step :  48 loss :  0.10221376270055771\n",
      "step :  49 loss :  0.08789088577032089\n",
      "step :  50 loss :  0.09169501066207886\n",
      "step :  51 loss :  0.08816719800233841\n",
      "step :  52 loss :  0.10010017454624176\n",
      "step :  53 loss :  0.08767440170049667\n",
      "step :  54 loss :  0.09505422413349152\n",
      "step :  55 loss :  0.09288043528795242\n",
      "step :  56 loss :  0.092397041618824\n",
      "step :  57 loss :  0.08527825772762299\n",
      "step :  58 loss :  0.08369318395853043\n",
      "step :  59 loss :  0.0841788500547409\n",
      "step :  60 loss :  0.08187058568000793\n",
      "step :  61 loss :  0.10323518514633179\n",
      "step :  62 loss :  0.08533569425344467\n",
      "step :  63 loss :  0.0888010635972023\n",
      "step :  64 loss :  0.08366493135690689\n",
      "step :  65 loss :  0.085591159760952\n",
      "step :  66 loss :  0.08202788978815079\n",
      "step :  67 loss :  0.08206424862146378\n",
      "step :  68 loss :  0.08553765714168549\n",
      "step :  69 loss :  0.09253419935703278\n",
      "step :  70 loss :  0.09102559834718704\n",
      "step :  71 loss :  0.08413460850715637\n",
      "step :  72 loss :  0.08574122190475464\n",
      "step :  73 loss :  0.08650678396224976\n",
      "step :  74 loss :  0.09070199728012085\n",
      "step :  75 loss :  0.08468804508447647\n",
      "step :  76 loss :  0.0828997790813446\n",
      "step :  77 loss :  0.08539804071187973\n",
      "step :  78 loss :  0.08938256651163101\n",
      "step :  79 loss :  0.0840618833899498\n",
      "step :  80 loss :  0.08403556793928146\n",
      "step :  81 loss :  0.08286243677139282\n",
      "step :  82 loss :  0.08460059762001038\n",
      "step :  83 loss :  0.08788740634918213\n",
      "step :  84 loss :  0.08319038152694702\n",
      "step :  85 loss :  0.08134915679693222\n",
      "step :  86 loss :  0.08084151148796082\n",
      "step :  87 loss :  0.0817163810133934\n",
      "step :  88 loss :  0.08014734089374542\n",
      "step :  89 loss :  0.08141393959522247\n",
      "step :  90 loss :  0.08094223588705063\n",
      "step :  91 loss :  0.08334292471408844\n",
      "step :  92 loss :  0.08046866208314896\n",
      "step :  93 loss :  0.08053665608167648\n",
      "step :  94 loss :  0.08059387654066086\n",
      "step :  95 loss :  0.08112921565771103\n",
      "step :  96 loss :  0.08091728389263153\n",
      "step :  97 loss :  0.08154993504285812\n",
      "step :  98 loss :  0.0814429298043251\n",
      "step :  99 loss :  0.08003126084804535\n",
      "step :  100 loss :  0.08013395965099335\n",
      "step :  101 loss :  0.08064881712198257\n",
      "step :  102 loss :  0.08000282198190689\n",
      "step :  103 loss :  0.08027723431587219\n",
      "step :  104 loss :  0.08007758110761642\n",
      "step :  105 loss :  0.07992953807115555\n",
      "step :  106 loss :  0.08039206266403198\n",
      "step :  107 loss :  0.08002285659313202\n",
      "step :  108 loss :  0.08074763417243958\n",
      "step :  109 loss :  0.08094289153814316\n",
      "step :  110 loss :  0.0800463929772377\n",
      "step :  111 loss :  0.079627625644207\n",
      "step :  112 loss :  0.0799974650144577\n",
      "step :  113 loss :  0.07965538650751114\n",
      "step :  114 loss :  0.07992774993181229\n",
      "step :  115 loss :  0.07952812314033508\n",
      "step :  116 loss :  0.07942550629377365\n",
      "step :  117 loss :  0.08008134365081787\n",
      "step :  118 loss :  0.07952466607093811\n",
      "step :  119 loss :  0.07979419827461243\n",
      "step :  120 loss :  0.08005355298519135\n",
      "step :  121 loss :  0.08041317015886307\n",
      "step :  122 loss :  0.07972848415374756\n",
      "step :  123 loss :  0.08040376752614975\n",
      "step :  124 loss :  0.08001448959112167\n",
      "step :  125 loss :  0.07998707890510559\n",
      "step :  126 loss :  0.079605832695961\n",
      "step :  127 loss :  0.07913824170827866\n",
      "step :  128 loss :  0.07923484593629837\n",
      "step :  129 loss :  0.07962307333946228\n",
      "step :  130 loss :  0.07921390980482101\n",
      "step :  131 loss :  0.07907698303461075\n",
      "step :  132 loss :  0.07911552488803864\n",
      "step :  133 loss :  0.07940635830163956\n",
      "step :  134 loss :  0.0791972279548645\n",
      "step :  135 loss :  0.07906167954206467\n",
      "step :  136 loss :  0.07897130399942398\n",
      "step :  137 loss :  0.07901817560195923\n",
      "step :  138 loss :  0.07959168404340744\n",
      "step :  139 loss :  0.0790008082985878\n",
      "step :  140 loss :  0.0797811970114708\n",
      "step :  141 loss :  0.07928832620382309\n",
      "step :  142 loss :  0.07908453792333603\n",
      "step :  143 loss :  0.07912424206733704\n",
      "step :  144 loss :  0.07904557883739471\n",
      "step :  145 loss :  0.07899217307567596\n",
      "step :  146 loss :  0.07893525063991547\n",
      "step :  147 loss :  0.07892105728387833\n",
      "step :  148 loss :  0.0796239972114563\n",
      "step :  149 loss :  0.07928378134965897\n",
      "step :  150 loss :  0.07899723947048187\n",
      "step :  151 loss :  0.07896605879068375\n",
      "step :  152 loss :  0.07905203104019165\n",
      "step :  153 loss :  0.07920467853546143\n",
      "step :  154 loss :  0.07921897619962692\n",
      "step :  155 loss :  0.07892154902219772\n",
      "step :  156 loss :  0.07903947681188583\n",
      "step :  157 loss :  0.07890822738409042\n",
      "step :  158 loss :  0.07901746779680252\n",
      "step :  159 loss :  0.07905290275812149\n",
      "step :  160 loss :  0.07895728945732117\n",
      "step :  161 loss :  0.07907221466302872\n",
      "step :  162 loss :  0.07891074568033218\n",
      "step :  163 loss :  0.07894989848136902\n",
      "step :  164 loss :  0.07882330566644669\n",
      "step :  165 loss :  0.07887724041938782\n",
      "step :  166 loss :  0.07903018593788147\n",
      "step :  167 loss :  0.07888713479042053\n",
      "step :  168 loss :  0.0788743644952774\n",
      "step :  169 loss :  0.07880853861570358\n",
      "step :  170 loss :  0.0788610428571701\n",
      "step :  171 loss :  0.07880695164203644\n",
      "step :  172 loss :  0.07888756692409515\n",
      "step :  173 loss :  0.07887206226587296\n",
      "step :  174 loss :  0.07896489650011063\n",
      "step :  175 loss :  0.07892589271068573\n",
      "step :  176 loss :  0.07890203595161438\n",
      "step :  177 loss :  0.07878024876117706\n",
      "step :  178 loss :  0.07911832630634308\n",
      "step :  179 loss :  0.07878118753433228\n",
      "step :  180 loss :  0.07891421020030975\n",
      "step :  181 loss :  0.07889819890260696\n",
      "step :  182 loss :  0.07885027676820755\n",
      "step :  183 loss :  0.07877808809280396\n",
      "step :  184 loss :  0.07883043587207794\n",
      "step :  185 loss :  0.07877317816019058\n",
      "step :  186 loss :  0.07878188788890839\n",
      "step :  187 loss :  0.07878049463033676\n",
      "step :  188 loss :  0.07895492017269135\n",
      "step :  189 loss :  0.0787787139415741\n",
      "step :  190 loss :  0.07903633266687393\n",
      "step :  191 loss :  0.07873356342315674\n",
      "step :  192 loss :  0.07888980954885483\n",
      "step :  193 loss :  0.078793466091156\n",
      "step :  194 loss :  0.07877600193023682\n",
      "step :  195 loss :  0.07881344109773636\n",
      "step :  196 loss :  0.07874849438667297\n",
      "step :  197 loss :  0.07892380654811859\n",
      "step :  198 loss :  0.07871689647436142\n",
      "step :  199 loss :  0.0788525864481926\n",
      "step :  200 loss :  0.07873675227165222\n",
      "step :  201 loss :  0.07884258031845093\n",
      "step :  202 loss :  0.07872850447893143\n",
      "step :  203 loss :  0.07881669700145721\n",
      "step :  204 loss :  0.07879836112260818\n",
      "step :  205 loss :  0.07877056300640106\n",
      "step :  206 loss :  0.07886186987161636\n",
      "step :  207 loss :  0.07872654497623444\n",
      "step :  208 loss :  0.07887066900730133\n",
      "step :  209 loss :  0.07877012342214584\n",
      "step :  210 loss :  0.07875676453113556\n",
      "step :  211 loss :  0.07875500619411469\n",
      "step :  212 loss :  0.07874932885169983\n",
      "step :  213 loss :  0.07873939722776413\n",
      "step :  214 loss :  0.07878710329532623\n",
      "step :  215 loss :  0.07873275130987167\n",
      "step :  216 loss :  0.0788460522890091\n",
      "step :  217 loss :  0.07870396226644516\n",
      "step :  218 loss :  0.07881436496973038\n",
      "step :  219 loss :  0.07875130325555801\n",
      "step :  220 loss :  0.07872938364744186\n",
      "step :  221 loss :  0.07876650243997574\n",
      "step :  222 loss :  0.07873436063528061\n",
      "step :  223 loss :  0.07870849221944809\n",
      "step :  224 loss :  0.0787934884428978\n",
      "step :  225 loss :  0.07870050519704819\n",
      "step :  226 loss :  0.07878176122903824\n",
      "step :  227 loss :  0.07874888926744461\n",
      "step :  228 loss :  0.0787334069609642\n",
      "step :  229 loss :  0.07875414192676544\n",
      "step :  230 loss :  0.07871797680854797\n",
      "step :  231 loss :  0.07872641086578369\n",
      "step :  232 loss :  0.07873295992612839\n",
      "step :  233 loss :  0.07872261852025986\n",
      "step :  234 loss :  0.07875102013349533\n",
      "step :  235 loss :  0.07870785146951675\n",
      "step :  236 loss :  0.07872255146503448\n",
      "step :  237 loss :  0.07871272414922714\n",
      "step :  238 loss :  0.078712597489357\n",
      "step :  239 loss :  0.0787339136004448\n",
      "step :  240 loss :  0.07871077954769135\n",
      "step :  241 loss :  0.07874038070440292\n",
      "step :  242 loss :  0.07870945334434509\n",
      "step :  243 loss :  0.07871638238430023\n",
      "step :  244 loss :  0.0787128359079361\n",
      "step :  245 loss :  0.07869230210781097\n",
      "step :  246 loss :  0.07872793078422546\n",
      "step :  247 loss :  0.07872722297906876\n",
      "step :  248 loss :  0.07872100174427032\n",
      "step :  249 loss :  0.07871993631124496\n",
      "step :  250 loss :  0.07871835678815842\n",
      "step :  251 loss :  0.07870782911777496\n",
      "step :  252 loss :  0.07870224118232727\n",
      "step :  253 loss :  0.07870262116193771\n",
      "step :  254 loss :  0.07869430631399155\n",
      "step :  255 loss :  0.0787222683429718\n",
      "step :  256 loss :  0.07869902998209\n",
      "step :  257 loss :  0.07869476079940796\n",
      "step :  258 loss :  0.0787121057510376\n",
      "step :  259 loss :  0.07869470864534378\n",
      "step :  260 loss :  0.07872474193572998\n",
      "step :  261 loss :  0.07869235426187515\n",
      "step :  262 loss :  0.07869050651788712\n",
      "step :  263 loss :  0.07872877269983292\n",
      "step :  264 loss :  0.07869856804609299\n",
      "step :  265 loss :  0.07868669182062149\n",
      "step :  266 loss :  0.07870909571647644\n",
      "step :  267 loss :  0.07870159298181534\n",
      "step :  268 loss :  0.07870333641767502\n",
      "step :  269 loss :  0.07871384918689728\n",
      "step :  270 loss :  0.07869594544172287\n",
      "step :  271 loss :  0.07869455218315125\n",
      "step :  272 loss :  0.07871715724468231\n",
      "step :  273 loss :  0.0786893218755722\n",
      "step :  274 loss :  0.07870235294103622\n",
      "step :  275 loss :  0.07869952917098999\n",
      "step :  276 loss :  0.07869505137205124\n",
      "step :  277 loss :  0.07869052141904831\n",
      "step :  278 loss :  0.0786888375878334\n",
      "step :  279 loss :  0.07870324701070786\n",
      "step :  280 loss :  0.07868491858243942\n",
      "step :  281 loss :  0.07869187742471695\n",
      "step :  282 loss :  0.0786902979016304\n",
      "step :  283 loss :  0.07868946343660355\n",
      "step :  284 loss :  0.07868994772434235\n",
      "step :  285 loss :  0.07868945598602295\n",
      "step :  286 loss :  0.07869181782007217\n",
      "step :  287 loss :  0.07868751883506775\n",
      "step :  288 loss :  0.078696608543396\n",
      "step :  289 loss :  0.07868565618991852\n",
      "step :  290 loss :  0.07869605720043182\n",
      "step :  291 loss :  0.07868945598602295\n",
      "step :  292 loss :  0.07868462055921555\n",
      "step :  293 loss :  0.07868879288434982\n",
      "step :  294 loss :  0.07868760079145432\n",
      "step :  295 loss :  0.07869048416614532\n",
      "step :  296 loss :  0.07868517190217972\n",
      "step :  297 loss :  0.078696608543396\n",
      "step :  298 loss :  0.07868433743715286\n",
      "step :  299 loss :  0.07869955897331238\n",
      "step :  300 loss :  0.07868987321853638\n",
      "step :  301 loss :  0.07868536561727524\n",
      "step :  302 loss :  0.0786881148815155\n",
      "step :  303 loss :  0.07868418842554092\n",
      "step :  304 loss :  0.0786895826458931\n",
      "step :  305 loss :  0.07868517190217972\n",
      "step :  306 loss :  0.07868969440460205\n",
      "step :  307 loss :  0.07868807762861252\n",
      "step :  308 loss :  0.07869477570056915\n",
      "step :  309 loss :  0.07868914306163788\n",
      "step :  310 loss :  0.07868792861700058\n",
      "step :  311 loss :  0.078692726790905\n",
      "step :  312 loss :  0.07868839800357819\n",
      "step :  313 loss :  0.07868895679712296\n",
      "step :  314 loss :  0.07868815213441849\n",
      "step :  315 loss :  0.07868719100952148\n",
      "step :  316 loss :  0.07869493961334229\n",
      "step :  317 loss :  0.07868359237909317\n",
      "step :  318 loss :  0.07869384437799454\n",
      "step :  319 loss :  0.07868843525648117\n",
      "step :  320 loss :  0.07869364321231842\n",
      "step :  321 loss :  0.07869207859039307\n",
      "step :  322 loss :  0.07869097590446472\n",
      "step :  323 loss :  0.07868903875350952\n",
      "step :  324 loss :  0.07868614047765732\n",
      "step :  325 loss :  0.07869497686624527\n",
      "step :  326 loss :  0.07868754863739014\n",
      "step :  327 loss :  0.07868960499763489\n",
      "step :  328 loss :  0.07868748903274536\n",
      "step :  329 loss :  0.07868760824203491\n",
      "step :  330 loss :  0.07868672162294388\n",
      "step :  331 loss :  0.07868677377700806\n",
      "step :  332 loss :  0.07868585735559464\n",
      "step :  333 loss :  0.07868517935276031\n",
      "step :  334 loss :  0.07868710905313492\n",
      "step :  335 loss :  0.07868488878011703\n",
      "step :  336 loss :  0.07869290560483932\n",
      "step :  337 loss :  0.07868744432926178\n",
      "step :  338 loss :  0.07868601381778717\n",
      "step :  339 loss :  0.0786878913640976\n",
      "step :  340 loss :  0.07868672162294388\n",
      "step :  341 loss :  0.0786854550242424\n",
      "step :  342 loss :  0.0786871686577797\n",
      "step :  343 loss :  0.07868532836437225\n",
      "step :  344 loss :  0.07869001477956772\n",
      "step :  345 loss :  0.07868465036153793\n",
      "step :  346 loss :  0.07868748158216476\n",
      "step :  347 loss :  0.07868584245443344\n",
      "step :  348 loss :  0.07868612557649612\n",
      "step :  349 loss :  0.0786861926317215\n",
      "step :  350 loss :  0.07868436723947525\n",
      "step :  351 loss :  0.07868890464305878\n",
      "step :  352 loss :  0.07868590950965881\n",
      "step :  353 loss :  0.07868459075689316\n",
      "step :  354 loss :  0.07868701964616776\n",
      "step :  355 loss :  0.07868613302707672\n",
      "step :  356 loss :  0.07868605852127075\n",
      "step :  357 loss :  0.0786869078874588\n",
      "step :  358 loss :  0.07868541777133942\n",
      "step :  359 loss :  0.07868661731481552\n",
      "step :  360 loss :  0.07868440449237823\n",
      "step :  361 loss :  0.0786871537566185\n",
      "step :  362 loss :  0.07868378609418869\n",
      "step :  363 loss :  0.07868336886167526\n",
      "step :  364 loss :  0.07868579030036926\n",
      "step :  365 loss :  0.07868314534425735\n",
      "step :  366 loss :  0.07868736237287521\n",
      "step :  367 loss :  0.07868557423353195\n",
      "step :  368 loss :  0.07868319749832153\n",
      "step :  369 loss :  0.0786871612071991\n",
      "step :  370 loss :  0.07868649810552597\n",
      "step :  371 loss :  0.0786830335855484\n",
      "step :  372 loss :  0.07868712395429611\n",
      "step :  373 loss :  0.07868575304746628\n",
      "step :  374 loss :  0.07868382334709167\n",
      "step :  375 loss :  0.0786837786436081\n",
      "step :  376 loss :  0.07868355512619019\n",
      "step :  377 loss :  0.07868514209985733\n",
      "step :  378 loss :  0.0786840096116066\n",
      "step :  379 loss :  0.07868397235870361\n",
      "step :  380 loss :  0.07868605852127075\n",
      "step :  381 loss :  0.078684002161026\n",
      "step :  382 loss :  0.07868561148643494\n",
      "step :  383 loss :  0.07868432998657227\n",
      "step :  384 loss :  0.07868583500385284\n",
      "step :  385 loss :  0.07868382334709167\n",
      "step :  386 loss :  0.078684002161026\n",
      "step :  387 loss :  0.07868659496307373\n",
      "step :  388 loss :  0.07868553698062897\n",
      "step :  389 loss :  0.07868437469005585\n",
      "step :  390 loss :  0.07868642359972\n",
      "step :  391 loss :  0.0786847248673439\n",
      "step :  392 loss :  0.0786844789981842\n",
      "step :  393 loss :  0.07868484407663345\n",
      "step :  394 loss :  0.07868436723947525\n",
      "step :  395 loss :  0.07868440449237823\n",
      "step :  396 loss :  0.07868384569883347\n",
      "step :  397 loss :  0.07868500053882599\n",
      "step :  398 loss :  0.07868419587612152\n",
      "step :  399 loss :  0.07868348062038422\n",
      "step :  400 loss :  0.07868404686450958\n",
      "step :  401 loss :  0.07868411391973495\n",
      "step :  402 loss :  0.07868301123380661\n",
      "step :  403 loss :  0.07868330925703049\n",
      "step :  404 loss :  0.07868396490812302\n",
      "step :  405 loss :  0.07868296653032303\n",
      "step :  406 loss :  0.07868410646915436\n",
      "step :  407 loss :  0.07868382334709167\n",
      "step :  408 loss :  0.07868374139070511\n",
      "step :  409 loss :  0.07868389785289764\n",
      "step :  410 loss :  0.07868440449237823\n",
      "step :  411 loss :  0.0786835327744484\n",
      "step :  412 loss :  0.07868465781211853\n",
      "step :  413 loss :  0.07868391275405884\n",
      "step :  414 loss :  0.07868446409702301\n",
      "step :  415 loss :  0.07868414372205734\n",
      "step :  416 loss :  0.07868395000696182\n",
      "step :  417 loss :  0.07868387550115585\n",
      "step :  418 loss :  0.07868494838476181\n",
      "step :  419 loss :  0.07868385314941406\n",
      "step :  420 loss :  0.07868432998657227\n",
      "step :  421 loss :  0.07868456095457077\n",
      "step :  422 loss :  0.07868391275405884\n",
      "step :  423 loss :  0.07868409901857376\n",
      "step :  424 loss :  0.07868437469005585\n",
      "step :  425 loss :  0.07868349552154541\n",
      "step :  426 loss :  0.07868395000696182\n",
      "step :  427 loss :  0.07868374139070511\n",
      "step :  428 loss :  0.07868345826864243\n",
      "step :  429 loss :  0.07868428528308868\n",
      "step :  430 loss :  0.0786830484867096\n",
      "step :  431 loss :  0.07868338376283646\n",
      "step :  432 loss :  0.07868392765522003\n",
      "step :  433 loss :  0.07868316769599915\n",
      "step :  434 loss :  0.07868294417858124\n",
      "step :  435 loss :  0.0786839947104454\n",
      "step :  436 loss :  0.07868364453315735\n",
      "step :  437 loss :  0.07868314534425735\n",
      "step :  438 loss :  0.07868371158838272\n",
      "step :  439 loss :  0.07868323475122452\n",
      "step :  440 loss :  0.07868359982967377\n",
      "step :  441 loss :  0.07868309319019318\n",
      "step :  442 loss :  0.07868354767560959\n",
      "step :  443 loss :  0.07868321985006332\n",
      "step :  444 loss :  0.07868379354476929\n",
      "step :  445 loss :  0.07868338376283646\n",
      "step :  446 loss :  0.07868370413780212\n",
      "step :  447 loss :  0.07868345081806183\n",
      "step :  448 loss :  0.07868355512619019\n",
      "step :  449 loss :  0.07868357002735138\n",
      "step :  450 loss :  0.07868337631225586\n",
      "step :  451 loss :  0.07868368178606033\n",
      "step :  452 loss :  0.07868333160877228\n",
      "step :  453 loss :  0.07868359982967377\n",
      "step :  454 loss :  0.0786832720041275\n",
      "step :  455 loss :  0.07868364453315735\n",
      "step :  456 loss :  0.07868342101573944\n",
      "step :  457 loss :  0.07868355512619019\n",
      "step :  458 loss :  0.07868324220180511\n",
      "step :  459 loss :  0.07868334650993347\n",
      "step :  460 loss :  0.07868334650993347\n",
      "step :  461 loss :  0.07868301868438721\n",
      "step :  462 loss :  0.07868299633264542\n",
      "step :  463 loss :  0.07868334650993347\n",
      "step :  464 loss :  0.07868322730064392\n",
      "step :  465 loss :  0.07868293672800064\n",
      "step :  466 loss :  0.0786830484867096\n",
      "step :  467 loss :  0.07868330925703049\n",
      "step :  468 loss :  0.07868301123380661\n",
      "step :  469 loss :  0.0786828026175499\n",
      "step :  470 loss :  0.07868333905935287\n",
      "step :  471 loss :  0.0786832794547081\n",
      "step :  472 loss :  0.0786827951669693\n",
      "step :  473 loss :  0.07868307083845139\n",
      "step :  474 loss :  0.07868333160877228\n",
      "step :  475 loss :  0.07868301868438721\n",
      "step :  476 loss :  0.07868295907974243\n",
      "step :  477 loss :  0.07868331670761108\n",
      "step :  478 loss :  0.0786832645535469\n",
      "step :  479 loss :  0.07868310809135437\n",
      "step :  480 loss :  0.07868293672800064\n",
      "step :  481 loss :  0.07868334650993347\n",
      "step :  482 loss :  0.07868330925703049\n",
      "step :  483 loss :  0.07868297398090363\n",
      "step :  484 loss :  0.07868308573961258\n",
      "step :  485 loss :  0.07868334650993347\n",
      "step :  486 loss :  0.0786830484867096\n",
      "step :  487 loss :  0.07868310809135437\n",
      "step :  488 loss :  0.0786835178732872\n",
      "step :  489 loss :  0.07868316769599915\n",
      "step :  490 loss :  0.0786830484867096\n",
      "step :  491 loss :  0.0786832943558693\n",
      "step :  492 loss :  0.07868333905935287\n",
      "step :  493 loss :  0.07868305593729019\n",
      "step :  494 loss :  0.07868307083845139\n",
      "step :  495 loss :  0.07868348807096481\n",
      "step :  496 loss :  0.07868321985006332\n",
      "step :  497 loss :  0.07868310809135437\n",
      "step :  498 loss :  0.07868313044309616\n",
      "step :  499 loss :  0.07868325710296631\n",
      "Training model 8\n",
      "step :  0 loss :  0.36038368940353394\n",
      "step :  1 loss :  0.3106691241264343\n",
      "step :  2 loss :  0.20193031430244446\n",
      "step :  3 loss :  0.22223086655139923\n",
      "step :  4 loss :  0.18597739934921265\n",
      "step :  5 loss :  0.20008599758148193\n",
      "step :  6 loss :  0.2336016446352005\n",
      "step :  7 loss :  0.23419851064682007\n",
      "step :  8 loss :  0.19344398379325867\n",
      "step :  9 loss :  0.1876523494720459\n",
      "step :  10 loss :  0.2038232833147049\n",
      "step :  11 loss :  0.17217449843883514\n",
      "step :  12 loss :  0.16791610419750214\n",
      "step :  13 loss :  0.16506123542785645\n",
      "step :  14 loss :  0.16392789781093597\n",
      "step :  15 loss :  0.14971736073493958\n",
      "step :  16 loss :  0.14357785880565643\n",
      "step :  17 loss :  0.1473175287246704\n",
      "step :  18 loss :  0.13586251437664032\n",
      "step :  19 loss :  0.13374626636505127\n",
      "step :  20 loss :  0.13420115411281586\n",
      "step :  21 loss :  0.30578866600990295\n",
      "step :  22 loss :  0.12062564492225647\n",
      "step :  23 loss :  0.11369482427835464\n",
      "step :  24 loss :  0.15237201750278473\n",
      "step :  25 loss :  0.12672051787376404\n",
      "step :  26 loss :  0.12758728861808777\n",
      "step :  27 loss :  0.11253299564123154\n",
      "step :  28 loss :  0.11878889054059982\n",
      "step :  29 loss :  0.10179299116134644\n",
      "step :  30 loss :  0.10872936993837357\n",
      "step :  31 loss :  0.13475383818149567\n",
      "step :  32 loss :  0.11732828617095947\n",
      "step :  33 loss :  0.1053009107708931\n",
      "step :  34 loss :  0.10558225959539413\n",
      "step :  35 loss :  0.10026905685663223\n",
      "step :  36 loss :  0.09373992681503296\n",
      "step :  37 loss :  0.104863740503788\n",
      "step :  38 loss :  0.11139819025993347\n",
      "step :  39 loss :  0.09644544124603271\n",
      "step :  40 loss :  0.09168116003274918\n",
      "step :  41 loss :  0.10904091596603394\n",
      "step :  42 loss :  0.09096979349851608\n",
      "step :  43 loss :  0.09399235248565674\n",
      "step :  44 loss :  0.08961684256792068\n",
      "step :  45 loss :  0.08944776654243469\n",
      "step :  46 loss :  0.11324377357959747\n",
      "step :  47 loss :  0.0887417122721672\n",
      "step :  48 loss :  0.0866711214184761\n",
      "step :  49 loss :  0.09121512621641159\n",
      "step :  50 loss :  0.09135252982378006\n",
      "step :  51 loss :  0.08685518056154251\n",
      "step :  52 loss :  0.10056565701961517\n",
      "step :  53 loss :  0.0909191146492958\n",
      "step :  54 loss :  0.08608917146921158\n",
      "step :  55 loss :  0.09425941109657288\n",
      "step :  56 loss :  0.09780541807413101\n",
      "step :  57 loss :  0.08577240258455276\n",
      "step :  58 loss :  0.08599382638931274\n",
      "step :  59 loss :  0.08559054136276245\n",
      "step :  60 loss :  0.088102787733078\n",
      "step :  61 loss :  0.09478135406970978\n",
      "step :  62 loss :  0.09593702107667923\n",
      "step :  63 loss :  0.08446110785007477\n",
      "step :  64 loss :  0.08448412269353867\n",
      "step :  65 loss :  0.08343147486448288\n",
      "step :  66 loss :  0.08440357446670532\n",
      "step :  67 loss :  0.08282913267612457\n",
      "step :  68 loss :  0.08364704251289368\n",
      "step :  69 loss :  0.09274686872959137\n",
      "step :  70 loss :  0.08316311985254288\n",
      "step :  71 loss :  0.08599746972322464\n",
      "step :  72 loss :  0.08296433836221695\n",
      "step :  73 loss :  0.09034625440835953\n",
      "step :  74 loss :  0.08341146260499954\n",
      "step :  75 loss :  0.08329945057630539\n",
      "step :  76 loss :  0.08712662011384964\n",
      "step :  77 loss :  0.08432219177484512\n",
      "step :  78 loss :  0.08348961174488068\n",
      "step :  79 loss :  0.08685147762298584\n",
      "step :  80 loss :  0.08494268357753754\n",
      "step :  81 loss :  0.08413147181272507\n",
      "step :  82 loss :  0.0865631103515625\n",
      "step :  83 loss :  0.08517637103796005\n",
      "step :  84 loss :  0.08422238379716873\n",
      "step :  85 loss :  0.08470684289932251\n",
      "step :  86 loss :  0.08577965945005417\n",
      "step :  87 loss :  0.08559767156839371\n",
      "step :  88 loss :  0.08334518224000931\n",
      "step :  89 loss :  0.08189985156059265\n",
      "step :  90 loss :  0.08281134814023972\n",
      "step :  91 loss :  0.08193448930978775\n",
      "step :  92 loss :  0.08185802400112152\n",
      "step :  93 loss :  0.08714338392019272\n",
      "step :  94 loss :  0.08277370780706406\n",
      "step :  95 loss :  0.08271171897649765\n",
      "step :  96 loss :  0.08374271541833878\n",
      "step :  97 loss :  0.08412104845046997\n",
      "step :  98 loss :  0.08646099269390106\n",
      "step :  99 loss :  0.08195972442626953\n",
      "step :  100 loss :  0.08225703984498978\n",
      "step :  101 loss :  0.08216482400894165\n",
      "step :  102 loss :  0.08198872953653336\n",
      "step :  103 loss :  0.08168557286262512\n",
      "step :  104 loss :  0.08369079232215881\n",
      "step :  105 loss :  0.08163220435380936\n",
      "step :  106 loss :  0.08163871616125107\n",
      "step :  107 loss :  0.08197952806949615\n",
      "step :  108 loss :  0.08156241476535797\n",
      "step :  109 loss :  0.08171147108078003\n",
      "step :  110 loss :  0.08217020332813263\n",
      "step :  111 loss :  0.08169980347156525\n",
      "step :  112 loss :  0.0822538286447525\n",
      "step :  113 loss :  0.08219898492097855\n",
      "step :  114 loss :  0.08444131910800934\n",
      "step :  115 loss :  0.08159463852643967\n",
      "step :  116 loss :  0.082326740026474\n",
      "step :  117 loss :  0.08189726620912552\n",
      "step :  118 loss :  0.08150961995124817\n",
      "step :  119 loss :  0.08215593546628952\n",
      "step :  120 loss :  0.08229805529117584\n",
      "step :  121 loss :  0.08144107460975647\n",
      "step :  122 loss :  0.08196685463190079\n",
      "step :  123 loss :  0.08173616975545883\n",
      "step :  124 loss :  0.08129896223545074\n",
      "step :  125 loss :  0.08242768794298172\n",
      "step :  126 loss :  0.08136742562055588\n",
      "step :  127 loss :  0.08125251531600952\n",
      "step :  128 loss :  0.08175717294216156\n",
      "step :  129 loss :  0.08186160027980804\n",
      "step :  130 loss :  0.08123195171356201\n",
      "step :  131 loss :  0.08189500868320465\n",
      "step :  132 loss :  0.08119004219770432\n",
      "step :  133 loss :  0.08176136016845703\n",
      "step :  134 loss :  0.08116582036018372\n",
      "step :  135 loss :  0.08142977207899094\n",
      "step :  136 loss :  0.08138898760080338\n",
      "step :  137 loss :  0.08123448491096497\n",
      "step :  138 loss :  0.08168663829565048\n",
      "step :  139 loss :  0.08130171149969101\n",
      "step :  140 loss :  0.08140458166599274\n",
      "step :  141 loss :  0.08109661936759949\n",
      "step :  142 loss :  0.08169812709093094\n",
      "step :  143 loss :  0.08118239790201187\n",
      "step :  144 loss :  0.08149111270904541\n",
      "step :  145 loss :  0.08121271431446075\n",
      "step :  146 loss :  0.08114162087440491\n",
      "step :  147 loss :  0.08105746656656265\n",
      "step :  148 loss :  0.08125656098127365\n",
      "step :  149 loss :  0.08136741816997528\n",
      "step :  150 loss :  0.08125241845846176\n",
      "step :  151 loss :  0.08127767592668533\n",
      "step :  152 loss :  0.08109581470489502\n",
      "step :  153 loss :  0.08127886056900024\n",
      "step :  154 loss :  0.08128158748149872\n",
      "step :  155 loss :  0.08109749108552933\n",
      "step :  156 loss :  0.08133845031261444\n",
      "step :  157 loss :  0.08103737235069275\n",
      "step :  158 loss :  0.08130479604005814\n",
      "step :  159 loss :  0.08112385123968124\n",
      "step :  160 loss :  0.08106876909732819\n",
      "step :  161 loss :  0.08100014179944992\n",
      "step :  162 loss :  0.0810193195939064\n",
      "step :  163 loss :  0.0810534656047821\n",
      "step :  164 loss :  0.08095742017030716\n",
      "step :  165 loss :  0.08113342523574829\n",
      "step :  166 loss :  0.08096744865179062\n",
      "step :  167 loss :  0.08106082677841187\n",
      "step :  168 loss :  0.080943264067173\n",
      "step :  169 loss :  0.08118047565221786\n",
      "step :  170 loss :  0.08091959357261658\n",
      "step :  171 loss :  0.08114957064390182\n",
      "step :  172 loss :  0.08094066381454468\n",
      "step :  173 loss :  0.08108459413051605\n",
      "step :  174 loss :  0.0809415951371193\n",
      "step :  175 loss :  0.0810319036245346\n",
      "step :  176 loss :  0.08099956065416336\n",
      "step :  177 loss :  0.08097279071807861\n",
      "step :  178 loss :  0.08092132210731506\n",
      "step :  179 loss :  0.08105537295341492\n",
      "step :  180 loss :  0.08088620752096176\n",
      "step :  181 loss :  0.08107093721628189\n",
      "step :  182 loss :  0.0809556394815445\n",
      "step :  183 loss :  0.08101649582386017\n",
      "step :  184 loss :  0.0809500515460968\n",
      "step :  185 loss :  0.08090867102146149\n",
      "step :  186 loss :  0.08099954575300217\n",
      "step :  187 loss :  0.08086758852005005\n",
      "step :  188 loss :  0.08104412257671356\n",
      "step :  189 loss :  0.08090496808290482\n",
      "step :  190 loss :  0.08104052394628525\n",
      "step :  191 loss :  0.08090794086456299\n",
      "step :  192 loss :  0.08090884983539581\n",
      "step :  193 loss :  0.08089353889226913\n",
      "step :  194 loss :  0.08087103068828583\n",
      "step :  195 loss :  0.08087680488824844\n",
      "step :  196 loss :  0.080940380692482\n",
      "step :  197 loss :  0.08087531477212906\n",
      "step :  198 loss :  0.08094793558120728\n",
      "step :  199 loss :  0.08089882880449295\n",
      "step :  200 loss :  0.08086559921503067\n",
      "step :  201 loss :  0.0809335708618164\n",
      "step :  202 loss :  0.08082576841115952\n",
      "step :  203 loss :  0.080885149538517\n",
      "step :  204 loss :  0.08083432912826538\n",
      "step :  205 loss :  0.08084641396999359\n",
      "step :  206 loss :  0.08084046840667725\n",
      "step :  207 loss :  0.08081956952810287\n",
      "step :  208 loss :  0.08089366555213928\n",
      "step :  209 loss :  0.08078313618898392\n",
      "step :  210 loss :  0.08090861886739731\n",
      "step :  211 loss :  0.08078429102897644\n",
      "step :  212 loss :  0.08089382201433182\n",
      "step :  213 loss :  0.08081058412790298\n",
      "step :  214 loss :  0.08083216100931168\n",
      "step :  215 loss :  0.0808040127158165\n",
      "step :  216 loss :  0.08079171925783157\n",
      "step :  217 loss :  0.08081746846437454\n",
      "step :  218 loss :  0.08079130202531815\n",
      "step :  219 loss :  0.08083689212799072\n",
      "step :  220 loss :  0.08078724890947342\n",
      "step :  221 loss :  0.08082897216081619\n",
      "step :  222 loss :  0.08078732341527939\n",
      "step :  223 loss :  0.08079111576080322\n",
      "step :  224 loss :  0.08077522367238998\n",
      "step :  225 loss :  0.08079760521650314\n",
      "step :  226 loss :  0.08077991008758545\n",
      "step :  227 loss :  0.08078262209892273\n",
      "step :  228 loss :  0.08077709376811981\n",
      "step :  229 loss :  0.08077457547187805\n",
      "step :  230 loss :  0.08078477531671524\n",
      "step :  231 loss :  0.08075901865959167\n",
      "step :  232 loss :  0.08081843703985214\n",
      "step :  233 loss :  0.08075075596570969\n",
      "step :  234 loss :  0.08079741150140762\n",
      "step :  235 loss :  0.0807625949382782\n",
      "step :  236 loss :  0.08078515529632568\n",
      "step :  237 loss :  0.08075869083404541\n",
      "step :  238 loss :  0.08078283071517944\n",
      "step :  239 loss :  0.08076729625463486\n",
      "step :  240 loss :  0.08075568079948425\n",
      "step :  241 loss :  0.08076241612434387\n",
      "step :  242 loss :  0.08077625185251236\n",
      "step :  243 loss :  0.0807420015335083\n",
      "step :  244 loss :  0.08081398904323578\n",
      "step :  245 loss :  0.08075819164514542\n",
      "step :  246 loss :  0.08076675236225128\n",
      "step :  247 loss :  0.08073727041482925\n",
      "step :  248 loss :  0.08075439929962158\n",
      "step :  249 loss :  0.08074390143156052\n",
      "step :  250 loss :  0.08075603097677231\n",
      "step :  251 loss :  0.0807471051812172\n",
      "step :  252 loss :  0.08076000213623047\n",
      "step :  253 loss :  0.08072765171527863\n",
      "step :  254 loss :  0.08080097287893295\n",
      "step :  255 loss :  0.08072704076766968\n",
      "step :  256 loss :  0.08074132353067398\n",
      "step :  257 loss :  0.08073675632476807\n",
      "step :  258 loss :  0.0807347297668457\n",
      "step :  259 loss :  0.08073831349611282\n",
      "step :  260 loss :  0.08073818683624268\n",
      "step :  261 loss :  0.08073442429304123\n",
      "step :  262 loss :  0.08073686063289642\n",
      "step :  263 loss :  0.08074799925088882\n",
      "step :  264 loss :  0.08072958886623383\n",
      "step :  265 loss :  0.08073832094669342\n",
      "step :  266 loss :  0.08071626722812653\n",
      "step :  267 loss :  0.08078736066818237\n",
      "step :  268 loss :  0.08072115480899811\n",
      "step :  269 loss :  0.0807119607925415\n",
      "step :  270 loss :  0.08078472316265106\n",
      "step :  271 loss :  0.08071606606245041\n",
      "step :  272 loss :  0.08070480078458786\n",
      "step :  273 loss :  0.08077220618724823\n",
      "step :  274 loss :  0.08071424812078476\n",
      "step :  275 loss :  0.08069998025894165\n",
      "step :  276 loss :  0.08075280487537384\n",
      "step :  277 loss :  0.08070846647024155\n",
      "step :  278 loss :  0.08070062845945358\n",
      "step :  279 loss :  0.08072991669178009\n",
      "step :  280 loss :  0.0807080939412117\n",
      "step :  281 loss :  0.08071081340312958\n",
      "step :  282 loss :  0.08071474730968475\n",
      "step :  283 loss :  0.0807170420885086\n",
      "step :  284 loss :  0.08071797341108322\n",
      "step :  285 loss :  0.08071998506784439\n",
      "step :  286 loss :  0.08071920275688171\n",
      "step :  287 loss :  0.08071493357419968\n",
      "step :  288 loss :  0.08070109784603119\n",
      "step :  289 loss :  0.08072198927402496\n",
      "step :  290 loss :  0.08070468157529831\n",
      "step :  291 loss :  0.08070220798254013\n",
      "step :  292 loss :  0.08071893453598022\n",
      "step :  293 loss :  0.08070451766252518\n",
      "step :  294 loss :  0.08071605116128922\n",
      "step :  295 loss :  0.08070732653141022\n",
      "step :  296 loss :  0.08070927113294601\n",
      "step :  297 loss :  0.08070941269397736\n",
      "step :  298 loss :  0.0807136744260788\n",
      "step :  299 loss :  0.08071334660053253\n",
      "step :  300 loss :  0.08070635050535202\n",
      "step :  301 loss :  0.0807117372751236\n",
      "step :  302 loss :  0.08070284128189087\n",
      "step :  303 loss :  0.08070550858974457\n",
      "step :  304 loss :  0.08070721477270126\n",
      "step :  305 loss :  0.08070673793554306\n",
      "step :  306 loss :  0.08070676773786545\n",
      "step :  307 loss :  0.08070388436317444\n",
      "step :  308 loss :  0.08070565015077591\n",
      "step :  309 loss :  0.08070634305477142\n",
      "step :  310 loss :  0.08070579916238785\n",
      "step :  311 loss :  0.08071195334196091\n",
      "step :  312 loss :  0.08070697635412216\n",
      "step :  313 loss :  0.08070537447929382\n",
      "step :  314 loss :  0.08070335537195206\n",
      "step :  315 loss :  0.08070702850818634\n",
      "step :  316 loss :  0.08070734143257141\n",
      "step :  317 loss :  0.08070376515388489\n",
      "step :  318 loss :  0.08071187883615494\n",
      "step :  319 loss :  0.08070044219493866\n",
      "step :  320 loss :  0.0807044580578804\n",
      "step :  321 loss :  0.08070807158946991\n",
      "step :  322 loss :  0.08069618046283722\n",
      "step :  323 loss :  0.08070212602615356\n",
      "step :  324 loss :  0.08070669323205948\n",
      "step :  325 loss :  0.08070027828216553\n",
      "step :  326 loss :  0.08070246130228043\n",
      "step :  327 loss :  0.08070141822099686\n",
      "step :  328 loss :  0.08070302754640579\n",
      "step :  329 loss :  0.08070268481969833\n",
      "step :  330 loss :  0.08070085197687149\n",
      "step :  331 loss :  0.08069892227649689\n",
      "step :  332 loss :  0.08070292323827744\n",
      "step :  333 loss :  0.0806967169046402\n",
      "step :  334 loss :  0.08069699257612228\n",
      "step :  335 loss :  0.08070351928472519\n",
      "step :  336 loss :  0.08069965243339539\n",
      "step :  337 loss :  0.08069974929094315\n",
      "step :  338 loss :  0.08070781826972961\n",
      "step :  339 loss :  0.08069849759340286\n",
      "step :  340 loss :  0.080697201192379\n",
      "step :  341 loss :  0.08070255815982819\n",
      "step :  342 loss :  0.08070047199726105\n",
      "step :  343 loss :  0.08069881051778793\n",
      "step :  344 loss :  0.08070255070924759\n",
      "step :  345 loss :  0.0806962251663208\n",
      "step :  346 loss :  0.08069831132888794\n",
      "step :  347 loss :  0.08070341497659683\n",
      "step :  348 loss :  0.08069760352373123\n",
      "step :  349 loss :  0.08069755882024765\n",
      "step :  350 loss :  0.08069923520088196\n",
      "step :  351 loss :  0.08069892972707748\n",
      "step :  352 loss :  0.08069715648889542\n",
      "step :  353 loss :  0.08069873601198196\n",
      "step :  354 loss :  0.08070145547389984\n",
      "step :  355 loss :  0.08069818466901779\n",
      "step :  356 loss :  0.08069900423288345\n",
      "step :  357 loss :  0.08070457726716995\n",
      "step :  358 loss :  0.08069727569818497\n",
      "step :  359 loss :  0.08069556951522827\n",
      "step :  360 loss :  0.08070025593042374\n",
      "step :  361 loss :  0.0806988924741745\n",
      "step :  362 loss :  0.08069738745689392\n",
      "step :  363 loss :  0.0807001069188118\n",
      "step :  364 loss :  0.08069757372140884\n",
      "step :  365 loss :  0.08069851994514465\n",
      "step :  366 loss :  0.08070123940706253\n",
      "step :  367 loss :  0.08070020377635956\n",
      "step :  368 loss :  0.08069989830255508\n",
      "step :  369 loss :  0.08070005476474762\n",
      "step :  370 loss :  0.08069849759340286\n",
      "step :  371 loss :  0.08069952577352524\n",
      "step :  372 loss :  0.08070000261068344\n",
      "step :  373 loss :  0.0806993767619133\n",
      "step :  374 loss :  0.08069930970668793\n",
      "step :  375 loss :  0.08070109039545059\n",
      "step :  376 loss :  0.08069978654384613\n",
      "step :  377 loss :  0.0807008147239685\n",
      "step :  378 loss :  0.0807000994682312\n",
      "step :  379 loss :  0.08070002496242523\n",
      "step :  380 loss :  0.08069886267185211\n",
      "step :  381 loss :  0.08070007711648941\n",
      "step :  382 loss :  0.08069928735494614\n",
      "step :  383 loss :  0.08069805055856705\n",
      "step :  384 loss :  0.08069998025894165\n",
      "step :  385 loss :  0.08070050179958344\n",
      "step :  386 loss :  0.08070003986358643\n",
      "step :  387 loss :  0.08070115745067596\n",
      "step :  388 loss :  0.08069824427366257\n",
      "step :  389 loss :  0.08069945126771927\n",
      "step :  390 loss :  0.08069781959056854\n",
      "step :  391 loss :  0.08069801330566406\n",
      "step :  392 loss :  0.08070150017738342\n",
      "step :  393 loss :  0.08069760352373123\n",
      "step :  394 loss :  0.08069715648889542\n",
      "step :  395 loss :  0.08070108294487\n",
      "step :  396 loss :  0.08069879561662674\n",
      "step :  397 loss :  0.08069773018360138\n",
      "step :  398 loss :  0.0806988924741745\n",
      "step :  399 loss :  0.08069783449172974\n",
      "step :  400 loss :  0.08069855719804764\n",
      "step :  401 loss :  0.08070105314254761\n",
      "step :  402 loss :  0.08069799095392227\n",
      "step :  403 loss :  0.08069804310798645\n",
      "step :  404 loss :  0.0807005763053894\n",
      "step :  405 loss :  0.08069929480552673\n",
      "step :  406 loss :  0.08069809526205063\n",
      "step :  407 loss :  0.08069851994514465\n",
      "step :  408 loss :  0.0806984156370163\n",
      "step :  409 loss :  0.08069785684347153\n",
      "step :  410 loss :  0.08069919794797897\n",
      "step :  411 loss :  0.080698162317276\n",
      "step :  412 loss :  0.08069784194231033\n",
      "step :  413 loss :  0.08069818466901779\n",
      "step :  414 loss :  0.0806986466050148\n",
      "step :  415 loss :  0.08069805800914764\n",
      "step :  416 loss :  0.08069760352373123\n",
      "step :  417 loss :  0.0806979164481163\n",
      "step :  418 loss :  0.08069948107004166\n",
      "step :  419 loss :  0.08069711923599243\n",
      "step :  420 loss :  0.08069734275341034\n",
      "step :  421 loss :  0.0806986391544342\n",
      "step :  422 loss :  0.0806976929306984\n",
      "step :  423 loss :  0.08069726824760437\n",
      "step :  424 loss :  0.08069819957017899\n",
      "step :  425 loss :  0.08069729059934616\n",
      "step :  426 loss :  0.08069683611392975\n",
      "step :  427 loss :  0.08069778978824615\n",
      "step :  428 loss :  0.0806979387998581\n",
      "step :  429 loss :  0.0806984156370163\n",
      "step :  430 loss :  0.08069749176502228\n",
      "step :  431 loss :  0.08069776743650436\n",
      "step :  432 loss :  0.08069813251495361\n",
      "step :  433 loss :  0.08069818466901779\n",
      "step :  434 loss :  0.08069774508476257\n",
      "step :  435 loss :  0.08069811016321182\n",
      "step :  436 loss :  0.08069798350334167\n",
      "step :  437 loss :  0.08069834113121033\n",
      "step :  438 loss :  0.08069825917482376\n",
      "step :  439 loss :  0.0806989073753357\n",
      "step :  440 loss :  0.08069827407598495\n",
      "step :  441 loss :  0.08069679886102676\n",
      "step :  442 loss :  0.08069705218076706\n",
      "step :  443 loss :  0.0806984230875969\n",
      "step :  444 loss :  0.08069809526205063\n",
      "step :  445 loss :  0.0806969478726387\n",
      "step :  446 loss :  0.0806971937417984\n",
      "step :  447 loss :  0.08069787174463272\n",
      "step :  448 loss :  0.0806976705789566\n",
      "step :  449 loss :  0.08069756627082825\n",
      "step :  450 loss :  0.08069732785224915\n",
      "step :  451 loss :  0.0806971937417984\n",
      "step :  452 loss :  0.08069738745689392\n",
      "step :  453 loss :  0.08069773018360138\n",
      "step :  454 loss :  0.0806979089975357\n",
      "step :  455 loss :  0.08069790154695511\n",
      "step :  456 loss :  0.08069808036088943\n",
      "step :  457 loss :  0.08069827407598495\n",
      "step :  458 loss :  0.08069773018360138\n",
      "step :  459 loss :  0.08069761842489243\n",
      "step :  460 loss :  0.08069770783185959\n",
      "step :  461 loss :  0.0806974321603775\n",
      "step :  462 loss :  0.08069786429405212\n",
      "step :  463 loss :  0.08069863170385361\n",
      "step :  464 loss :  0.08069760352373123\n",
      "step :  465 loss :  0.08069755882024765\n",
      "step :  466 loss :  0.08069782704114914\n",
      "step :  467 loss :  0.08069803565740585\n",
      "step :  468 loss :  0.08069738745689392\n",
      "step :  469 loss :  0.08069724589586258\n",
      "step :  470 loss :  0.08069756627082825\n",
      "step :  471 loss :  0.0806979387998581\n",
      "step :  472 loss :  0.08069756627082825\n",
      "step :  473 loss :  0.08069730550050735\n",
      "step :  474 loss :  0.08069760352373123\n",
      "step :  475 loss :  0.08069855719804764\n",
      "step :  476 loss :  0.08069758117198944\n",
      "step :  477 loss :  0.08069725334644318\n",
      "step :  478 loss :  0.08069732040166855\n",
      "step :  479 loss :  0.08069754391908646\n",
      "step :  480 loss :  0.08069764822721481\n",
      "step :  481 loss :  0.0806976929306984\n",
      "step :  482 loss :  0.08069746941328049\n",
      "step :  483 loss :  0.08069749921560287\n",
      "step :  484 loss :  0.0806976780295372\n",
      "step :  485 loss :  0.0806976854801178\n",
      "step :  486 loss :  0.0806974396109581\n",
      "step :  487 loss :  0.08069780468940735\n",
      "step :  488 loss :  0.08069739490747452\n",
      "step :  489 loss :  0.08069736510515213\n",
      "step :  490 loss :  0.08069727569818497\n",
      "step :  491 loss :  0.08069724589586258\n",
      "step :  492 loss :  0.08069749921560287\n",
      "step :  493 loss :  0.08069761842489243\n",
      "step :  494 loss :  0.08069732040166855\n",
      "step :  495 loss :  0.08069731295108795\n",
      "step :  496 loss :  0.08069735765457153\n",
      "step :  497 loss :  0.08069773018360138\n",
      "step :  498 loss :  0.08069736510515213\n",
      "step :  499 loss :  0.08069726824760437\n",
      "Training model 9\n",
      "step :  0 loss :  0.32311663031578064\n",
      "step :  1 loss :  0.24327637255191803\n",
      "step :  2 loss :  0.215192511677742\n",
      "step :  3 loss :  0.23897875845432281\n",
      "step :  4 loss :  0.20334860682487488\n",
      "step :  5 loss :  0.22878141701221466\n",
      "step :  6 loss :  0.20624564588069916\n",
      "step :  7 loss :  0.2092352658510208\n",
      "step :  8 loss :  0.18406692147254944\n",
      "step :  9 loss :  0.18391916155815125\n",
      "step :  10 loss :  0.16769756376743317\n",
      "step :  11 loss :  0.16183960437774658\n",
      "step :  12 loss :  0.15804290771484375\n",
      "step :  13 loss :  0.1332065463066101\n",
      "step :  14 loss :  0.15977470576763153\n",
      "step :  15 loss :  0.15045061707496643\n",
      "step :  16 loss :  0.16409309208393097\n",
      "step :  17 loss :  0.12733674049377441\n",
      "step :  18 loss :  0.14033018052577972\n",
      "step :  19 loss :  0.14748689532279968\n",
      "step :  20 loss :  0.1505320817232132\n",
      "step :  21 loss :  0.14811235666275024\n",
      "step :  22 loss :  0.11634641885757446\n",
      "step :  23 loss :  0.1140422523021698\n",
      "step :  24 loss :  0.11674892157316208\n",
      "step :  25 loss :  0.11717639118432999\n",
      "step :  26 loss :  0.11289017647504807\n",
      "step :  27 loss :  0.10942568629980087\n",
      "step :  28 loss :  0.10838819295167923\n",
      "step :  29 loss :  0.12453845888376236\n",
      "step :  30 loss :  0.10330057144165039\n",
      "step :  31 loss :  0.10873886197805405\n",
      "step :  32 loss :  0.1117788776755333\n",
      "step :  33 loss :  0.10251729935407639\n",
      "step :  34 loss :  0.09716852754354477\n",
      "step :  35 loss :  0.09446141868829727\n",
      "step :  36 loss :  0.10037209838628769\n",
      "step :  37 loss :  0.09933189302682877\n",
      "step :  38 loss :  0.09203090518712997\n",
      "step :  39 loss :  0.10915679484605789\n",
      "step :  40 loss :  0.09116977453231812\n",
      "step :  41 loss :  0.08922109752893448\n",
      "step :  42 loss :  0.08944445848464966\n",
      "step :  43 loss :  0.0892159715294838\n",
      "step :  44 loss :  0.09334547072649002\n",
      "step :  45 loss :  0.08495986461639404\n",
      "step :  46 loss :  0.08908507227897644\n",
      "step :  47 loss :  0.08447224646806717\n",
      "step :  48 loss :  0.08989083766937256\n",
      "step :  49 loss :  0.08357448875904083\n",
      "step :  50 loss :  0.09718014299869537\n",
      "step :  51 loss :  0.0980893224477768\n",
      "step :  52 loss :  0.08782221376895905\n",
      "step :  53 loss :  0.08883672952651978\n",
      "step :  54 loss :  0.08423148840665817\n",
      "step :  55 loss :  0.08976771682500839\n",
      "step :  56 loss :  0.09061700105667114\n",
      "step :  57 loss :  0.08433172106742859\n",
      "step :  58 loss :  0.08372297137975693\n",
      "step :  59 loss :  0.08775680512189865\n",
      "step :  60 loss :  0.0842970684170723\n",
      "step :  61 loss :  0.08354178816080093\n",
      "step :  62 loss :  0.08503822982311249\n",
      "step :  63 loss :  0.08663132786750793\n",
      "step :  64 loss :  0.09379363805055618\n",
      "step :  65 loss :  0.08385400474071503\n",
      "step :  66 loss :  0.08363858610391617\n",
      "step :  67 loss :  0.08595547825098038\n",
      "step :  68 loss :  0.08810033649206161\n",
      "step :  69 loss :  0.09173402935266495\n",
      "step :  70 loss :  0.08770260214805603\n",
      "step :  71 loss :  0.08536352962255478\n",
      "step :  72 loss :  0.08362677693367004\n",
      "step :  73 loss :  0.08696703612804413\n",
      "step :  74 loss :  0.08726842701435089\n",
      "step :  75 loss :  0.08355476707220078\n",
      "step :  76 loss :  0.08316004276275635\n",
      "step :  77 loss :  0.08302686363458633\n",
      "step :  78 loss :  0.08835837244987488\n",
      "step :  79 loss :  0.08611328154802322\n",
      "step :  80 loss :  0.08364462107419968\n",
      "step :  81 loss :  0.08262739330530167\n",
      "step :  82 loss :  0.08282407373189926\n",
      "step :  83 loss :  0.08506760001182556\n",
      "step :  84 loss :  0.08390834182500839\n",
      "step :  85 loss :  0.08193624764680862\n",
      "step :  86 loss :  0.08376428484916687\n",
      "step :  87 loss :  0.08446875959634781\n",
      "step :  88 loss :  0.08158884942531586\n",
      "step :  89 loss :  0.08253875374794006\n",
      "step :  90 loss :  0.08272150158882141\n",
      "step :  91 loss :  0.08547012507915497\n",
      "step :  92 loss :  0.08551707118749619\n",
      "step :  93 loss :  0.08549874275922775\n",
      "step :  94 loss :  0.08459550887346268\n",
      "step :  95 loss :  0.08316010236740112\n",
      "step :  96 loss :  0.08302302658557892\n",
      "step :  97 loss :  0.08276962488889694\n",
      "step :  98 loss :  0.08407386392354965\n",
      "step :  99 loss :  0.08301323652267456\n",
      "step :  100 loss :  0.08247227966785431\n",
      "step :  101 loss :  0.08385977149009705\n",
      "step :  102 loss :  0.08156003057956696\n",
      "step :  103 loss :  0.08165179938077927\n",
      "step :  104 loss :  0.0816144198179245\n",
      "step :  105 loss :  0.08133044093847275\n",
      "step :  106 loss :  0.08106738328933716\n",
      "step :  107 loss :  0.08096116781234741\n",
      "step :  108 loss :  0.08113544434309006\n",
      "step :  109 loss :  0.08104108273983002\n",
      "step :  110 loss :  0.082118920981884\n",
      "step :  111 loss :  0.0834333598613739\n",
      "step :  112 loss :  0.08144348114728928\n",
      "step :  113 loss :  0.0820838212966919\n",
      "step :  114 loss :  0.08242680132389069\n",
      "step :  115 loss :  0.08140098303556442\n",
      "step :  116 loss :  0.08145444840192795\n",
      "step :  117 loss :  0.08118936419487\n",
      "step :  118 loss :  0.08094824105501175\n",
      "step :  119 loss :  0.08085384219884872\n",
      "step :  120 loss :  0.08099295943975449\n",
      "step :  121 loss :  0.08078759908676147\n",
      "step :  122 loss :  0.08070024847984314\n",
      "step :  123 loss :  0.08064649254083633\n",
      "step :  124 loss :  0.08060503005981445\n",
      "step :  125 loss :  0.08053463697433472\n",
      "step :  126 loss :  0.08053219318389893\n",
      "step :  127 loss :  0.08039414882659912\n",
      "step :  128 loss :  0.08045404404401779\n",
      "step :  129 loss :  0.08043040335178375\n",
      "step :  130 loss :  0.08082848787307739\n",
      "step :  131 loss :  0.08067887276411057\n",
      "step :  132 loss :  0.0804041400551796\n",
      "step :  133 loss :  0.080403633415699\n",
      "step :  134 loss :  0.08031067252159119\n",
      "step :  135 loss :  0.08023116737604141\n",
      "step :  136 loss :  0.0802222415804863\n",
      "step :  137 loss :  0.08014606684446335\n",
      "step :  138 loss :  0.08022808283567429\n",
      "step :  139 loss :  0.08015555888414383\n",
      "step :  140 loss :  0.08014072477817535\n",
      "step :  141 loss :  0.08010672777891159\n",
      "step :  142 loss :  0.08024106174707413\n",
      "step :  143 loss :  0.08008594810962677\n",
      "step :  144 loss :  0.0800127387046814\n",
      "step :  145 loss :  0.08006379753351212\n",
      "step :  146 loss :  0.07999606430530548\n",
      "step :  147 loss :  0.08004123717546463\n",
      "step :  148 loss :  0.07999713718891144\n",
      "step :  149 loss :  0.07994256913661957\n",
      "step :  150 loss :  0.08004158735275269\n",
      "step :  151 loss :  0.08015339076519012\n",
      "step :  152 loss :  0.07994219660758972\n",
      "step :  153 loss :  0.0799536257982254\n",
      "step :  154 loss :  0.07996091991662979\n",
      "step :  155 loss :  0.07984437048435211\n",
      "step :  156 loss :  0.07985569536685944\n",
      "step :  157 loss :  0.07981697469949722\n",
      "step :  158 loss :  0.07981494814157486\n",
      "step :  159 loss :  0.07983085513114929\n",
      "step :  160 loss :  0.07999870926141739\n",
      "step :  161 loss :  0.08011951297521591\n",
      "step :  162 loss :  0.07980391383171082\n",
      "step :  163 loss :  0.07981640845537186\n",
      "step :  164 loss :  0.07980639487504959\n",
      "step :  165 loss :  0.07978370785713196\n",
      "step :  166 loss :  0.07975929975509644\n",
      "step :  167 loss :  0.07976184040307999\n",
      "step :  168 loss :  0.07977531105279922\n",
      "step :  169 loss :  0.0797552838921547\n",
      "step :  170 loss :  0.07981044054031372\n",
      "step :  171 loss :  0.07974618673324585\n",
      "step :  172 loss :  0.07974711805582047\n",
      "step :  173 loss :  0.07969768345355988\n",
      "step :  174 loss :  0.07974439859390259\n",
      "step :  175 loss :  0.07967832684516907\n",
      "step :  176 loss :  0.0797247439622879\n",
      "step :  177 loss :  0.07969442009925842\n",
      "step :  178 loss :  0.07967604696750641\n",
      "step :  179 loss :  0.07969264686107635\n",
      "step :  180 loss :  0.07966772466897964\n",
      "step :  181 loss :  0.07967556267976761\n",
      "step :  182 loss :  0.07962004095315933\n",
      "step :  183 loss :  0.07962541282176971\n",
      "step :  184 loss :  0.07963136583566666\n",
      "step :  185 loss :  0.07961955666542053\n",
      "step :  186 loss :  0.07960538566112518\n",
      "step :  187 loss :  0.0795823484659195\n",
      "step :  188 loss :  0.07957927137613297\n",
      "step :  189 loss :  0.0795750543475151\n",
      "step :  190 loss :  0.07957138121128082\n",
      "step :  191 loss :  0.07955935597419739\n",
      "step :  192 loss :  0.07955396920442581\n",
      "step :  193 loss :  0.07954157143831253\n",
      "step :  194 loss :  0.07953771948814392\n",
      "step :  195 loss :  0.0795423835515976\n",
      "step :  196 loss :  0.07957178354263306\n",
      "step :  197 loss :  0.07953273504972458\n",
      "step :  198 loss :  0.07952338457107544\n",
      "step :  199 loss :  0.07951227575540543\n",
      "step :  200 loss :  0.0795108750462532\n",
      "step :  201 loss :  0.07949940115213394\n",
      "step :  202 loss :  0.07950497418642044\n",
      "step :  203 loss :  0.0795019343495369\n",
      "step :  204 loss :  0.07949768006801605\n",
      "step :  205 loss :  0.07949487864971161\n",
      "step :  206 loss :  0.07949291169643402\n",
      "step :  207 loss :  0.0794973149895668\n",
      "step :  208 loss :  0.07951810956001282\n",
      "step :  209 loss :  0.07958449423313141\n",
      "step :  210 loss :  0.07949287444353104\n",
      "step :  211 loss :  0.07948774099349976\n",
      "step :  212 loss :  0.07947848737239838\n",
      "step :  213 loss :  0.07948049902915955\n",
      "step :  214 loss :  0.07947757095098495\n",
      "step :  215 loss :  0.07946859300136566\n",
      "step :  216 loss :  0.07949499785900116\n",
      "step :  217 loss :  0.07946416735649109\n",
      "step :  218 loss :  0.0794721320271492\n",
      "step :  219 loss :  0.07946205884218216\n",
      "step :  220 loss :  0.07946314662694931\n",
      "step :  221 loss :  0.07945608347654343\n",
      "step :  222 loss :  0.07945038378238678\n",
      "step :  223 loss :  0.07944764941930771\n",
      "step :  224 loss :  0.07945387810468674\n",
      "step :  225 loss :  0.07943131774663925\n",
      "step :  226 loss :  0.07943993806838989\n",
      "step :  227 loss :  0.0794411450624466\n",
      "step :  228 loss :  0.0794379785656929\n",
      "step :  229 loss :  0.07945453375577927\n",
      "step :  230 loss :  0.07942504435777664\n",
      "step :  231 loss :  0.07943332195281982\n",
      "step :  232 loss :  0.07942438870668411\n",
      "step :  233 loss :  0.07941877096891403\n",
      "step :  234 loss :  0.07942181080579758\n",
      "step :  235 loss :  0.07941768318414688\n",
      "step :  236 loss :  0.07940984517335892\n",
      "step :  237 loss :  0.07940948754549026\n",
      "step :  238 loss :  0.07941009849309921\n",
      "step :  239 loss :  0.07940955460071564\n",
      "step :  240 loss :  0.0794263482093811\n",
      "step :  241 loss :  0.07940027862787247\n",
      "step :  242 loss :  0.07942298799753189\n",
      "step :  243 loss :  0.07940392196178436\n",
      "step :  244 loss :  0.07940015941858292\n",
      "step :  245 loss :  0.07941536605358124\n",
      "step :  246 loss :  0.07939562201499939\n",
      "step :  247 loss :  0.07939545065164566\n",
      "step :  248 loss :  0.0793970599770546\n",
      "step :  249 loss :  0.07939237356185913\n",
      "step :  250 loss :  0.07939118146896362\n",
      "step :  251 loss :  0.07938579469919205\n",
      "step :  252 loss :  0.07938608527183533\n",
      "step :  253 loss :  0.0793842226266861\n",
      "step :  254 loss :  0.07939308136701584\n",
      "step :  255 loss :  0.0793943777680397\n",
      "step :  256 loss :  0.07938425987958908\n",
      "step :  257 loss :  0.07938292622566223\n",
      "step :  258 loss :  0.0793822780251503\n",
      "step :  259 loss :  0.07938271015882492\n",
      "step :  260 loss :  0.0793825164437294\n",
      "step :  261 loss :  0.07938238978385925\n",
      "step :  262 loss :  0.07938268035650253\n",
      "step :  263 loss :  0.07938486337661743\n",
      "step :  264 loss :  0.0793771967291832\n",
      "step :  265 loss :  0.07938432693481445\n",
      "step :  266 loss :  0.07937273383140564\n",
      "step :  267 loss :  0.07937151193618774\n",
      "step :  268 loss :  0.0793784111738205\n",
      "step :  269 loss :  0.07937341928482056\n",
      "step :  270 loss :  0.07937098294496536\n",
      "step :  271 loss :  0.07937038689851761\n",
      "step :  272 loss :  0.07936884462833405\n",
      "step :  273 loss :  0.07936996221542358\n",
      "step :  274 loss :  0.07936687022447586\n",
      "step :  275 loss :  0.07937010377645493\n",
      "step :  276 loss :  0.07937130331993103\n",
      "step :  277 loss :  0.07936854660511017\n",
      "step :  278 loss :  0.07936735451221466\n",
      "step :  279 loss :  0.07936827093362808\n",
      "step :  280 loss :  0.07937438786029816\n",
      "step :  281 loss :  0.07936955243349075\n",
      "step :  282 loss :  0.07936573028564453\n",
      "step :  283 loss :  0.07936341315507889\n",
      "step :  284 loss :  0.07936674356460571\n",
      "step :  285 loss :  0.07936272025108337\n",
      "step :  286 loss :  0.07935947179794312\n",
      "step :  287 loss :  0.0793580412864685\n",
      "step :  288 loss :  0.07935795933008194\n",
      "step :  289 loss :  0.07936471700668335\n",
      "step :  290 loss :  0.07935895025730133\n",
      "step :  291 loss :  0.07936064153909683\n",
      "step :  292 loss :  0.07935848087072372\n",
      "step :  293 loss :  0.0793592631816864\n",
      "step :  294 loss :  0.0793597549200058\n",
      "step :  295 loss :  0.07935689389705658\n",
      "step :  296 loss :  0.07935583591461182\n",
      "step :  297 loss :  0.07936179637908936\n",
      "step :  298 loss :  0.07935908436775208\n",
      "step :  299 loss :  0.07935578376054764\n",
      "step :  300 loss :  0.07935462892055511\n",
      "step :  301 loss :  0.07935384660959244\n",
      "step :  302 loss :  0.07935308665037155\n",
      "step :  303 loss :  0.07935421168804169\n",
      "step :  304 loss :  0.07935471832752228\n",
      "step :  305 loss :  0.07935169339179993\n",
      "step :  306 loss :  0.07935172319412231\n",
      "step :  307 loss :  0.07935743778944016\n",
      "step :  308 loss :  0.07935354113578796\n",
      "step :  309 loss :  0.07935205101966858\n",
      "step :  310 loss :  0.07935164868831635\n",
      "step :  311 loss :  0.07935132086277008\n",
      "step :  312 loss :  0.07935293018817902\n",
      "step :  313 loss :  0.0793517455458641\n",
      "step :  314 loss :  0.07935094833374023\n",
      "step :  315 loss :  0.07935060560703278\n",
      "step :  316 loss :  0.0793495774269104\n",
      "step :  317 loss :  0.07934814691543579\n",
      "step :  318 loss :  0.07934706658124924\n",
      "step :  319 loss :  0.07934807986021042\n",
      "step :  320 loss :  0.0793500542640686\n",
      "step :  321 loss :  0.07934772223234177\n",
      "step :  322 loss :  0.07934798300266266\n",
      "step :  323 loss :  0.07934768497943878\n",
      "step :  324 loss :  0.07934602349996567\n",
      "step :  325 loss :  0.07934676855802536\n",
      "step :  326 loss :  0.07934732735157013\n",
      "step :  327 loss :  0.07934801280498505\n",
      "step :  328 loss :  0.07934639602899551\n",
      "step :  329 loss :  0.07934572547674179\n",
      "step :  330 loss :  0.07934606075286865\n",
      "step :  331 loss :  0.07934613525867462\n",
      "step :  332 loss :  0.07934454083442688\n",
      "step :  333 loss :  0.07934440672397614\n",
      "step :  334 loss :  0.0793452188372612\n",
      "step :  335 loss :  0.07934493571519852\n",
      "step :  336 loss :  0.07934409379959106\n",
      "step :  337 loss :  0.0793435126543045\n",
      "step :  338 loss :  0.07934368401765823\n",
      "step :  339 loss :  0.07934293895959854\n",
      "step :  340 loss :  0.07934224605560303\n",
      "step :  341 loss :  0.07934331893920898\n",
      "step :  342 loss :  0.07934322208166122\n",
      "step :  343 loss :  0.07934276759624481\n",
      "step :  344 loss :  0.07934224605560303\n",
      "step :  345 loss :  0.07934276759624481\n",
      "step :  346 loss :  0.07934222370386124\n",
      "step :  347 loss :  0.07934194803237915\n",
      "step :  348 loss :  0.07934131473302841\n",
      "step :  349 loss :  0.07934144139289856\n",
      "step :  350 loss :  0.07934156805276871\n",
      "step :  351 loss :  0.07934064418077469\n",
      "step :  352 loss :  0.07934191077947617\n",
      "step :  353 loss :  0.07934075593948364\n",
      "step :  354 loss :  0.07934120297431946\n",
      "step :  355 loss :  0.07934112846851349\n",
      "step :  356 loss :  0.07934063673019409\n",
      "step :  357 loss :  0.0793406143784523\n",
      "step :  358 loss :  0.07933998852968216\n",
      "step :  359 loss :  0.07933961600065231\n",
      "step :  360 loss :  0.07933980226516724\n",
      "step :  361 loss :  0.07934020459651947\n",
      "step :  362 loss :  0.0793401375412941\n",
      "step :  363 loss :  0.07933980226516724\n",
      "step :  364 loss :  0.07933949679136276\n",
      "step :  365 loss :  0.07933962345123291\n",
      "step :  366 loss :  0.07933937013149261\n",
      "step :  367 loss :  0.07933874428272247\n",
      "step :  368 loss :  0.07933907210826874\n",
      "step :  369 loss :  0.07933933287858963\n",
      "step :  370 loss :  0.07933878153562546\n",
      "step :  371 loss :  0.07933833450078964\n",
      "step :  372 loss :  0.0793381929397583\n",
      "step :  373 loss :  0.07933828979730606\n",
      "step :  374 loss :  0.07933809608221054\n",
      "step :  375 loss :  0.07933788001537323\n",
      "step :  376 loss :  0.07933790236711502\n",
      "step :  377 loss :  0.07933760434389114\n",
      "step :  378 loss :  0.0793379694223404\n",
      "step :  379 loss :  0.0793379545211792\n",
      "step :  380 loss :  0.07933764159679413\n",
      "step :  381 loss :  0.07933733612298965\n",
      "step :  382 loss :  0.07933719456195831\n",
      "step :  383 loss :  0.07933719456195831\n",
      "step :  384 loss :  0.07933729887008667\n",
      "step :  385 loss :  0.07933735102415085\n",
      "step :  386 loss :  0.07933704555034637\n",
      "step :  387 loss :  0.0793367549777031\n",
      "step :  388 loss :  0.07933665066957474\n",
      "step :  389 loss :  0.07933659851551056\n",
      "step :  390 loss :  0.07933646440505981\n",
      "step :  391 loss :  0.07933630049228668\n",
      "step :  392 loss :  0.07933638989925385\n",
      "step :  393 loss :  0.0793365091085434\n",
      "step :  394 loss :  0.07933633774518967\n",
      "step :  395 loss :  0.07933644950389862\n",
      "step :  396 loss :  0.07933630794286728\n",
      "step :  397 loss :  0.07933591306209564\n",
      "step :  398 loss :  0.07933612912893295\n",
      "step :  399 loss :  0.07933641225099564\n",
      "step :  400 loss :  0.07933633774518967\n",
      "step :  401 loss :  0.07933584600687027\n",
      "step :  402 loss :  0.07933580130338669\n",
      "step :  403 loss :  0.07933587580919266\n",
      "step :  404 loss :  0.07933583110570908\n",
      "step :  405 loss :  0.0793355405330658\n",
      "step :  406 loss :  0.07933536171913147\n",
      "step :  407 loss :  0.07933536171913147\n",
      "step :  408 loss :  0.07933544367551804\n",
      "step :  409 loss :  0.0793355330824852\n",
      "step :  410 loss :  0.07933565229177475\n",
      "step :  411 loss :  0.07933538407087326\n",
      "step :  412 loss :  0.07933523505926132\n",
      "step :  413 loss :  0.07933534681797028\n",
      "step :  414 loss :  0.07933540642261505\n",
      "step :  415 loss :  0.07933517545461655\n",
      "step :  416 loss :  0.07933498173952103\n",
      "step :  417 loss :  0.07933511584997177\n",
      "step :  418 loss :  0.07933510839939117\n",
      "step :  419 loss :  0.0793350413441658\n",
      "step :  420 loss :  0.07933490723371506\n",
      "step :  421 loss :  0.07933506369590759\n",
      "step :  422 loss :  0.079335056245327\n",
      "step :  423 loss :  0.0793348178267479\n",
      "step :  424 loss :  0.0793348178267479\n",
      "step :  425 loss :  0.0793348029255867\n",
      "step :  426 loss :  0.07933477312326431\n",
      "step :  427 loss :  0.07933477312326431\n",
      "step :  428 loss :  0.07933484017848969\n",
      "step :  429 loss :  0.07933476567268372\n",
      "step :  430 loss :  0.07933459430932999\n",
      "step :  431 loss :  0.07933466136455536\n",
      "step :  432 loss :  0.07933469861745834\n",
      "step :  433 loss :  0.07933463156223297\n",
      "step :  434 loss :  0.07933457940816879\n",
      "step :  435 loss :  0.07933449000120163\n",
      "step :  436 loss :  0.07933451980352402\n",
      "step :  437 loss :  0.07933443784713745\n",
      "step :  438 loss :  0.07933437079191208\n",
      "step :  439 loss :  0.0793343186378479\n",
      "step :  440 loss :  0.07933440059423447\n",
      "step :  441 loss :  0.07933437824249268\n",
      "step :  442 loss :  0.07933435589075089\n",
      "step :  443 loss :  0.07933423668146133\n",
      "step :  444 loss :  0.07933414727449417\n",
      "step :  445 loss :  0.07933422178030014\n",
      "step :  446 loss :  0.07933425903320312\n",
      "step :  447 loss :  0.07933421432971954\n",
      "step :  448 loss :  0.07933415472507477\n",
      "step :  449 loss :  0.07933415472507477\n",
      "step :  450 loss :  0.07933419197797775\n",
      "step :  451 loss :  0.07933423668146133\n",
      "step :  452 loss :  0.07933417707681656\n",
      "step :  453 loss :  0.07933411747217178\n",
      "step :  454 loss :  0.07933410257101059\n",
      "step :  455 loss :  0.07933405041694641\n",
      "step :  456 loss :  0.07933397591114044\n",
      "step :  457 loss :  0.07933393120765686\n",
      "step :  458 loss :  0.07933400571346283\n",
      "step :  459 loss :  0.07933399826288223\n",
      "step :  460 loss :  0.07933393865823746\n",
      "step :  461 loss :  0.07933389395475388\n",
      "step :  462 loss :  0.07933393120765686\n",
      "step :  463 loss :  0.07933393865823746\n",
      "step :  464 loss :  0.07933393865823746\n",
      "step :  465 loss :  0.07933391630649567\n",
      "step :  466 loss :  0.07933388650417328\n",
      "step :  467 loss :  0.0793338268995285\n",
      "step :  468 loss :  0.07933372259140015\n",
      "step :  469 loss :  0.07933378964662552\n",
      "step :  470 loss :  0.0793338492512703\n",
      "step :  471 loss :  0.07933378964662552\n",
      "step :  472 loss :  0.07933375239372253\n",
      "step :  473 loss :  0.07933375239372253\n",
      "step :  474 loss :  0.07933373749256134\n",
      "step :  475 loss :  0.07933367788791656\n",
      "step :  476 loss :  0.07933368533849716\n",
      "step :  477 loss :  0.07933367043733597\n",
      "step :  478 loss :  0.07933364808559418\n",
      "step :  479 loss :  0.07933364063501358\n",
      "step :  480 loss :  0.0793336033821106\n",
      "step :  481 loss :  0.0793336033821106\n",
      "step :  482 loss :  0.0793335884809494\n",
      "step :  483 loss :  0.0793336033821106\n",
      "step :  484 loss :  0.07933361083269119\n",
      "step :  485 loss :  0.0793335884809494\n",
      "step :  486 loss :  0.07933355867862701\n",
      "step :  487 loss :  0.07933355867862701\n",
      "step :  488 loss :  0.07933349907398224\n",
      "step :  489 loss :  0.07933348417282104\n",
      "step :  490 loss :  0.07933349162340164\n",
      "step :  491 loss :  0.07933348417282104\n",
      "step :  492 loss :  0.07933346182107925\n",
      "step :  493 loss :  0.07933346182107925\n",
      "step :  494 loss :  0.07933346182107925\n",
      "step :  495 loss :  0.07933345437049866\n",
      "step :  496 loss :  0.07933341711759567\n",
      "step :  497 loss :  0.07933341711759567\n",
      "step :  498 loss :  0.07933341711759567\n",
      "step :  499 loss :  0.07933340966701508\n",
      "Training model 10\n",
      "step :  0 loss :  0.3734346926212311\n",
      "step :  1 loss :  0.24084706604480743\n",
      "step :  2 loss :  0.21424277126789093\n",
      "step :  3 loss :  0.2227206826210022\n",
      "step :  4 loss :  0.1955590397119522\n",
      "step :  5 loss :  0.18290093541145325\n",
      "step :  6 loss :  0.22700172662734985\n",
      "step :  7 loss :  0.20485994219779968\n",
      "step :  8 loss :  0.19753587245941162\n",
      "step :  9 loss :  0.1970190554857254\n",
      "step :  10 loss :  0.179314523935318\n",
      "step :  11 loss :  0.17689216136932373\n",
      "step :  12 loss :  0.16251660883426666\n",
      "step :  13 loss :  0.14550474286079407\n",
      "step :  14 loss :  0.1311936229467392\n",
      "step :  15 loss :  0.11723504960536957\n",
      "step :  16 loss :  0.10973279923200607\n",
      "step :  17 loss :  0.10478178411722183\n",
      "step :  18 loss :  0.11125072836875916\n",
      "step :  19 loss :  0.13681915402412415\n",
      "step :  20 loss :  0.11577275395393372\n",
      "step :  21 loss :  0.191319078207016\n",
      "step :  22 loss :  0.1318768858909607\n",
      "step :  23 loss :  0.1129523515701294\n",
      "step :  24 loss :  0.1375115066766739\n",
      "step :  25 loss :  0.14647367596626282\n",
      "step :  26 loss :  0.12839554250240326\n",
      "step :  27 loss :  0.11837409436702728\n",
      "step :  28 loss :  0.12848570942878723\n",
      "step :  29 loss :  0.09338285773992538\n",
      "step :  30 loss :  0.10431241244077682\n",
      "step :  31 loss :  0.0879741832613945\n",
      "step :  32 loss :  0.14313334226608276\n",
      "step :  33 loss :  0.1021166443824768\n",
      "step :  34 loss :  0.12265145778656006\n",
      "step :  35 loss :  0.0873938575387001\n",
      "step :  36 loss :  0.08419054746627808\n",
      "step :  37 loss :  0.12059348821640015\n",
      "step :  38 loss :  0.09061280637979507\n",
      "step :  39 loss :  0.13879583775997162\n",
      "step :  40 loss :  0.08773933351039886\n",
      "step :  41 loss :  0.11055975407361984\n",
      "step :  42 loss :  0.10431864112615585\n",
      "step :  43 loss :  0.08483311533927917\n",
      "step :  44 loss :  0.09606222063302994\n",
      "step :  45 loss :  0.0810098648071289\n",
      "step :  46 loss :  0.0848812460899353\n",
      "step :  47 loss :  0.0827915146946907\n",
      "step :  48 loss :  0.1193196251988411\n",
      "step :  49 loss :  0.10065082460641861\n",
      "step :  50 loss :  0.09350042790174484\n",
      "step :  51 loss :  0.09437776356935501\n",
      "step :  52 loss :  0.08486758917570114\n",
      "step :  53 loss :  0.08101820200681686\n",
      "step :  54 loss :  0.08462484925985336\n",
      "step :  55 loss :  0.08195222914218903\n",
      "step :  56 loss :  0.09776273369789124\n",
      "step :  57 loss :  0.08159766346216202\n",
      "step :  58 loss :  0.09222995489835739\n",
      "step :  59 loss :  0.08296851813793182\n",
      "step :  60 loss :  0.09035482257604599\n",
      "step :  61 loss :  0.08167380839586258\n",
      "step :  62 loss :  0.08305075764656067\n",
      "step :  63 loss :  0.08467318117618561\n",
      "step :  64 loss :  0.08836573362350464\n",
      "step :  65 loss :  0.08641473948955536\n",
      "step :  66 loss :  0.09025551378726959\n",
      "step :  67 loss :  0.08251180499792099\n",
      "step :  68 loss :  0.08078641444444656\n",
      "step :  69 loss :  0.08060495555400848\n",
      "step :  70 loss :  0.0887872502207756\n",
      "step :  71 loss :  0.08089811354875565\n",
      "step :  72 loss :  0.09014574438333511\n",
      "step :  73 loss :  0.08194231241941452\n",
      "step :  74 loss :  0.0817641019821167\n",
      "step :  75 loss :  0.0809377133846283\n",
      "step :  76 loss :  0.07978494465351105\n",
      "step :  77 loss :  0.0853230431675911\n",
      "step :  78 loss :  0.08190073072910309\n",
      "step :  79 loss :  0.08234470337629318\n",
      "step :  80 loss :  0.08078788220882416\n",
      "step :  81 loss :  0.08128949999809265\n",
      "step :  82 loss :  0.08275192975997925\n",
      "step :  83 loss :  0.07982353866100311\n",
      "step :  84 loss :  0.07989975064992905\n",
      "step :  85 loss :  0.07921668887138367\n",
      "step :  86 loss :  0.07983358949422836\n",
      "step :  87 loss :  0.08067134767770767\n",
      "step :  88 loss :  0.0809066891670227\n",
      "step :  89 loss :  0.081769660115242\n",
      "step :  90 loss :  0.08269647508859634\n",
      "step :  91 loss :  0.07932973653078079\n",
      "step :  92 loss :  0.07952209562063217\n",
      "step :  93 loss :  0.07994277775287628\n",
      "step :  94 loss :  0.07926452159881592\n",
      "step :  95 loss :  0.08052770793437958\n",
      "step :  96 loss :  0.07898719608783722\n",
      "step :  97 loss :  0.07936529070138931\n",
      "step :  98 loss :  0.07931925356388092\n",
      "step :  99 loss :  0.07868700474500656\n",
      "step :  100 loss :  0.07968495041131973\n",
      "step :  101 loss :  0.07913889735937119\n",
      "step :  102 loss :  0.08131273835897446\n",
      "step :  103 loss :  0.07988833636045456\n",
      "step :  104 loss :  0.07870275527238846\n",
      "step :  105 loss :  0.07929763942956924\n",
      "step :  106 loss :  0.0791647657752037\n",
      "step :  107 loss :  0.07916662842035294\n",
      "step :  108 loss :  0.0786922499537468\n",
      "step :  109 loss :  0.08050552010536194\n",
      "step :  110 loss :  0.0789656862616539\n",
      "step :  111 loss :  0.07936228066682816\n",
      "step :  112 loss :  0.0790720209479332\n",
      "step :  113 loss :  0.07908397912979126\n",
      "step :  114 loss :  0.08017271012067795\n",
      "step :  115 loss :  0.07862583547830582\n",
      "step :  116 loss :  0.0788741484284401\n",
      "step :  117 loss :  0.07857688516378403\n",
      "step :  118 loss :  0.07850436866283417\n",
      "step :  119 loss :  0.07877248525619507\n",
      "step :  120 loss :  0.07845725864171982\n",
      "step :  121 loss :  0.07851478457450867\n",
      "step :  122 loss :  0.07835119962692261\n",
      "step :  123 loss :  0.07836800068616867\n",
      "step :  124 loss :  0.078605055809021\n",
      "step :  125 loss :  0.07835592329502106\n",
      "step :  126 loss :  0.07841800898313522\n",
      "step :  127 loss :  0.07872305810451508\n",
      "step :  128 loss :  0.07962993532419205\n",
      "step :  129 loss :  0.07948958873748779\n",
      "step :  130 loss :  0.07874006032943726\n",
      "step :  131 loss :  0.0784609392285347\n",
      "step :  132 loss :  0.07841641455888748\n",
      "step :  133 loss :  0.07855557650327682\n",
      "step :  134 loss :  0.07889141142368317\n",
      "step :  135 loss :  0.0789559930562973\n",
      "step :  136 loss :  0.07844047248363495\n",
      "step :  137 loss :  0.07827253639698029\n",
      "step :  138 loss :  0.07834375649690628\n",
      "step :  139 loss :  0.07850686460733414\n",
      "step :  140 loss :  0.07818560302257538\n",
      "step :  141 loss :  0.0782165452837944\n",
      "step :  142 loss :  0.07826732844114304\n",
      "step :  143 loss :  0.07822054624557495\n",
      "step :  144 loss :  0.0781996101140976\n",
      "step :  145 loss :  0.07829193770885468\n",
      "step :  146 loss :  0.07815609872341156\n",
      "step :  147 loss :  0.07819927483797073\n",
      "step :  148 loss :  0.07819612324237823\n",
      "step :  149 loss :  0.0784369483590126\n",
      "step :  150 loss :  0.07830717414617538\n",
      "step :  151 loss :  0.07822520285844803\n",
      "step :  152 loss :  0.07812396436929703\n",
      "step :  153 loss :  0.07818646728992462\n",
      "step :  154 loss :  0.07816143333911896\n",
      "step :  155 loss :  0.07824196666479111\n",
      "step :  156 loss :  0.07817480713129044\n",
      "step :  157 loss :  0.07821168750524521\n",
      "step :  158 loss :  0.07812541723251343\n",
      "step :  159 loss :  0.07811586558818817\n",
      "step :  160 loss :  0.07810822874307632\n",
      "step :  161 loss :  0.07807854562997818\n",
      "step :  162 loss :  0.0783379003405571\n",
      "step :  163 loss :  0.07805346697568893\n",
      "step :  164 loss :  0.0781477838754654\n",
      "step :  165 loss :  0.07820548117160797\n",
      "step :  166 loss :  0.0780954360961914\n",
      "step :  167 loss :  0.07807653397321701\n",
      "step :  168 loss :  0.07811500877141953\n",
      "step :  169 loss :  0.07809899747371674\n",
      "step :  170 loss :  0.07806899398565292\n",
      "step :  171 loss :  0.07810130715370178\n",
      "step :  172 loss :  0.07805171608924866\n",
      "step :  173 loss :  0.07808872312307358\n",
      "step :  174 loss :  0.07805245369672775\n",
      "step :  175 loss :  0.0781487375497818\n",
      "step :  176 loss :  0.07802261412143707\n",
      "step :  177 loss :  0.07803420722484589\n",
      "step :  178 loss :  0.07808959484100342\n",
      "step :  179 loss :  0.07802357524633408\n",
      "step :  180 loss :  0.07811576128005981\n",
      "step :  181 loss :  0.07804650068283081\n",
      "step :  182 loss :  0.07822470366954803\n",
      "step :  183 loss :  0.07805100828409195\n",
      "step :  184 loss :  0.07824365049600601\n",
      "step :  185 loss :  0.07803473621606827\n",
      "step :  186 loss :  0.07806947082281113\n",
      "step :  187 loss :  0.07807149738073349\n",
      "step :  188 loss :  0.07805093377828598\n",
      "step :  189 loss :  0.0780373066663742\n",
      "step :  190 loss :  0.07802597433328629\n",
      "step :  191 loss :  0.07808219641447067\n",
      "step :  192 loss :  0.07802794128656387\n",
      "step :  193 loss :  0.07808681577444077\n",
      "step :  194 loss :  0.07804388552904129\n",
      "step :  195 loss :  0.0780300572514534\n",
      "step :  196 loss :  0.07803322374820709\n",
      "step :  197 loss :  0.078047014772892\n",
      "step :  198 loss :  0.07802066951990128\n",
      "step :  199 loss :  0.07804211229085922\n",
      "step :  200 loss :  0.07800639420747757\n",
      "step :  201 loss :  0.07803139835596085\n",
      "step :  202 loss :  0.07799897342920303\n",
      "step :  203 loss :  0.07801498472690582\n",
      "step :  204 loss :  0.07802177220582962\n",
      "step :  205 loss :  0.07799998670816422\n",
      "step :  206 loss :  0.07802668958902359\n",
      "step :  207 loss :  0.0780102014541626\n",
      "step :  208 loss :  0.07800242304801941\n",
      "step :  209 loss :  0.07801248878240585\n",
      "step :  210 loss :  0.07799264788627625\n",
      "step :  211 loss :  0.07801293581724167\n",
      "step :  212 loss :  0.07800376415252686\n",
      "step :  213 loss :  0.07799730449914932\n",
      "step :  214 loss :  0.07801671326160431\n",
      "step :  215 loss :  0.07798738777637482\n",
      "step :  216 loss :  0.07805166393518448\n",
      "step :  217 loss :  0.07798484712839127\n",
      "step :  218 loss :  0.077997125685215\n",
      "step :  219 loss :  0.07800512760877609\n",
      "step :  220 loss :  0.07798806577920914\n",
      "step :  221 loss :  0.07800385355949402\n",
      "step :  222 loss :  0.0779929906129837\n",
      "step :  223 loss :  0.078001469373703\n",
      "step :  224 loss :  0.07799717783927917\n",
      "step :  225 loss :  0.07799406349658966\n",
      "step :  226 loss :  0.07799277454614639\n",
      "step :  227 loss :  0.07800464332103729\n",
      "step :  228 loss :  0.07798925787210464\n",
      "step :  229 loss :  0.0779985785484314\n",
      "step :  230 loss :  0.07798826694488525\n",
      "step :  231 loss :  0.07799436897039413\n",
      "step :  232 loss :  0.07798602432012558\n",
      "step :  233 loss :  0.07798822969198227\n",
      "step :  234 loss :  0.07798361033201218\n",
      "step :  235 loss :  0.0779840350151062\n",
      "step :  236 loss :  0.07798461616039276\n",
      "step :  237 loss :  0.0779983177781105\n",
      "step :  238 loss :  0.07797921448945999\n",
      "step :  239 loss :  0.07798509299755096\n",
      "step :  240 loss :  0.07797831296920776\n",
      "step :  241 loss :  0.07798808813095093\n",
      "step :  242 loss :  0.07798705250024796\n",
      "step :  243 loss :  0.07798069715499878\n",
      "step :  244 loss :  0.07797881215810776\n",
      "step :  245 loss :  0.07799134403467178\n",
      "step :  246 loss :  0.07797472923994064\n",
      "step :  247 loss :  0.07797487080097198\n",
      "step :  248 loss :  0.0779755562543869\n",
      "step :  249 loss :  0.0779719427227974\n",
      "step :  250 loss :  0.07797354459762573\n",
      "step :  251 loss :  0.07797225564718246\n",
      "step :  252 loss :  0.07796954363584518\n",
      "step :  253 loss :  0.07799520343542099\n",
      "step :  254 loss :  0.07796905189752579\n",
      "step :  255 loss :  0.07797834277153015\n",
      "step :  256 loss :  0.07797279208898544\n",
      "step :  257 loss :  0.07796739041805267\n",
      "step :  258 loss :  0.07797978818416595\n",
      "step :  259 loss :  0.07796725630760193\n",
      "step :  260 loss :  0.07798352092504501\n",
      "step :  261 loss :  0.0779653936624527\n",
      "step :  262 loss :  0.07796555757522583\n",
      "step :  263 loss :  0.07797764986753464\n",
      "step :  264 loss :  0.07796205580234528\n",
      "step :  265 loss :  0.0779847800731659\n",
      "step :  266 loss :  0.07796528190374374\n",
      "step :  267 loss :  0.07796356081962585\n",
      "step :  268 loss :  0.07798144966363907\n",
      "step :  269 loss :  0.07797089964151382\n",
      "step :  270 loss :  0.07796027511358261\n",
      "step :  271 loss :  0.07797102630138397\n",
      "step :  272 loss :  0.0779607966542244\n",
      "step :  273 loss :  0.07797034084796906\n",
      "step :  274 loss :  0.0779629796743393\n",
      "step :  275 loss :  0.07796216011047363\n",
      "step :  276 loss :  0.07796180993318558\n",
      "step :  277 loss :  0.07796125113964081\n",
      "step :  278 loss :  0.07796800136566162\n",
      "step :  279 loss :  0.07795938849449158\n",
      "step :  280 loss :  0.07797368615865707\n",
      "step :  281 loss :  0.07796023041009903\n",
      "step :  282 loss :  0.07797077298164368\n",
      "step :  283 loss :  0.07796261459589005\n",
      "step :  284 loss :  0.07796555012464523\n",
      "step :  285 loss :  0.07795949280261993\n",
      "step :  286 loss :  0.07795794308185577\n",
      "step :  287 loss :  0.07796622812747955\n",
      "step :  288 loss :  0.07795663923025131\n",
      "step :  289 loss :  0.07796323299407959\n",
      "step :  290 loss :  0.07795689254999161\n",
      "step :  291 loss :  0.07796147465705872\n",
      "step :  292 loss :  0.07795552164316177\n",
      "step :  293 loss :  0.07795581221580505\n",
      "step :  294 loss :  0.07795348018407822\n",
      "step :  295 loss :  0.07796455919742584\n",
      "step :  296 loss :  0.07795430719852448\n",
      "step :  297 loss :  0.07796084880828857\n",
      "step :  298 loss :  0.07795462757349014\n",
      "step :  299 loss :  0.07796242833137512\n",
      "step :  300 loss :  0.0779566690325737\n",
      "step :  301 loss :  0.0779540166258812\n",
      "step :  302 loss :  0.07795825600624084\n",
      "step :  303 loss :  0.0779527798295021\n",
      "step :  304 loss :  0.0779590830206871\n",
      "step :  305 loss :  0.07795299589633942\n",
      "step :  306 loss :  0.07795816659927368\n",
      "step :  307 loss :  0.07795453816652298\n",
      "step :  308 loss :  0.07795228064060211\n",
      "step :  309 loss :  0.07795903831720352\n",
      "step :  310 loss :  0.07795225083827972\n",
      "step :  311 loss :  0.07795876264572144\n",
      "step :  312 loss :  0.07795418798923492\n",
      "step :  313 loss :  0.07795156538486481\n",
      "step :  314 loss :  0.07795275747776031\n",
      "step :  315 loss :  0.07795146107673645\n",
      "step :  316 loss :  0.07795140892267227\n",
      "step :  317 loss :  0.0779542326927185\n",
      "step :  318 loss :  0.07795149832963943\n",
      "step :  319 loss :  0.0779595598578453\n",
      "step :  320 loss :  0.07795244455337524\n",
      "step :  321 loss :  0.07795032113790512\n",
      "step :  322 loss :  0.07795551419258118\n",
      "step :  323 loss :  0.07794960588216782\n",
      "step :  324 loss :  0.07795167714357376\n",
      "step :  325 loss :  0.0779513269662857\n",
      "step :  326 loss :  0.07794980704784393\n",
      "step :  327 loss :  0.07795176655054092\n",
      "step :  328 loss :  0.07794979214668274\n",
      "step :  329 loss :  0.07795625925064087\n",
      "step :  330 loss :  0.07794924080371857\n",
      "step :  331 loss :  0.07794927805662155\n",
      "step :  332 loss :  0.07795090973377228\n",
      "step :  333 loss :  0.07794874161481857\n",
      "step :  334 loss :  0.07794894278049469\n",
      "step :  335 loss :  0.07795026898384094\n",
      "step :  336 loss :  0.07794859260320663\n",
      "step :  337 loss :  0.07795016467571259\n",
      "step :  338 loss :  0.07794797420501709\n",
      "step :  339 loss :  0.0779496431350708\n",
      "step :  340 loss :  0.07794798910617828\n",
      "step :  341 loss :  0.07794897258281708\n",
      "step :  342 loss :  0.07794899493455887\n",
      "step :  343 loss :  0.0779496431350708\n",
      "step :  344 loss :  0.07794679701328278\n",
      "step :  345 loss :  0.07794934511184692\n",
      "step :  346 loss :  0.07794704288244247\n",
      "step :  347 loss :  0.07794835418462753\n",
      "step :  348 loss :  0.07794693112373352\n",
      "step :  349 loss :  0.07794737815856934\n",
      "step :  350 loss :  0.07794656604528427\n",
      "step :  351 loss :  0.07794889062643051\n",
      "step :  352 loss :  0.07794813811779022\n",
      "step :  353 loss :  0.07794663310050964\n",
      "step :  354 loss :  0.07794628292322159\n",
      "step :  355 loss :  0.07794571667909622\n",
      "step :  356 loss :  0.07794586569070816\n",
      "step :  357 loss :  0.07794579863548279\n",
      "step :  358 loss :  0.07794811576604843\n",
      "step :  359 loss :  0.07794585078954697\n",
      "step :  360 loss :  0.07794801890850067\n",
      "step :  361 loss :  0.07794596254825592\n",
      "step :  362 loss :  0.07794714719057083\n",
      "step :  363 loss :  0.07794549316167831\n",
      "step :  364 loss :  0.0779460147023201\n",
      "step :  365 loss :  0.07794555276632309\n",
      "step :  366 loss :  0.07794693857431412\n",
      "step :  367 loss :  0.07794513553380966\n",
      "step :  368 loss :  0.07794599235057831\n",
      "step :  369 loss :  0.07794548571109772\n",
      "step :  370 loss :  0.07794473320245743\n",
      "step :  371 loss :  0.0779448077082634\n",
      "step :  372 loss :  0.07794658839702606\n",
      "step :  373 loss :  0.07794526219367981\n",
      "step :  374 loss :  0.077944315969944\n",
      "step :  375 loss :  0.07794652879238129\n",
      "step :  376 loss :  0.0779445692896843\n",
      "step :  377 loss :  0.07794482260942459\n",
      "step :  378 loss :  0.07794485241174698\n",
      "step :  379 loss :  0.07794521003961563\n",
      "step :  380 loss :  0.07794473320245743\n",
      "step :  381 loss :  0.07794494181871414\n",
      "step :  382 loss :  0.07794401794672012\n",
      "step :  383 loss :  0.07794410735368729\n",
      "step :  384 loss :  0.07794473320245743\n",
      "step :  385 loss :  0.07794398069381714\n",
      "step :  386 loss :  0.0779450461268425\n",
      "step :  387 loss :  0.07794433832168579\n",
      "step :  388 loss :  0.07794391363859177\n",
      "step :  389 loss :  0.07794467359781265\n",
      "step :  390 loss :  0.07794371247291565\n",
      "step :  391 loss :  0.07794387638568878\n",
      "step :  392 loss :  0.07794357091188431\n",
      "step :  393 loss :  0.07794354110956192\n",
      "step :  394 loss :  0.07794444262981415\n",
      "step :  395 loss :  0.07794368267059326\n",
      "step :  396 loss :  0.07794398814439774\n",
      "step :  397 loss :  0.07794343680143356\n",
      "step :  398 loss :  0.0779443010687828\n",
      "step :  399 loss :  0.07794339954853058\n",
      "step :  400 loss :  0.07794325798749924\n",
      "step :  401 loss :  0.07794298231601715\n",
      "step :  402 loss :  0.0779438316822052\n",
      "step :  403 loss :  0.07794300466775894\n",
      "step :  404 loss :  0.07794442772865295\n",
      "step :  405 loss :  0.07794368267059326\n",
      "step :  406 loss :  0.07794298976659775\n",
      "step :  407 loss :  0.07794387638568878\n",
      "step :  408 loss :  0.07794282585382462\n",
      "step :  409 loss :  0.07794294506311417\n",
      "step :  410 loss :  0.07794363051652908\n",
      "step :  411 loss :  0.0779428780078888\n",
      "step :  412 loss :  0.07794294506311417\n",
      "step :  413 loss :  0.07794272899627686\n",
      "step :  414 loss :  0.07794256508350372\n",
      "step :  415 loss :  0.07794322073459625\n",
      "step :  416 loss :  0.07794273644685745\n",
      "step :  417 loss :  0.07794288545846939\n",
      "step :  418 loss :  0.07794257998466492\n",
      "step :  419 loss :  0.0779423788189888\n",
      "step :  420 loss :  0.0779435932636261\n",
      "step :  421 loss :  0.07794258743524551\n",
      "step :  422 loss :  0.0779423639178276\n",
      "step :  423 loss :  0.07794339209794998\n",
      "step :  424 loss :  0.07794250547885895\n",
      "step :  425 loss :  0.07794244587421417\n",
      "step :  426 loss :  0.07794240117073059\n",
      "step :  427 loss :  0.07794234901666641\n",
      "step :  428 loss :  0.07794227451086044\n",
      "step :  429 loss :  0.07794252783060074\n",
      "step :  430 loss :  0.07794220000505447\n",
      "step :  431 loss :  0.07794241607189178\n",
      "step :  432 loss :  0.07794231176376343\n",
      "step :  433 loss :  0.0779421404004097\n",
      "step :  434 loss :  0.0779421254992485\n",
      "step :  435 loss :  0.07794291526079178\n",
      "step :  436 loss :  0.07794192433357239\n",
      "step :  437 loss :  0.07794204354286194\n",
      "step :  438 loss :  0.07794314622879028\n",
      "step :  439 loss :  0.07794249057769775\n",
      "step :  440 loss :  0.07794185727834702\n",
      "step :  441 loss :  0.07794226706027985\n",
      "step :  442 loss :  0.07794192433357239\n",
      "step :  443 loss :  0.07794187217950821\n",
      "step :  444 loss :  0.07794199883937836\n",
      "step :  445 loss :  0.07794172316789627\n",
      "step :  446 loss :  0.07794231176376343\n",
      "step :  447 loss :  0.0779418870806694\n",
      "step :  448 loss :  0.07794182747602463\n",
      "step :  449 loss :  0.07794223725795746\n",
      "step :  450 loss :  0.07794186472892761\n",
      "step :  451 loss :  0.0779416561126709\n",
      "step :  452 loss :  0.07794173806905746\n",
      "step :  453 loss :  0.07794167846441269\n",
      "step :  454 loss :  0.07794179022312164\n",
      "step :  455 loss :  0.07794174551963806\n",
      "step :  456 loss :  0.07794161140918732\n",
      "step :  457 loss :  0.07794222980737686\n",
      "step :  458 loss :  0.0779418870806694\n",
      "step :  459 loss :  0.07794170826673508\n",
      "step :  460 loss :  0.07794186472892761\n",
      "step :  461 loss :  0.07794181257486343\n",
      "step :  462 loss :  0.07794160395860672\n",
      "step :  463 loss :  0.07794158905744553\n",
      "step :  464 loss :  0.07794155180454254\n",
      "step :  465 loss :  0.07794151455163956\n",
      "step :  466 loss :  0.0779418870806694\n",
      "step :  467 loss :  0.07794158905744553\n",
      "step :  468 loss :  0.07794152200222015\n",
      "step :  469 loss :  0.07794163376092911\n",
      "step :  470 loss :  0.07794144749641418\n",
      "step :  471 loss :  0.07794153690338135\n",
      "step :  472 loss :  0.07794170826673508\n",
      "step :  473 loss :  0.0779416486620903\n",
      "step :  474 loss :  0.07794144749641418\n",
      "step :  475 loss :  0.07794151455163956\n",
      "step :  476 loss :  0.07794148474931717\n",
      "step :  477 loss :  0.07794167101383209\n",
      "step :  478 loss :  0.0779414176940918\n",
      "step :  479 loss :  0.07794148474931717\n",
      "step :  480 loss :  0.07794139534235\n",
      "step :  481 loss :  0.07794139534235\n",
      "step :  482 loss :  0.07794163376092911\n",
      "step :  483 loss :  0.07794146239757538\n",
      "step :  484 loss :  0.0779414102435112\n",
      "step :  485 loss :  0.07794176042079926\n",
      "step :  486 loss :  0.07794151455163956\n",
      "step :  487 loss :  0.07794135063886642\n",
      "step :  488 loss :  0.07794163376092911\n",
      "step :  489 loss :  0.0779414251446724\n",
      "step :  490 loss :  0.07794139534235\n",
      "step :  491 loss :  0.0779414102435112\n",
      "step :  492 loss :  0.07794135063886642\n",
      "step :  493 loss :  0.07794133573770523\n",
      "step :  494 loss :  0.07794137299060822\n",
      "step :  495 loss :  0.07794128358364105\n",
      "step :  496 loss :  0.07794148474931717\n",
      "step :  497 loss :  0.07794133573770523\n",
      "step :  498 loss :  0.07794124633073807\n",
      "step :  499 loss :  0.07794155180454254\n",
      "Training model 1\n",
      "step :  0 loss :  0.38382697105407715\n",
      "step :  1 loss :  0.22180180251598358\n",
      "step :  2 loss :  0.20937088131904602\n",
      "step :  3 loss :  0.24858702719211578\n",
      "step :  4 loss :  0.22508718073368073\n",
      "step :  5 loss :  0.19491231441497803\n",
      "step :  6 loss :  0.225652277469635\n",
      "step :  7 loss :  0.2230907827615738\n",
      "step :  8 loss :  0.2100776582956314\n",
      "step :  9 loss :  0.1894504874944687\n",
      "step :  10 loss :  0.19439247250556946\n",
      "step :  11 loss :  0.17398138344287872\n",
      "step :  12 loss :  0.1651727259159088\n",
      "step :  13 loss :  0.15558677911758423\n",
      "step :  14 loss :  0.1525605171918869\n",
      "step :  15 loss :  0.13091689348220825\n",
      "step :  16 loss :  0.14515772461891174\n",
      "step :  17 loss :  0.1617920845746994\n",
      "step :  18 loss :  0.16911837458610535\n",
      "step :  19 loss :  0.1228281632065773\n",
      "step :  20 loss :  0.13273920118808746\n",
      "step :  21 loss :  0.11952030658721924\n",
      "step :  22 loss :  0.11360422521829605\n",
      "step :  23 loss :  0.1243322342634201\n",
      "step :  24 loss :  0.11516769230365753\n",
      "step :  25 loss :  0.10570823401212692\n",
      "step :  26 loss :  0.11595337092876434\n",
      "step :  27 loss :  0.12569023668766022\n",
      "step :  28 loss :  0.12478739023208618\n",
      "step :  29 loss :  0.10709214955568314\n",
      "step :  30 loss :  0.10948823392391205\n",
      "step :  31 loss :  0.09904330968856812\n",
      "step :  32 loss :  0.12096475809812546\n",
      "step :  33 loss :  0.10801642388105392\n",
      "step :  34 loss :  0.09653057903051376\n",
      "step :  35 loss :  0.09401939064264297\n",
      "step :  36 loss :  0.10677976161241531\n",
      "step :  37 loss :  0.09074758738279343\n",
      "step :  38 loss :  0.11515045166015625\n",
      "step :  39 loss :  0.09213092923164368\n",
      "step :  40 loss :  0.11654394119977951\n",
      "step :  41 loss :  0.09506084769964218\n",
      "step :  42 loss :  0.09598467499017715\n",
      "step :  43 loss :  0.09085424244403839\n",
      "step :  44 loss :  0.09115978330373764\n",
      "step :  45 loss :  0.10404422134160995\n",
      "step :  46 loss :  0.09706201404333115\n",
      "step :  47 loss :  0.0874636098742485\n",
      "step :  48 loss :  0.08946043998003006\n",
      "step :  49 loss :  0.09218651801347733\n",
      "step :  50 loss :  0.09525805711746216\n",
      "step :  51 loss :  0.091099813580513\n",
      "step :  52 loss :  0.08589990437030792\n",
      "step :  53 loss :  0.09338990598917007\n",
      "step :  54 loss :  0.09144780784845352\n",
      "step :  55 loss :  0.08620958775281906\n",
      "step :  56 loss :  0.08404611796140671\n",
      "step :  57 loss :  0.08492486923933029\n",
      "step :  58 loss :  0.08588409423828125\n",
      "step :  59 loss :  0.0830826386809349\n",
      "step :  60 loss :  0.08988023549318314\n",
      "step :  61 loss :  0.08386600762605667\n",
      "step :  62 loss :  0.10022930800914764\n",
      "step :  63 loss :  0.0855918601155281\n",
      "step :  64 loss :  0.08697281777858734\n",
      "step :  65 loss :  0.08494193106889725\n",
      "step :  66 loss :  0.0839255079627037\n",
      "step :  67 loss :  0.09062740951776505\n",
      "step :  68 loss :  0.0873037576675415\n",
      "step :  69 loss :  0.08303502947092056\n",
      "step :  70 loss :  0.08425205945968628\n",
      "step :  71 loss :  0.08406417816877365\n",
      "step :  72 loss :  0.08660510927438736\n",
      "step :  73 loss :  0.0830783024430275\n",
      "step :  74 loss :  0.08359583467245102\n",
      "step :  75 loss :  0.08923763036727905\n",
      "step :  76 loss :  0.08546973019838333\n",
      "step :  77 loss :  0.08640888333320618\n",
      "step :  78 loss :  0.08688879758119583\n",
      "step :  79 loss :  0.08246030658483505\n",
      "step :  80 loss :  0.08541777729988098\n",
      "step :  81 loss :  0.08641403913497925\n",
      "step :  82 loss :  0.08365540206432343\n",
      "step :  83 loss :  0.08274023234844208\n",
      "step :  84 loss :  0.08169319480657578\n",
      "step :  85 loss :  0.08625806868076324\n",
      "step :  86 loss :  0.08418848365545273\n",
      "step :  87 loss :  0.0819440558552742\n",
      "step :  88 loss :  0.08298385143280029\n",
      "step :  89 loss :  0.083518385887146\n",
      "step :  90 loss :  0.08662956953048706\n",
      "step :  91 loss :  0.08572423458099365\n",
      "step :  92 loss :  0.08464773744344711\n",
      "step :  93 loss :  0.08493606746196747\n",
      "step :  94 loss :  0.08721727132797241\n",
      "step :  95 loss :  0.0850684866309166\n",
      "step :  96 loss :  0.08677259832620621\n",
      "step :  97 loss :  0.08433795720338821\n",
      "step :  98 loss :  0.08398833870887756\n",
      "step :  99 loss :  0.08313237875699997\n",
      "step :  100 loss :  0.0819387137889862\n",
      "step :  101 loss :  0.08183395117521286\n",
      "step :  102 loss :  0.08141334354877472\n",
      "step :  103 loss :  0.08131906390190125\n",
      "step :  104 loss :  0.08112899214029312\n",
      "step :  105 loss :  0.08117115497589111\n",
      "step :  106 loss :  0.081077940762043\n",
      "step :  107 loss :  0.08085352182388306\n",
      "step :  108 loss :  0.08115437626838684\n",
      "step :  109 loss :  0.08073020726442337\n",
      "step :  110 loss :  0.08094768226146698\n",
      "step :  111 loss :  0.08067331463098526\n",
      "step :  112 loss :  0.08097227662801743\n",
      "step :  113 loss :  0.0805836096405983\n",
      "step :  114 loss :  0.08049197494983673\n",
      "step :  115 loss :  0.08059735596179962\n",
      "step :  116 loss :  0.08050244301557541\n",
      "step :  117 loss :  0.08079401403665543\n",
      "step :  118 loss :  0.080600805580616\n",
      "step :  119 loss :  0.08052646368741989\n",
      "step :  120 loss :  0.0802350714802742\n",
      "step :  121 loss :  0.08033958077430725\n",
      "step :  122 loss :  0.08015929162502289\n",
      "step :  123 loss :  0.08033204823732376\n",
      "step :  124 loss :  0.08017610758543015\n",
      "step :  125 loss :  0.08044097572565079\n",
      "step :  126 loss :  0.08013841509819031\n",
      "step :  127 loss :  0.0803079605102539\n",
      "step :  128 loss :  0.0800599753856659\n",
      "step :  129 loss :  0.07998374104499817\n",
      "step :  130 loss :  0.08019756525754929\n",
      "step :  131 loss :  0.08005104959011078\n",
      "step :  132 loss :  0.08077771961688995\n",
      "step :  133 loss :  0.08019395172595978\n",
      "step :  134 loss :  0.0800822377204895\n",
      "step :  135 loss :  0.07994585484266281\n",
      "step :  136 loss :  0.0798662006855011\n",
      "step :  137 loss :  0.0798155665397644\n",
      "step :  138 loss :  0.07987528294324875\n",
      "step :  139 loss :  0.07985716313123703\n",
      "step :  140 loss :  0.07981088757514954\n",
      "step :  141 loss :  0.07980474084615707\n",
      "step :  142 loss :  0.07987839728593826\n",
      "step :  143 loss :  0.07995817065238953\n",
      "step :  144 loss :  0.07965877652168274\n",
      "step :  145 loss :  0.07981466501951218\n",
      "step :  146 loss :  0.07972713559865952\n",
      "step :  147 loss :  0.0797542855143547\n",
      "step :  148 loss :  0.07974559813737869\n",
      "step :  149 loss :  0.07978178560733795\n",
      "step :  150 loss :  0.07988402247428894\n",
      "step :  151 loss :  0.0797635093331337\n",
      "step :  152 loss :  0.0797131210565567\n",
      "step :  153 loss :  0.07962024211883545\n",
      "step :  154 loss :  0.07981326431035995\n",
      "step :  155 loss :  0.07966429740190506\n",
      "step :  156 loss :  0.07991992682218552\n",
      "step :  157 loss :  0.07965781539678574\n",
      "step :  158 loss :  0.07970335334539413\n",
      "step :  159 loss :  0.07958503812551498\n",
      "step :  160 loss :  0.0797758474946022\n",
      "step :  161 loss :  0.07952021807432175\n",
      "step :  162 loss :  0.07964283227920532\n",
      "step :  163 loss :  0.0795770138502121\n",
      "step :  164 loss :  0.07953280210494995\n",
      "step :  165 loss :  0.07962075620889664\n",
      "step :  166 loss :  0.07963979244232178\n",
      "step :  167 loss :  0.0795542523264885\n",
      "step :  168 loss :  0.07974805682897568\n",
      "step :  169 loss :  0.07944074273109436\n",
      "step :  170 loss :  0.079619862139225\n",
      "step :  171 loss :  0.0794229581952095\n",
      "step :  172 loss :  0.07957489043474197\n",
      "step :  173 loss :  0.07980137318372726\n",
      "step :  174 loss :  0.07946102321147919\n",
      "step :  175 loss :  0.07974612712860107\n",
      "step :  176 loss :  0.07947451621294022\n",
      "step :  177 loss :  0.0794641524553299\n",
      "step :  178 loss :  0.07949279248714447\n",
      "step :  179 loss :  0.07937856018543243\n",
      "step :  180 loss :  0.07960876822471619\n",
      "step :  181 loss :  0.07948377728462219\n",
      "step :  182 loss :  0.07941040396690369\n",
      "step :  183 loss :  0.07957080751657486\n",
      "step :  184 loss :  0.07938045263290405\n",
      "step :  185 loss :  0.07935057580471039\n",
      "step :  186 loss :  0.07938522100448608\n",
      "step :  187 loss :  0.07934356480836868\n",
      "step :  188 loss :  0.07942090183496475\n",
      "step :  189 loss :  0.07933289557695389\n",
      "step :  190 loss :  0.07935681939125061\n",
      "step :  191 loss :  0.07933904975652695\n",
      "step :  192 loss :  0.0793619453907013\n",
      "step :  193 loss :  0.07936877757310867\n",
      "step :  194 loss :  0.07938171923160553\n",
      "step :  195 loss :  0.07936238497495651\n",
      "step :  196 loss :  0.07937537133693695\n",
      "step :  197 loss :  0.079328253865242\n",
      "step :  198 loss :  0.07943350821733475\n",
      "step :  199 loss :  0.07935025542974472\n",
      "step :  200 loss :  0.07933933287858963\n",
      "step :  201 loss :  0.07939158380031586\n",
      "step :  202 loss :  0.07928888499736786\n",
      "step :  203 loss :  0.07930301129817963\n",
      "step :  204 loss :  0.07930591702461243\n",
      "step :  205 loss :  0.07930024713277817\n",
      "step :  206 loss :  0.07930587977170944\n",
      "step :  207 loss :  0.07929860055446625\n",
      "step :  208 loss :  0.07927574217319489\n",
      "step :  209 loss :  0.07926677912473679\n",
      "step :  210 loss :  0.07931818813085556\n",
      "step :  211 loss :  0.07929255068302155\n",
      "step :  212 loss :  0.07927623391151428\n",
      "step :  213 loss :  0.0792846754193306\n",
      "step :  214 loss :  0.07932405173778534\n",
      "step :  215 loss :  0.07925788313150406\n",
      "step :  216 loss :  0.07925016433000565\n",
      "step :  217 loss :  0.07928202301263809\n",
      "step :  218 loss :  0.07927098125219345\n",
      "step :  219 loss :  0.07927782088518143\n",
      "step :  220 loss :  0.079270139336586\n",
      "step :  221 loss :  0.07924847304821014\n",
      "step :  222 loss :  0.07924047857522964\n",
      "step :  223 loss :  0.07925857603549957\n",
      "step :  224 loss :  0.07925066351890564\n",
      "step :  225 loss :  0.07923107594251633\n",
      "step :  226 loss :  0.07924345135688782\n",
      "step :  227 loss :  0.07923442125320435\n",
      "step :  228 loss :  0.07923448085784912\n",
      "step :  229 loss :  0.07922355830669403\n",
      "step :  230 loss :  0.07925142347812653\n",
      "step :  231 loss :  0.07922539860010147\n",
      "step :  232 loss :  0.07924218475818634\n",
      "step :  233 loss :  0.07924212515354156\n",
      "step :  234 loss :  0.0792442336678505\n",
      "step :  235 loss :  0.0792144238948822\n",
      "step :  236 loss :  0.07922814041376114\n",
      "step :  237 loss :  0.07925374060869217\n",
      "step :  238 loss :  0.07919473946094513\n",
      "step :  239 loss :  0.07921711355447769\n",
      "step :  240 loss :  0.07923910766839981\n",
      "step :  241 loss :  0.07918477803468704\n",
      "step :  242 loss :  0.07919073849916458\n",
      "step :  243 loss :  0.07921834290027618\n",
      "step :  244 loss :  0.07919156551361084\n",
      "step :  245 loss :  0.0791863352060318\n",
      "step :  246 loss :  0.07921987771987915\n",
      "step :  247 loss :  0.07918845862150192\n",
      "step :  248 loss :  0.07918702811002731\n",
      "step :  249 loss :  0.0792079046368599\n",
      "step :  250 loss :  0.07917783409357071\n",
      "step :  251 loss :  0.07918766140937805\n",
      "step :  252 loss :  0.07919815182685852\n",
      "step :  253 loss :  0.07917787879705429\n",
      "step :  254 loss :  0.07917831093072891\n",
      "step :  255 loss :  0.07917484641075134\n",
      "step :  256 loss :  0.07918708771467209\n",
      "step :  257 loss :  0.07919300347566605\n",
      "step :  258 loss :  0.07917610555887222\n",
      "step :  259 loss :  0.07918375730514526\n",
      "step :  260 loss :  0.07916974276304245\n",
      "step :  261 loss :  0.0791766569018364\n",
      "step :  262 loss :  0.079173743724823\n",
      "step :  263 loss :  0.07917270809412003\n",
      "step :  264 loss :  0.0791652724146843\n",
      "step :  265 loss :  0.07918872684240341\n",
      "step :  266 loss :  0.07917241752147675\n",
      "step :  267 loss :  0.07916375994682312\n",
      "step :  268 loss :  0.0791841596364975\n",
      "step :  269 loss :  0.0791662409901619\n",
      "step :  270 loss :  0.07915879040956497\n",
      "step :  271 loss :  0.07916354387998581\n",
      "step :  272 loss :  0.07916029542684555\n",
      "step :  273 loss :  0.07916722446680069\n",
      "step :  274 loss :  0.07916919142007828\n",
      "step :  275 loss :  0.07916854321956635\n",
      "step :  276 loss :  0.07916174083948135\n",
      "step :  277 loss :  0.0791652724146843\n",
      "step :  278 loss :  0.07915326952934265\n",
      "step :  279 loss :  0.0791688933968544\n",
      "step :  280 loss :  0.07916273176670074\n",
      "step :  281 loss :  0.07916072756052017\n",
      "step :  282 loss :  0.07917143404483795\n",
      "step :  283 loss :  0.07915987074375153\n",
      "step :  284 loss :  0.07915236055850983\n",
      "step :  285 loss :  0.07915910333395004\n",
      "step :  286 loss :  0.0791565403342247\n",
      "step :  287 loss :  0.07915625721216202\n",
      "step :  288 loss :  0.07917055487632751\n",
      "step :  289 loss :  0.07916150987148285\n",
      "step :  290 loss :  0.0791531428694725\n",
      "step :  291 loss :  0.07917014509439468\n",
      "step :  292 loss :  0.0791669487953186\n",
      "step :  293 loss :  0.07915398478507996\n",
      "step :  294 loss :  0.07917087525129318\n",
      "step :  295 loss :  0.07916755974292755\n",
      "step :  296 loss :  0.07915251702070236\n",
      "step :  297 loss :  0.07915088534355164\n",
      "step :  298 loss :  0.07915648072957993\n",
      "step :  299 loss :  0.07915503531694412\n",
      "step :  300 loss :  0.07914935052394867\n",
      "step :  301 loss :  0.07914623618125916\n",
      "step :  302 loss :  0.07915430516004562\n",
      "step :  303 loss :  0.07914844155311584\n",
      "step :  304 loss :  0.07914536446332932\n",
      "step :  305 loss :  0.07914577424526215\n",
      "step :  306 loss :  0.07914812117815018\n",
      "step :  307 loss :  0.07914997637271881\n",
      "step :  308 loss :  0.07914838939905167\n",
      "step :  309 loss :  0.0791533961892128\n",
      "step :  310 loss :  0.0791429728269577\n",
      "step :  311 loss :  0.07915353029966354\n",
      "step :  312 loss :  0.0791429877281189\n",
      "step :  313 loss :  0.07915672659873962\n",
      "step :  314 loss :  0.07915492355823517\n",
      "step :  315 loss :  0.07914245873689651\n",
      "step :  316 loss :  0.07915400713682175\n",
      "step :  317 loss :  0.07914447784423828\n",
      "step :  318 loss :  0.07914409786462784\n",
      "step :  319 loss :  0.07914742082357407\n",
      "step :  320 loss :  0.07914731651544571\n",
      "step :  321 loss :  0.07914318889379501\n",
      "step :  322 loss :  0.07915059477090836\n",
      "step :  323 loss :  0.07914193719625473\n",
      "step :  324 loss :  0.07913966476917267\n",
      "step :  325 loss :  0.079148069024086\n",
      "step :  326 loss :  0.07914503663778305\n",
      "step :  327 loss :  0.07913839817047119\n",
      "step :  328 loss :  0.07914569228887558\n",
      "step :  329 loss :  0.07914642244577408\n",
      "step :  330 loss :  0.07913734763860703\n",
      "step :  331 loss :  0.07913901656866074\n",
      "step :  332 loss :  0.07914835959672928\n",
      "step :  333 loss :  0.079140305519104\n",
      "step :  334 loss :  0.07913821190595627\n",
      "step :  335 loss :  0.07914426922798157\n",
      "step :  336 loss :  0.07914385944604874\n",
      "step :  337 loss :  0.07913742959499359\n",
      "step :  338 loss :  0.07913953810930252\n",
      "step :  339 loss :  0.07913991808891296\n",
      "step :  340 loss :  0.07914315164089203\n",
      "step :  341 loss :  0.07913830131292343\n",
      "step :  342 loss :  0.0791395902633667\n",
      "step :  343 loss :  0.07914265245199203\n",
      "step :  344 loss :  0.07914024591445923\n",
      "step :  345 loss :  0.07913931459188461\n",
      "step :  346 loss :  0.07913815975189209\n",
      "step :  347 loss :  0.0791364535689354\n",
      "step :  348 loss :  0.07913713157176971\n",
      "step :  349 loss :  0.07913699001073837\n",
      "step :  350 loss :  0.0791366770863533\n",
      "step :  351 loss :  0.07913763076066971\n",
      "step :  352 loss :  0.07913648337125778\n",
      "step :  353 loss :  0.07913471758365631\n",
      "step :  354 loss :  0.07913623005151749\n",
      "step :  355 loss :  0.0791337788105011\n",
      "step :  356 loss :  0.07913818210363388\n",
      "step :  357 loss :  0.07913438230752945\n",
      "step :  358 loss :  0.07913562655448914\n",
      "step :  359 loss :  0.07913745939731598\n",
      "step :  360 loss :  0.07913428544998169\n",
      "step :  361 loss :  0.07914018630981445\n",
      "step :  362 loss :  0.07913755625486374\n",
      "step :  363 loss :  0.07913340628147125\n",
      "step :  364 loss :  0.07913582026958466\n",
      "step :  365 loss :  0.07913538068532944\n",
      "step :  366 loss :  0.07913319021463394\n",
      "step :  367 loss :  0.0791369080543518\n",
      "step :  368 loss :  0.0791347548365593\n",
      "step :  369 loss :  0.07913295179605484\n",
      "step :  370 loss :  0.07913526147603989\n",
      "step :  371 loss :  0.0791328102350235\n",
      "step :  372 loss :  0.07913342863321304\n",
      "step :  373 loss :  0.07913646847009659\n",
      "step :  374 loss :  0.07913459837436676\n",
      "step :  375 loss :  0.0791330635547638\n",
      "step :  376 loss :  0.07913340628147125\n",
      "step :  377 loss :  0.07913374155759811\n",
      "step :  378 loss :  0.07913277298212051\n",
      "step :  379 loss :  0.07913464307785034\n",
      "step :  380 loss :  0.0791320875287056\n",
      "step :  381 loss :  0.07913262397050858\n",
      "step :  382 loss :  0.07913488149642944\n",
      "step :  383 loss :  0.0791340321302414\n",
      "step :  384 loss :  0.07913302630186081\n",
      "step :  385 loss :  0.07913310825824738\n",
      "step :  386 loss :  0.07913197576999664\n",
      "step :  387 loss :  0.07913434505462646\n",
      "step :  388 loss :  0.07913325726985931\n",
      "step :  389 loss :  0.07913173735141754\n",
      "step :  390 loss :  0.07913452386856079\n",
      "step :  391 loss :  0.07913272827863693\n",
      "step :  392 loss :  0.07913146167993546\n",
      "step :  393 loss :  0.07913331687450409\n",
      "step :  394 loss :  0.07913324236869812\n",
      "step :  395 loss :  0.07913126051425934\n",
      "step :  396 loss :  0.07913289964199066\n",
      "step :  397 loss :  0.07913193106651306\n",
      "step :  398 loss :  0.07913262397050858\n",
      "step :  399 loss :  0.07913115620613098\n",
      "step :  400 loss :  0.079132080078125\n",
      "step :  401 loss :  0.07913286238908768\n",
      "step :  402 loss :  0.07913102209568024\n",
      "step :  403 loss :  0.07913326472043991\n",
      "step :  404 loss :  0.07913272827863693\n",
      "step :  405 loss :  0.0791308656334877\n",
      "step :  406 loss :  0.07913221418857574\n",
      "step :  407 loss :  0.0791315957903862\n",
      "step :  408 loss :  0.07913102209568024\n",
      "step :  409 loss :  0.07913178950548172\n",
      "step :  410 loss :  0.07913175970315933\n",
      "step :  411 loss :  0.07913076132535934\n",
      "step :  412 loss :  0.07913083583116531\n",
      "step :  413 loss :  0.07913114875555038\n",
      "step :  414 loss :  0.07913035899400711\n",
      "step :  415 loss :  0.07913133502006531\n",
      "step :  416 loss :  0.07913105934858322\n",
      "step :  417 loss :  0.079130619764328\n",
      "step :  418 loss :  0.07913143187761307\n",
      "step :  419 loss :  0.07913035154342651\n",
      "step :  420 loss :  0.07913046330213547\n",
      "step :  421 loss :  0.07913168519735336\n",
      "step :  422 loss :  0.07913040369749069\n",
      "step :  423 loss :  0.07913020253181458\n",
      "step :  424 loss :  0.0791313424706459\n",
      "step :  425 loss :  0.07913056761026382\n",
      "step :  426 loss :  0.07913021743297577\n",
      "step :  427 loss :  0.07913027703762054\n",
      "step :  428 loss :  0.07913020253181458\n",
      "step :  429 loss :  0.0791301429271698\n",
      "step :  430 loss :  0.07912980765104294\n",
      "step :  431 loss :  0.07913108170032501\n",
      "step :  432 loss :  0.07913113385438919\n",
      "step :  433 loss :  0.07912958413362503\n",
      "step :  434 loss :  0.07913058251142502\n",
      "step :  435 loss :  0.07913025468587875\n",
      "step :  436 loss :  0.07912974059581757\n",
      "step :  437 loss :  0.07913001626729965\n",
      "step :  438 loss :  0.07912996411323547\n",
      "step :  439 loss :  0.07912944257259369\n",
      "step :  440 loss :  0.07913025468587875\n",
      "step :  441 loss :  0.07913007587194443\n",
      "step :  442 loss :  0.07912939041852951\n",
      "step :  443 loss :  0.07913058251142502\n",
      "step :  444 loss :  0.07913007587194443\n",
      "step :  445 loss :  0.079129159450531\n",
      "step :  446 loss :  0.07912950962781906\n",
      "step :  447 loss :  0.07913051545619965\n",
      "step :  448 loss :  0.0791298896074295\n",
      "step :  449 loss :  0.07912921905517578\n",
      "step :  450 loss :  0.07912961393594742\n",
      "step :  451 loss :  0.07912947982549667\n",
      "step :  452 loss :  0.07912946492433548\n",
      "step :  453 loss :  0.07912950962781906\n",
      "step :  454 loss :  0.07912971824407578\n",
      "step :  455 loss :  0.07912948727607727\n",
      "step :  456 loss :  0.07912955433130264\n",
      "step :  457 loss :  0.07912950962781906\n",
      "step :  458 loss :  0.07912947237491608\n",
      "step :  459 loss :  0.07912947982549667\n",
      "step :  460 loss :  0.07912929356098175\n",
      "step :  461 loss :  0.07912919670343399\n",
      "step :  462 loss :  0.07912961393594742\n",
      "step :  463 loss :  0.0791291743516922\n",
      "step :  464 loss :  0.0791291743516922\n",
      "step :  465 loss :  0.07912946492433548\n",
      "step :  466 loss :  0.07912924885749817\n",
      "step :  467 loss :  0.07912899553775787\n",
      "step :  468 loss :  0.07912922650575638\n",
      "step :  469 loss :  0.07912908494472504\n",
      "step :  470 loss :  0.07912896573543549\n",
      "step :  471 loss :  0.07912904769182205\n",
      "step :  472 loss :  0.07912902534008026\n",
      "step :  473 loss :  0.07912897318601608\n",
      "step :  474 loss :  0.0791286826133728\n",
      "step :  475 loss :  0.07912895828485489\n",
      "step :  476 loss :  0.0791291892528534\n",
      "step :  477 loss :  0.0791289210319519\n",
      "step :  478 loss :  0.07912871241569519\n",
      "step :  479 loss :  0.07912883907556534\n",
      "step :  480 loss :  0.07912898808717728\n",
      "step :  481 loss :  0.07912873476743698\n",
      "step :  482 loss :  0.07912877947092056\n",
      "step :  483 loss :  0.07912880927324295\n",
      "step :  484 loss :  0.07912886142730713\n",
      "step :  485 loss :  0.07912878692150116\n",
      "step :  486 loss :  0.079128697514534\n",
      "step :  487 loss :  0.07912876456975937\n",
      "step :  488 loss :  0.07912895083427429\n",
      "step :  489 loss :  0.07912880182266235\n",
      "step :  490 loss :  0.07912859320640564\n",
      "step :  491 loss :  0.07912860065698624\n",
      "step :  492 loss :  0.0791289210319519\n",
      "step :  493 loss :  0.07912871241569519\n",
      "step :  494 loss :  0.07912852615118027\n",
      "step :  495 loss :  0.07912852615118027\n",
      "step :  496 loss :  0.07912887632846832\n",
      "step :  497 loss :  0.07912877947092056\n",
      "step :  498 loss :  0.0791284516453743\n",
      "step :  499 loss :  0.07912871241569519\n",
      "Training model 2\n",
      "step :  0 loss :  0.5074820518493652\n",
      "step :  1 loss :  0.20921066403388977\n",
      "step :  2 loss :  0.25271520018577576\n",
      "step :  3 loss :  0.25207778811454773\n",
      "step :  4 loss :  0.2441740185022354\n",
      "step :  5 loss :  0.18054868280887604\n",
      "step :  6 loss :  0.20518481731414795\n",
      "step :  7 loss :  0.22589436173439026\n",
      "step :  8 loss :  0.2124733030796051\n",
      "step :  9 loss :  0.1994982659816742\n",
      "step :  10 loss :  0.19884397089481354\n",
      "step :  11 loss :  0.1912315934896469\n",
      "step :  12 loss :  0.1729239523410797\n",
      "step :  13 loss :  0.1655278503894806\n",
      "step :  14 loss :  0.16131721436977386\n",
      "step :  15 loss :  0.15057815611362457\n",
      "step :  16 loss :  0.13736525177955627\n",
      "step :  17 loss :  0.14997094869613647\n",
      "step :  18 loss :  0.11582712829113007\n",
      "step :  19 loss :  0.30222654342651367\n",
      "step :  20 loss :  0.11042560636997223\n",
      "step :  21 loss :  0.13984230160713196\n",
      "step :  22 loss :  0.13027948141098022\n",
      "step :  23 loss :  0.1172432154417038\n",
      "step :  24 loss :  0.12205082923173904\n",
      "step :  25 loss :  0.10826331377029419\n",
      "step :  26 loss :  0.13834229111671448\n",
      "step :  27 loss :  0.10390647500753403\n",
      "step :  28 loss :  0.09874381124973297\n",
      "step :  29 loss :  0.10802537947893143\n",
      "step :  30 loss :  0.09457158297300339\n",
      "step :  31 loss :  0.159837007522583\n",
      "step :  32 loss :  0.10500309616327286\n",
      "step :  33 loss :  0.09077435731887817\n",
      "step :  34 loss :  0.10759840160608292\n",
      "step :  35 loss :  0.09182354062795639\n",
      "step :  36 loss :  0.12254088371992111\n",
      "step :  37 loss :  0.11073420941829681\n",
      "step :  38 loss :  0.10485557466745377\n",
      "step :  39 loss :  0.08819333463907242\n",
      "step :  40 loss :  0.09489043802022934\n",
      "step :  41 loss :  0.08988644927740097\n",
      "step :  42 loss :  0.09138903766870499\n",
      "step :  43 loss :  0.10554415732622147\n",
      "step :  44 loss :  0.1313532590866089\n",
      "step :  45 loss :  0.09353101998567581\n",
      "step :  46 loss :  0.10728635638952255\n",
      "step :  47 loss :  0.09138894826173782\n",
      "step :  48 loss :  0.08854863047599792\n",
      "step :  49 loss :  0.08427156507968903\n",
      "step :  50 loss :  0.08978750556707382\n",
      "step :  51 loss :  0.09875594824552536\n",
      "step :  52 loss :  0.09678786993026733\n",
      "step :  53 loss :  0.09468203783035278\n",
      "step :  54 loss :  0.09930481761693954\n",
      "step :  55 loss :  0.09116146713495255\n",
      "step :  56 loss :  0.08473450690507889\n",
      "step :  57 loss :  0.08584398031234741\n",
      "step :  58 loss :  0.08364507555961609\n",
      "step :  59 loss :  0.09196805208921432\n",
      "step :  60 loss :  0.08465412259101868\n",
      "step :  61 loss :  0.09541391581296921\n",
      "step :  62 loss :  0.10012807697057724\n",
      "step :  63 loss :  0.09644707292318344\n",
      "step :  64 loss :  0.09535850584506989\n",
      "step :  65 loss :  0.088578000664711\n",
      "step :  66 loss :  0.08779660612344742\n",
      "step :  67 loss :  0.08909712731838226\n",
      "step :  68 loss :  0.08455585688352585\n",
      "step :  69 loss :  0.08388505131006241\n",
      "step :  70 loss :  0.0861627608537674\n",
      "step :  71 loss :  0.08378461003303528\n",
      "step :  72 loss :  0.08503763377666473\n",
      "step :  73 loss :  0.08533565700054169\n",
      "step :  74 loss :  0.08276724070310593\n",
      "step :  75 loss :  0.08432454615831375\n",
      "step :  76 loss :  0.08314862102270126\n",
      "step :  77 loss :  0.08218104392290115\n",
      "step :  78 loss :  0.0857272818684578\n",
      "step :  79 loss :  0.08928896486759186\n",
      "step :  80 loss :  0.08832578361034393\n",
      "step :  81 loss :  0.08464702218770981\n",
      "step :  82 loss :  0.08261005580425262\n",
      "step :  83 loss :  0.08486886322498322\n",
      "step :  84 loss :  0.0860476940870285\n",
      "step :  85 loss :  0.08604548871517181\n",
      "step :  86 loss :  0.08264394849538803\n",
      "step :  87 loss :  0.08400276303291321\n",
      "step :  88 loss :  0.08243440091609955\n",
      "step :  89 loss :  0.08335927128791809\n",
      "step :  90 loss :  0.08395572751760483\n",
      "step :  91 loss :  0.08286764472723007\n",
      "step :  92 loss :  0.08277101814746857\n",
      "step :  93 loss :  0.08438338339328766\n",
      "step :  94 loss :  0.08340302854776382\n",
      "step :  95 loss :  0.08231449872255325\n",
      "step :  96 loss :  0.08162795007228851\n",
      "step :  97 loss :  0.08225904405117035\n",
      "step :  98 loss :  0.08302247524261475\n",
      "step :  99 loss :  0.08274345844984055\n",
      "step :  100 loss :  0.0818621814250946\n",
      "step :  101 loss :  0.08155637979507446\n",
      "step :  102 loss :  0.0820261612534523\n",
      "step :  103 loss :  0.0821371078491211\n",
      "step :  104 loss :  0.0812845453619957\n",
      "step :  105 loss :  0.0816785991191864\n",
      "step :  106 loss :  0.08186348527669907\n",
      "step :  107 loss :  0.08326555043458939\n",
      "step :  108 loss :  0.08225961774587631\n",
      "step :  109 loss :  0.08282121270895004\n",
      "step :  110 loss :  0.08167032897472382\n",
      "step :  111 loss :  0.08126188069581985\n",
      "step :  112 loss :  0.08110105991363525\n",
      "step :  113 loss :  0.08127199113368988\n",
      "step :  114 loss :  0.08209197968244553\n",
      "step :  115 loss :  0.08115708827972412\n",
      "step :  116 loss :  0.0817965716123581\n",
      "step :  117 loss :  0.08258742839097977\n",
      "step :  118 loss :  0.08195793628692627\n",
      "step :  119 loss :  0.08327721804380417\n",
      "step :  120 loss :  0.08268176764249802\n",
      "step :  121 loss :  0.08314697444438934\n",
      "step :  122 loss :  0.08121439814567566\n",
      "step :  123 loss :  0.08175220340490341\n",
      "step :  124 loss :  0.08179612457752228\n",
      "step :  125 loss :  0.08142977952957153\n",
      "step :  126 loss :  0.08106987178325653\n",
      "step :  127 loss :  0.08211242407560349\n",
      "step :  128 loss :  0.08144833147525787\n",
      "step :  129 loss :  0.08088724315166473\n",
      "step :  130 loss :  0.08098053187131882\n",
      "step :  131 loss :  0.08081385493278503\n",
      "step :  132 loss :  0.0808367133140564\n",
      "step :  133 loss :  0.08089197427034378\n",
      "step :  134 loss :  0.08096002042293549\n",
      "step :  135 loss :  0.08079449087381363\n",
      "step :  136 loss :  0.08080986142158508\n",
      "step :  137 loss :  0.0807887464761734\n",
      "step :  138 loss :  0.08078692108392715\n",
      "step :  139 loss :  0.08082103729248047\n",
      "step :  140 loss :  0.08073288202285767\n",
      "step :  141 loss :  0.08075600862503052\n",
      "step :  142 loss :  0.08059988915920258\n",
      "step :  143 loss :  0.0806402713060379\n",
      "step :  144 loss :  0.08090271800756454\n",
      "step :  145 loss :  0.08060232549905777\n",
      "step :  146 loss :  0.08075838536024094\n",
      "step :  147 loss :  0.08106081932783127\n",
      "step :  148 loss :  0.08056539297103882\n",
      "step :  149 loss :  0.08115749061107635\n",
      "step :  150 loss :  0.0805831030011177\n",
      "step :  151 loss :  0.08113649487495422\n",
      "step :  152 loss :  0.08066678792238235\n",
      "step :  153 loss :  0.08135237544775009\n",
      "step :  154 loss :  0.08058632910251617\n",
      "step :  155 loss :  0.0812145248055458\n",
      "step :  156 loss :  0.08050935715436935\n",
      "step :  157 loss :  0.08114832639694214\n",
      "step :  158 loss :  0.08051244169473648\n",
      "step :  159 loss :  0.08098661154508591\n",
      "step :  160 loss :  0.08051063865423203\n",
      "step :  161 loss :  0.0806274339556694\n",
      "step :  162 loss :  0.08050521463155746\n",
      "step :  163 loss :  0.0804779976606369\n",
      "step :  164 loss :  0.08045563101768494\n",
      "step :  165 loss :  0.0804467499256134\n",
      "step :  166 loss :  0.08040830492973328\n",
      "step :  167 loss :  0.08054747432470322\n",
      "step :  168 loss :  0.08039528131484985\n",
      "step :  169 loss :  0.08064666390419006\n",
      "step :  170 loss :  0.08038493990898132\n",
      "step :  171 loss :  0.08062787353992462\n",
      "step :  172 loss :  0.08041879534721375\n",
      "step :  173 loss :  0.08041767030954361\n",
      "step :  174 loss :  0.08047515153884888\n",
      "step :  175 loss :  0.08036909252405167\n",
      "step :  176 loss :  0.08055517077445984\n",
      "step :  177 loss :  0.08038391917943954\n",
      "step :  178 loss :  0.08044913411140442\n",
      "step :  179 loss :  0.08039838820695877\n",
      "step :  180 loss :  0.08041344583034515\n",
      "step :  181 loss :  0.08032149076461792\n",
      "step :  182 loss :  0.08034900575876236\n",
      "step :  183 loss :  0.08031732589006424\n",
      "step :  184 loss :  0.08029941469430923\n",
      "step :  185 loss :  0.08035852760076523\n",
      "step :  186 loss :  0.0803312286734581\n",
      "step :  187 loss :  0.08039066940546036\n",
      "step :  188 loss :  0.0803241953253746\n",
      "step :  189 loss :  0.08035768568515778\n",
      "step :  190 loss :  0.08032093197107315\n",
      "step :  191 loss :  0.08031481504440308\n",
      "step :  192 loss :  0.08030683547258377\n",
      "step :  193 loss :  0.08031680434942245\n",
      "step :  194 loss :  0.08032266050577164\n",
      "step :  195 loss :  0.08028054982423782\n",
      "step :  196 loss :  0.08040887862443924\n",
      "step :  197 loss :  0.08031008392572403\n",
      "step :  198 loss :  0.08034167438745499\n",
      "step :  199 loss :  0.08026976883411407\n",
      "step :  200 loss :  0.08037757873535156\n",
      "step :  201 loss :  0.08030587434768677\n",
      "step :  202 loss :  0.08032672852277756\n",
      "step :  203 loss :  0.08027226477861404\n",
      "step :  204 loss :  0.08030015230178833\n",
      "step :  205 loss :  0.08030828833580017\n",
      "step :  206 loss :  0.08027718961238861\n",
      "step :  207 loss :  0.08031105250120163\n",
      "step :  208 loss :  0.08025769889354706\n",
      "step :  209 loss :  0.08033724129199982\n",
      "step :  210 loss :  0.08023903518915176\n",
      "step :  211 loss :  0.08042018860578537\n",
      "step :  212 loss :  0.08033254742622375\n",
      "step :  213 loss :  0.08029504865407944\n",
      "step :  214 loss :  0.08031682670116425\n",
      "step :  215 loss :  0.08026019483804703\n",
      "step :  216 loss :  0.0802861750125885\n",
      "step :  217 loss :  0.08024251461029053\n",
      "step :  218 loss :  0.08032821863889694\n",
      "step :  219 loss :  0.08026816695928574\n",
      "step :  220 loss :  0.08027002960443497\n",
      "step :  221 loss :  0.08027252554893494\n",
      "step :  222 loss :  0.0802740603685379\n",
      "step :  223 loss :  0.08024553954601288\n",
      "step :  224 loss :  0.08031398802995682\n",
      "step :  225 loss :  0.08023228496313095\n",
      "step :  226 loss :  0.08026992529630661\n",
      "step :  227 loss :  0.08023550361394882\n",
      "step :  228 loss :  0.0802568718791008\n",
      "step :  229 loss :  0.08022862672805786\n",
      "step :  230 loss :  0.0802394300699234\n",
      "step :  231 loss :  0.08024110645055771\n",
      "step :  232 loss :  0.08022031933069229\n",
      "step :  233 loss :  0.08029328286647797\n",
      "step :  234 loss :  0.08022253960371017\n",
      "step :  235 loss :  0.08025480806827545\n",
      "step :  236 loss :  0.08022597432136536\n",
      "step :  237 loss :  0.08022656291723251\n",
      "step :  238 loss :  0.08024538308382034\n",
      "step :  239 loss :  0.08021619915962219\n",
      "step :  240 loss :  0.08023147284984589\n",
      "step :  241 loss :  0.0801948830485344\n",
      "step :  242 loss :  0.08024867624044418\n",
      "step :  243 loss :  0.08021192997694016\n",
      "step :  244 loss :  0.08022485673427582\n",
      "step :  245 loss :  0.08021210879087448\n",
      "step :  246 loss :  0.08022817224264145\n",
      "step :  247 loss :  0.08020444214344025\n",
      "step :  248 loss :  0.08020532876253128\n",
      "step :  249 loss :  0.08021529763936996\n",
      "step :  250 loss :  0.08021511882543564\n",
      "step :  251 loss :  0.08020184934139252\n",
      "step :  252 loss :  0.08019997924566269\n",
      "step :  253 loss :  0.08023090660572052\n",
      "step :  254 loss :  0.0801929235458374\n",
      "step :  255 loss :  0.0802077129483223\n",
      "step :  256 loss :  0.08022014051675797\n",
      "step :  257 loss :  0.08019798994064331\n",
      "step :  258 loss :  0.08022280037403107\n",
      "step :  259 loss :  0.08019772917032242\n",
      "step :  260 loss :  0.0802086591720581\n",
      "step :  261 loss :  0.0801970437169075\n",
      "step :  262 loss :  0.08020204305648804\n",
      "step :  263 loss :  0.0801975205540657\n",
      "step :  264 loss :  0.08022032678127289\n",
      "step :  265 loss :  0.08020105212926865\n",
      "step :  266 loss :  0.08020908385515213\n",
      "step :  267 loss :  0.08018863201141357\n",
      "step :  268 loss :  0.08019765466451645\n",
      "step :  269 loss :  0.08018609881401062\n",
      "step :  270 loss :  0.08018853515386581\n",
      "step :  271 loss :  0.08018560707569122\n",
      "step :  272 loss :  0.08018822968006134\n",
      "step :  273 loss :  0.08019154518842697\n",
      "step :  274 loss :  0.08018564432859421\n",
      "step :  275 loss :  0.080192431807518\n",
      "step :  276 loss :  0.08019299805164337\n",
      "step :  277 loss :  0.08018281310796738\n",
      "step :  278 loss :  0.0801820307970047\n",
      "step :  279 loss :  0.0801963061094284\n",
      "step :  280 loss :  0.08018932491540909\n",
      "step :  281 loss :  0.0801900327205658\n",
      "step :  282 loss :  0.08017627149820328\n",
      "step :  283 loss :  0.08018450438976288\n",
      "step :  284 loss :  0.08017832785844803\n",
      "step :  285 loss :  0.08017779141664505\n",
      "step :  286 loss :  0.08017829805612564\n",
      "step :  287 loss :  0.08017902821302414\n",
      "step :  288 loss :  0.08018923550844193\n",
      "step :  289 loss :  0.08017955720424652\n",
      "step :  290 loss :  0.0801902636885643\n",
      "step :  291 loss :  0.0801791176199913\n",
      "step :  292 loss :  0.08017855882644653\n",
      "step :  293 loss :  0.08017227798700333\n",
      "step :  294 loss :  0.08017544448375702\n",
      "step :  295 loss :  0.08018240332603455\n",
      "step :  296 loss :  0.08017008006572723\n",
      "step :  297 loss :  0.08018644899129868\n",
      "step :  298 loss :  0.08017701655626297\n",
      "step :  299 loss :  0.08017676323652267\n",
      "step :  300 loss :  0.08018306642770767\n",
      "step :  301 loss :  0.08017521351575851\n",
      "step :  302 loss :  0.08017716556787491\n",
      "step :  303 loss :  0.08017375320196152\n",
      "step :  304 loss :  0.08017397671937943\n",
      "step :  305 loss :  0.08019145578145981\n",
      "step :  306 loss :  0.08017482608556747\n",
      "step :  307 loss :  0.08017084747552872\n",
      "step :  308 loss :  0.08018062263727188\n",
      "step :  309 loss :  0.08017177879810333\n",
      "step :  310 loss :  0.08017155528068542\n",
      "step :  311 loss :  0.08017511665821075\n",
      "step :  312 loss :  0.08016359806060791\n",
      "step :  313 loss :  0.08016812801361084\n",
      "step :  314 loss :  0.0801752507686615\n",
      "step :  315 loss :  0.08016850054264069\n",
      "step :  316 loss :  0.08016957342624664\n",
      "step :  317 loss :  0.0801718682050705\n",
      "step :  318 loss :  0.0801657885313034\n",
      "step :  319 loss :  0.08017267286777496\n",
      "step :  320 loss :  0.08016874641180038\n",
      "step :  321 loss :  0.08016499131917953\n",
      "step :  322 loss :  0.0801752582192421\n",
      "step :  323 loss :  0.08016796410083771\n",
      "step :  324 loss :  0.08016735315322876\n",
      "step :  325 loss :  0.0801752582192421\n",
      "step :  326 loss :  0.08016548305749893\n",
      "step :  327 loss :  0.08016768842935562\n",
      "step :  328 loss :  0.0801699310541153\n",
      "step :  329 loss :  0.08016141504049301\n",
      "step :  330 loss :  0.08017053455114365\n",
      "step :  331 loss :  0.08016715943813324\n",
      "step :  332 loss :  0.08016464859247208\n",
      "step :  333 loss :  0.08017117530107498\n",
      "step :  334 loss :  0.08016624301671982\n",
      "step :  335 loss :  0.08016769587993622\n",
      "step :  336 loss :  0.08016557991504669\n",
      "step :  337 loss :  0.08016387373209\n",
      "step :  338 loss :  0.08016502857208252\n",
      "step :  339 loss :  0.08017179369926453\n",
      "step :  340 loss :  0.08016490936279297\n",
      "step :  341 loss :  0.08016575127840042\n",
      "step :  342 loss :  0.08016975224018097\n",
      "step :  343 loss :  0.08016406744718552\n",
      "step :  344 loss :  0.08016932010650635\n",
      "step :  345 loss :  0.0801640972495079\n",
      "step :  346 loss :  0.08016457408666611\n",
      "step :  347 loss :  0.08016751706600189\n",
      "step :  348 loss :  0.08016344904899597\n",
      "step :  349 loss :  0.08016359061002731\n",
      "step :  350 loss :  0.08016481250524521\n",
      "step :  351 loss :  0.0801624283194542\n",
      "step :  352 loss :  0.08016198128461838\n",
      "step :  353 loss :  0.08016597479581833\n",
      "step :  354 loss :  0.08016178011894226\n",
      "step :  355 loss :  0.08016524463891983\n",
      "step :  356 loss :  0.08016281574964523\n",
      "step :  357 loss :  0.0801645889878273\n",
      "step :  358 loss :  0.08016297221183777\n",
      "step :  359 loss :  0.08016139268875122\n",
      "step :  360 loss :  0.08016370236873627\n",
      "step :  361 loss :  0.08016128838062286\n",
      "step :  362 loss :  0.08016278594732285\n",
      "step :  363 loss :  0.08015970885753632\n",
      "step :  364 loss :  0.08016156405210495\n",
      "step :  365 loss :  0.0801609456539154\n",
      "step :  366 loss :  0.08016004413366318\n",
      "step :  367 loss :  0.08016322553157806\n",
      "step :  368 loss :  0.08016058057546616\n",
      "step :  369 loss :  0.08016138523817062\n",
      "step :  370 loss :  0.08016212284564972\n",
      "step :  371 loss :  0.08016016334295273\n",
      "step :  372 loss :  0.0801616683602333\n",
      "step :  373 loss :  0.08016061037778854\n",
      "step :  374 loss :  0.08016111701726913\n",
      "step :  375 loss :  0.08016077429056168\n",
      "step :  376 loss :  0.08015991002321243\n",
      "step :  377 loss :  0.08016113936901093\n",
      "step :  378 loss :  0.08015979826450348\n",
      "step :  379 loss :  0.08016233146190643\n",
      "step :  380 loss :  0.08015965670347214\n",
      "step :  381 loss :  0.08016076683998108\n",
      "step :  382 loss :  0.08016181737184525\n",
      "step :  383 loss :  0.08015903830528259\n",
      "step :  384 loss :  0.08015955984592438\n",
      "step :  385 loss :  0.08016329258680344\n",
      "step :  386 loss :  0.08015977591276169\n",
      "step :  387 loss :  0.08015952259302139\n",
      "step :  388 loss :  0.08016214519739151\n",
      "step :  389 loss :  0.0801602229475975\n",
      "step :  390 loss :  0.08015865087509155\n",
      "step :  391 loss :  0.08015856146812439\n",
      "step :  392 loss :  0.08016350120306015\n",
      "step :  393 loss :  0.080160953104496\n",
      "step :  394 loss :  0.08015713840723038\n",
      "step :  395 loss :  0.08015856891870499\n",
      "step :  396 loss :  0.08016197383403778\n",
      "step :  397 loss :  0.08015909045934677\n",
      "step :  398 loss :  0.0801587775349617\n",
      "step :  399 loss :  0.08015944063663483\n",
      "step :  400 loss :  0.08015920966863632\n",
      "step :  401 loss :  0.08015833050012589\n",
      "step :  402 loss :  0.08015958964824677\n",
      "step :  403 loss :  0.08015933632850647\n",
      "step :  404 loss :  0.08015844225883484\n",
      "step :  405 loss :  0.08016128093004227\n",
      "step :  406 loss :  0.08015882223844528\n",
      "step :  407 loss :  0.08015912771224976\n",
      "step :  408 loss :  0.08015917986631393\n",
      "step :  409 loss :  0.08015944808721542\n",
      "step :  410 loss :  0.08015851676464081\n",
      "step :  411 loss :  0.08015874028205872\n",
      "step :  412 loss :  0.08015882968902588\n",
      "step :  413 loss :  0.08015909045934677\n",
      "step :  414 loss :  0.08015859872102737\n",
      "step :  415 loss :  0.08015881478786469\n",
      "step :  416 loss :  0.08015856891870499\n",
      "step :  417 loss :  0.08015875518321991\n",
      "step :  418 loss :  0.08015861362218857\n",
      "step :  419 loss :  0.08015815913677216\n",
      "step :  420 loss :  0.08015896379947662\n",
      "step :  421 loss :  0.0801580473780632\n",
      "step :  422 loss :  0.08015943318605423\n",
      "step :  423 loss :  0.0801580548286438\n",
      "step :  424 loss :  0.0801578015089035\n",
      "step :  425 loss :  0.0801597312092781\n",
      "step :  426 loss :  0.08015825599431992\n",
      "step :  427 loss :  0.080158531665802\n",
      "step :  428 loss :  0.08015824109315872\n",
      "step :  429 loss :  0.08015896379947662\n",
      "step :  430 loss :  0.08015800267457962\n",
      "step :  431 loss :  0.08015798777341843\n",
      "step :  432 loss :  0.08015955984592438\n",
      "step :  433 loss :  0.08015846461057663\n",
      "step :  434 loss :  0.0801573246717453\n",
      "step :  435 loss :  0.0801582783460617\n",
      "step :  436 loss :  0.08015790581703186\n",
      "step :  437 loss :  0.0801578164100647\n",
      "step :  438 loss :  0.08015825599431992\n",
      "step :  439 loss :  0.08015777915716171\n",
      "step :  440 loss :  0.08015815913677216\n",
      "step :  441 loss :  0.08015760034322739\n",
      "step :  442 loss :  0.08015888184309006\n",
      "step :  443 loss :  0.08015767484903336\n",
      "step :  444 loss :  0.08015760779380798\n",
      "step :  445 loss :  0.08015812933444977\n",
      "step :  446 loss :  0.0801575779914856\n",
      "step :  447 loss :  0.08015801012516022\n",
      "step :  448 loss :  0.08015754073858261\n",
      "step :  449 loss :  0.08015802502632141\n",
      "step :  450 loss :  0.08015739172697067\n",
      "step :  451 loss :  0.08015778660774231\n",
      "step :  452 loss :  0.08015752583742142\n",
      "step :  453 loss :  0.08015775680541992\n",
      "step :  454 loss :  0.0801575630903244\n",
      "step :  455 loss :  0.08015748858451843\n",
      "step :  456 loss :  0.080157570540905\n",
      "step :  457 loss :  0.08015747368335724\n",
      "step :  458 loss :  0.08015753328800201\n",
      "step :  459 loss :  0.08015847951173782\n",
      "step :  460 loss :  0.08015752583742142\n",
      "step :  461 loss :  0.0801573395729065\n",
      "step :  462 loss :  0.08015815913677216\n",
      "step :  463 loss :  0.08015746623277664\n",
      "step :  464 loss :  0.08015802502632141\n",
      "step :  465 loss :  0.08015751093626022\n",
      "step :  466 loss :  0.08015734702348709\n",
      "step :  467 loss :  0.08015773445367813\n",
      "step :  468 loss :  0.08015775680541992\n",
      "step :  469 loss :  0.08015722036361694\n",
      "step :  470 loss :  0.08015793561935425\n",
      "step :  471 loss :  0.08015728741884232\n",
      "step :  472 loss :  0.08015720546245575\n",
      "step :  473 loss :  0.08015742152929306\n",
      "step :  474 loss :  0.08015725761651993\n",
      "step :  475 loss :  0.08015725761651993\n",
      "step :  476 loss :  0.08015725016593933\n",
      "step :  477 loss :  0.08015789836645126\n",
      "step :  478 loss :  0.08015721291303635\n",
      "step :  479 loss :  0.08015693724155426\n",
      "step :  480 loss :  0.0801575779914856\n",
      "step :  481 loss :  0.08015752583742142\n",
      "step :  482 loss :  0.080157071352005\n",
      "step :  483 loss :  0.08015703409910202\n",
      "step :  484 loss :  0.08015777915716171\n",
      "step :  485 loss :  0.08015719801187515\n",
      "step :  486 loss :  0.08015705645084381\n",
      "step :  487 loss :  0.08015749603509903\n",
      "step :  488 loss :  0.08015722036361694\n",
      "step :  489 loss :  0.08015694469213486\n",
      "step :  490 loss :  0.08015727251768112\n",
      "step :  491 loss :  0.08015706390142441\n",
      "step :  492 loss :  0.08015701919794083\n",
      "step :  493 loss :  0.08015720546245575\n",
      "step :  494 loss :  0.08015701919794083\n",
      "step :  495 loss :  0.08015710115432739\n",
      "step :  496 loss :  0.08015738427639008\n",
      "step :  497 loss :  0.08015705645084381\n",
      "step :  498 loss :  0.08015721291303635\n",
      "step :  499 loss :  0.08015710860490799\n",
      "Training model 3\n",
      "step :  0 loss :  0.34561923146247864\n",
      "step :  1 loss :  0.3124121129512787\n",
      "step :  2 loss :  0.1996186524629593\n",
      "step :  3 loss :  0.21527209877967834\n",
      "step :  4 loss :  0.1838848888874054\n",
      "step :  5 loss :  0.19174617528915405\n",
      "step :  6 loss :  0.2211986929178238\n",
      "step :  7 loss :  0.2168634831905365\n",
      "step :  8 loss :  0.1909167766571045\n",
      "step :  9 loss :  0.20423565804958344\n",
      "step :  10 loss :  0.17018823325634003\n",
      "step :  11 loss :  0.1630675345659256\n",
      "step :  12 loss :  0.15883029997348785\n",
      "step :  13 loss :  0.1628415733575821\n",
      "step :  14 loss :  0.14256298542022705\n",
      "step :  15 loss :  0.12496937811374664\n",
      "step :  16 loss :  0.11811266094446182\n",
      "step :  17 loss :  0.2662246823310852\n",
      "step :  18 loss :  0.18299731612205505\n",
      "step :  19 loss :  0.13273054361343384\n",
      "step :  20 loss :  0.1443110704421997\n",
      "step :  21 loss :  0.12185024470090866\n",
      "step :  22 loss :  0.14116598665714264\n",
      "step :  23 loss :  0.10490959882736206\n",
      "step :  24 loss :  0.1360393613576889\n",
      "step :  25 loss :  0.09424348175525665\n",
      "step :  26 loss :  0.09280042350292206\n",
      "step :  27 loss :  0.09188809245824814\n",
      "step :  28 loss :  0.08972597122192383\n",
      "step :  29 loss :  0.09107045829296112\n",
      "step :  30 loss :  0.09102484583854675\n",
      "step :  31 loss :  0.08885050565004349\n",
      "step :  32 loss :  0.09713506698608398\n",
      "step :  33 loss :  0.10066040605306625\n",
      "step :  34 loss :  0.18231084942817688\n",
      "step :  35 loss :  0.09353695809841156\n",
      "step :  36 loss :  0.11659754812717438\n",
      "step :  37 loss :  0.11323133111000061\n",
      "step :  38 loss :  0.09022265672683716\n",
      "step :  39 loss :  0.11877226084470749\n",
      "step :  40 loss :  0.11192809045314789\n",
      "step :  41 loss :  0.08449659496545792\n",
      "step :  42 loss :  0.08665742725133896\n",
      "step :  43 loss :  0.08549921214580536\n",
      "step :  44 loss :  0.09066680073738098\n",
      "step :  45 loss :  0.10250432789325714\n",
      "step :  46 loss :  0.09640973061323166\n",
      "step :  47 loss :  0.09699305146932602\n",
      "step :  48 loss :  0.08860243111848831\n",
      "step :  49 loss :  0.09542708098888397\n",
      "step :  50 loss :  0.09254889190196991\n",
      "step :  51 loss :  0.08576151728630066\n",
      "step :  52 loss :  0.10847149044275284\n",
      "step :  53 loss :  0.08666963130235672\n",
      "step :  54 loss :  0.08563341945409775\n",
      "step :  55 loss :  0.08325710147619247\n",
      "step :  56 loss :  0.08649606257677078\n",
      "step :  57 loss :  0.08173085004091263\n",
      "step :  58 loss :  0.08463478088378906\n",
      "step :  59 loss :  0.08402212709188461\n",
      "step :  60 loss :  0.08759012073278427\n",
      "step :  61 loss :  0.08946473151445389\n",
      "step :  62 loss :  0.09534715116024017\n",
      "step :  63 loss :  0.08716484159231186\n",
      "step :  64 loss :  0.0908920168876648\n",
      "step :  65 loss :  0.09288891404867172\n",
      "step :  66 loss :  0.08605311810970306\n",
      "step :  67 loss :  0.08244111388921738\n",
      "step :  68 loss :  0.08213617652654648\n",
      "step :  69 loss :  0.09448045492172241\n",
      "step :  70 loss :  0.08851756900548935\n",
      "step :  71 loss :  0.08336496353149414\n",
      "step :  72 loss :  0.08232660591602325\n",
      "step :  73 loss :  0.08377227932214737\n",
      "step :  74 loss :  0.08646243065595627\n",
      "step :  75 loss :  0.08182224631309509\n",
      "step :  76 loss :  0.08276870101690292\n",
      "step :  77 loss :  0.08442585170269012\n",
      "step :  78 loss :  0.08155767619609833\n",
      "step :  79 loss :  0.081607885658741\n",
      "step :  80 loss :  0.08288265764713287\n",
      "step :  81 loss :  0.08220699429512024\n",
      "step :  82 loss :  0.08234617859125137\n",
      "step :  83 loss :  0.0821961760520935\n",
      "step :  84 loss :  0.08870472013950348\n",
      "step :  85 loss :  0.0813613161444664\n",
      "step :  86 loss :  0.08109962940216064\n",
      "step :  87 loss :  0.08084134757518768\n",
      "step :  88 loss :  0.08171626925468445\n",
      "step :  89 loss :  0.08496591448783875\n",
      "step :  90 loss :  0.08615846186876297\n",
      "step :  91 loss :  0.08583854883909225\n",
      "step :  92 loss :  0.08133973181247711\n",
      "step :  93 loss :  0.0811861902475357\n",
      "step :  94 loss :  0.08136128634214401\n",
      "step :  95 loss :  0.0882810726761818\n",
      "step :  96 loss :  0.08206800371408463\n",
      "step :  97 loss :  0.0810973048210144\n",
      "step :  98 loss :  0.08119384944438934\n",
      "step :  99 loss :  0.08150903135538101\n",
      "step :  100 loss :  0.08286277949810028\n",
      "step :  101 loss :  0.08048190921545029\n",
      "step :  102 loss :  0.08058878779411316\n",
      "step :  103 loss :  0.0807315781712532\n",
      "step :  104 loss :  0.08041692525148392\n",
      "step :  105 loss :  0.08078569173812866\n",
      "step :  106 loss :  0.08094669133424759\n",
      "step :  107 loss :  0.080571748316288\n",
      "step :  108 loss :  0.08064918965101242\n",
      "step :  109 loss :  0.08093277364969254\n",
      "step :  110 loss :  0.0820736214518547\n",
      "step :  111 loss :  0.08279535174369812\n",
      "step :  112 loss :  0.08131778240203857\n",
      "step :  113 loss :  0.08036588132381439\n",
      "step :  114 loss :  0.08079923689365387\n",
      "step :  115 loss :  0.08018003404140472\n",
      "step :  116 loss :  0.08095785975456238\n",
      "step :  117 loss :  0.08041862398386002\n",
      "step :  118 loss :  0.0814724713563919\n",
      "step :  119 loss :  0.08032648265361786\n",
      "step :  120 loss :  0.08181800693273544\n",
      "step :  121 loss :  0.080679751932621\n",
      "step :  122 loss :  0.08015533536672592\n",
      "step :  123 loss :  0.08144114166498184\n",
      "step :  124 loss :  0.08148360997438431\n",
      "step :  125 loss :  0.08076467365026474\n",
      "step :  126 loss :  0.08008483797311783\n",
      "step :  127 loss :  0.08031624555587769\n",
      "step :  128 loss :  0.08040302246809006\n",
      "step :  129 loss :  0.08069582283496857\n",
      "step :  130 loss :  0.08076488226652145\n",
      "step :  131 loss :  0.0800715833902359\n",
      "step :  132 loss :  0.07990799844264984\n",
      "step :  133 loss :  0.0801379457116127\n",
      "step :  134 loss :  0.07997133582830429\n",
      "step :  135 loss :  0.07993865758180618\n",
      "step :  136 loss :  0.08026955276727676\n",
      "step :  137 loss :  0.08002709597349167\n",
      "step :  138 loss :  0.08101704716682434\n",
      "step :  139 loss :  0.08000191301107407\n",
      "step :  140 loss :  0.07997594773769379\n",
      "step :  141 loss :  0.07995551824569702\n",
      "step :  142 loss :  0.07992308586835861\n",
      "step :  143 loss :  0.079865962266922\n",
      "step :  144 loss :  0.08022113144397736\n",
      "step :  145 loss :  0.08025307953357697\n",
      "step :  146 loss :  0.0798196792602539\n",
      "step :  147 loss :  0.07982004433870316\n",
      "step :  148 loss :  0.07984545081853867\n",
      "step :  149 loss :  0.07981493324041367\n",
      "step :  150 loss :  0.07996998727321625\n",
      "step :  151 loss :  0.080073282122612\n",
      "step :  152 loss :  0.07974474877119064\n",
      "step :  153 loss :  0.07972834259271622\n",
      "step :  154 loss :  0.07993003726005554\n",
      "step :  155 loss :  0.07983911782503128\n",
      "step :  156 loss :  0.07970207184553146\n",
      "step :  157 loss :  0.0798276886343956\n",
      "step :  158 loss :  0.07963753491640091\n",
      "step :  159 loss :  0.07987333834171295\n",
      "step :  160 loss :  0.07978443801403046\n",
      "step :  161 loss :  0.079658143222332\n",
      "step :  162 loss :  0.07977747172117233\n",
      "step :  163 loss :  0.07986462116241455\n",
      "step :  164 loss :  0.07994449138641357\n",
      "step :  165 loss :  0.07973501831293106\n",
      "step :  166 loss :  0.07975015789270401\n",
      "step :  167 loss :  0.07964573055505753\n",
      "step :  168 loss :  0.07979264855384827\n",
      "step :  169 loss :  0.07965631037950516\n",
      "step :  170 loss :  0.079680897295475\n",
      "step :  171 loss :  0.07964347302913666\n",
      "step :  172 loss :  0.07966482639312744\n",
      "step :  173 loss :  0.07961857318878174\n",
      "step :  174 loss :  0.07963313162326813\n",
      "step :  175 loss :  0.07963055372238159\n",
      "step :  176 loss :  0.07960344851016998\n",
      "step :  177 loss :  0.07960375398397446\n",
      "step :  178 loss :  0.07958738505840302\n",
      "step :  179 loss :  0.07961797714233398\n",
      "step :  180 loss :  0.07960877567529678\n",
      "step :  181 loss :  0.07958005368709564\n",
      "step :  182 loss :  0.07958631962537766\n",
      "step :  183 loss :  0.07954157888889313\n",
      "step :  184 loss :  0.0796610414981842\n",
      "step :  185 loss :  0.07956524193286896\n",
      "step :  186 loss :  0.0795632004737854\n",
      "step :  187 loss :  0.079586923122406\n",
      "step :  188 loss :  0.0795329287648201\n",
      "step :  189 loss :  0.0795387551188469\n",
      "step :  190 loss :  0.07956910878419876\n",
      "step :  191 loss :  0.07951758801937103\n",
      "step :  192 loss :  0.07950601726770401\n",
      "step :  193 loss :  0.07953055948019028\n",
      "step :  194 loss :  0.07953353226184845\n",
      "step :  195 loss :  0.0795421227812767\n",
      "step :  196 loss :  0.07953111082315445\n",
      "step :  197 loss :  0.07950414717197418\n",
      "step :  198 loss :  0.07954590022563934\n",
      "step :  199 loss :  0.0795050784945488\n",
      "step :  200 loss :  0.07952728122472763\n",
      "step :  201 loss :  0.07949191331863403\n",
      "step :  202 loss :  0.07950251549482346\n",
      "step :  203 loss :  0.07950543612241745\n",
      "step :  204 loss :  0.07956268638372421\n",
      "step :  205 loss :  0.07955726981163025\n",
      "step :  206 loss :  0.07960274070501328\n",
      "step :  207 loss :  0.07952245324850082\n",
      "step :  208 loss :  0.0795278251171112\n",
      "step :  209 loss :  0.07947304099798203\n",
      "step :  210 loss :  0.07950159907341003\n",
      "step :  211 loss :  0.07950402051210403\n",
      "step :  212 loss :  0.07948150485754013\n",
      "step :  213 loss :  0.07948770374059677\n",
      "step :  214 loss :  0.0794978141784668\n",
      "step :  215 loss :  0.07947058975696564\n",
      "step :  216 loss :  0.07950369268655777\n",
      "step :  217 loss :  0.0794580951333046\n",
      "step :  218 loss :  0.07953125983476639\n",
      "step :  219 loss :  0.07945454120635986\n",
      "step :  220 loss :  0.07953233271837234\n",
      "step :  221 loss :  0.07947462797164917\n",
      "step :  222 loss :  0.07947291433811188\n",
      "step :  223 loss :  0.07946626842021942\n",
      "step :  224 loss :  0.07948439568281174\n",
      "step :  225 loss :  0.07945475727319717\n",
      "step :  226 loss :  0.07945654541254044\n",
      "step :  227 loss :  0.07946253567934036\n",
      "step :  228 loss :  0.07944704592227936\n",
      "step :  229 loss :  0.07943838089704514\n",
      "step :  230 loss :  0.07953045517206192\n",
      "step :  231 loss :  0.079449363052845\n",
      "step :  232 loss :  0.07947242259979248\n",
      "step :  233 loss :  0.07945309579372406\n",
      "step :  234 loss :  0.07945587486028671\n",
      "step :  235 loss :  0.07943576574325562\n",
      "step :  236 loss :  0.07946644723415375\n",
      "step :  237 loss :  0.0794437825679779\n",
      "step :  238 loss :  0.07946216315031052\n",
      "step :  239 loss :  0.07943537086248398\n",
      "step :  240 loss :  0.07946580648422241\n",
      "step :  241 loss :  0.07942317426204681\n",
      "step :  242 loss :  0.07944293320178986\n",
      "step :  243 loss :  0.07943132519721985\n",
      "step :  244 loss :  0.07943271100521088\n",
      "step :  245 loss :  0.07942984253168106\n",
      "step :  246 loss :  0.07942413538694382\n",
      "step :  247 loss :  0.07945038378238678\n",
      "step :  248 loss :  0.07942644506692886\n",
      "step :  249 loss :  0.07944382727146149\n",
      "step :  250 loss :  0.07942444831132889\n",
      "step :  251 loss :  0.07944510132074356\n",
      "step :  252 loss :  0.07941829413175583\n",
      "step :  253 loss :  0.07942560315132141\n",
      "step :  254 loss :  0.0794362798333168\n",
      "step :  255 loss :  0.07942263036966324\n",
      "step :  256 loss :  0.07943437248468399\n",
      "step :  257 loss :  0.07942373305559158\n",
      "step :  258 loss :  0.07942222058773041\n",
      "step :  259 loss :  0.07942328602075577\n",
      "step :  260 loss :  0.07942042499780655\n",
      "step :  261 loss :  0.07941436767578125\n",
      "step :  262 loss :  0.07942583411931992\n",
      "step :  263 loss :  0.07941178977489471\n",
      "step :  264 loss :  0.07944341003894806\n",
      "step :  265 loss :  0.07941195368766785\n",
      "step :  266 loss :  0.0794171690940857\n",
      "step :  267 loss :  0.07942137122154236\n",
      "step :  268 loss :  0.07941887527704239\n",
      "step :  269 loss :  0.0794171616435051\n",
      "step :  270 loss :  0.07941611856222153\n",
      "step :  271 loss :  0.07941745221614838\n",
      "step :  272 loss :  0.07942435145378113\n",
      "step :  273 loss :  0.07940676063299179\n",
      "step :  274 loss :  0.07941168546676636\n",
      "step :  275 loss :  0.079432412981987\n",
      "step :  276 loss :  0.07940723747015\n",
      "step :  277 loss :  0.07941114157438278\n",
      "step :  278 loss :  0.07941456139087677\n",
      "step :  279 loss :  0.07940936833620071\n",
      "step :  280 loss :  0.07940429449081421\n",
      "step :  281 loss :  0.07940208911895752\n",
      "step :  282 loss :  0.07940506190061569\n",
      "step :  283 loss :  0.07940930128097534\n",
      "step :  284 loss :  0.0794050544500351\n",
      "step :  285 loss :  0.07940449565649033\n",
      "step :  286 loss :  0.07940153032541275\n",
      "step :  287 loss :  0.07940828800201416\n",
      "step :  288 loss :  0.07940082252025604\n",
      "step :  289 loss :  0.07940711081027985\n",
      "step :  290 loss :  0.07940217852592468\n",
      "step :  291 loss :  0.07940530776977539\n",
      "step :  292 loss :  0.0794004574418068\n",
      "step :  293 loss :  0.07940313965082169\n",
      "step :  294 loss :  0.07940256595611572\n",
      "step :  295 loss :  0.07940344512462616\n",
      "step :  296 loss :  0.07940248399972916\n",
      "step :  297 loss :  0.07941058278083801\n",
      "step :  298 loss :  0.07939895242452621\n",
      "step :  299 loss :  0.07940477132797241\n",
      "step :  300 loss :  0.0794004574418068\n",
      "step :  301 loss :  0.07939507067203522\n",
      "step :  302 loss :  0.07941503077745438\n",
      "step :  303 loss :  0.07940302789211273\n",
      "step :  304 loss :  0.07940059900283813\n",
      "step :  305 loss :  0.07940360903739929\n",
      "step :  306 loss :  0.07939770817756653\n",
      "step :  307 loss :  0.07939759641885757\n",
      "step :  308 loss :  0.07941154390573502\n",
      "step :  309 loss :  0.07940176129341125\n",
      "step :  310 loss :  0.07939847558736801\n",
      "step :  311 loss :  0.0794159397482872\n",
      "step :  312 loss :  0.07940009981393814\n",
      "step :  313 loss :  0.07939554750919342\n",
      "step :  314 loss :  0.07940754294395447\n",
      "step :  315 loss :  0.0793975442647934\n",
      "step :  316 loss :  0.07939523458480835\n",
      "step :  317 loss :  0.0794060081243515\n",
      "step :  318 loss :  0.07939475029706955\n",
      "step :  319 loss :  0.07939540594816208\n",
      "step :  320 loss :  0.0794026255607605\n",
      "step :  321 loss :  0.07939369231462479\n",
      "step :  322 loss :  0.0793939083814621\n",
      "step :  323 loss :  0.07940002530813217\n",
      "step :  324 loss :  0.07939526438713074\n",
      "step :  325 loss :  0.07939314097166061\n",
      "step :  326 loss :  0.0793926864862442\n",
      "step :  327 loss :  0.0793944001197815\n",
      "step :  328 loss :  0.07939150929450989\n",
      "step :  329 loss :  0.07939985394477844\n",
      "step :  330 loss :  0.07939666509628296\n",
      "step :  331 loss :  0.07939113676548004\n",
      "step :  332 loss :  0.07939258962869644\n",
      "step :  333 loss :  0.0793909803032875\n",
      "step :  334 loss :  0.07939375191926956\n",
      "step :  335 loss :  0.07939618825912476\n",
      "step :  336 loss :  0.0793914794921875\n",
      "step :  337 loss :  0.07939109951257706\n",
      "step :  338 loss :  0.07939110696315765\n",
      "step :  339 loss :  0.07939060777425766\n",
      "step :  340 loss :  0.07938915491104126\n",
      "step :  341 loss :  0.07939034700393677\n",
      "step :  342 loss :  0.0793890729546547\n",
      "step :  343 loss :  0.07939088344573975\n",
      "step :  344 loss :  0.07938854396343231\n",
      "step :  345 loss :  0.07939250767230988\n",
      "step :  346 loss :  0.07938869297504425\n",
      "step :  347 loss :  0.07938927412033081\n",
      "step :  348 loss :  0.07938820868730545\n",
      "step :  349 loss :  0.07938876748085022\n",
      "step :  350 loss :  0.07938916236162186\n",
      "step :  351 loss :  0.07938773930072784\n",
      "step :  352 loss :  0.07938850671052933\n",
      "step :  353 loss :  0.07939014583826065\n",
      "step :  354 loss :  0.07938826084136963\n",
      "step :  355 loss :  0.07938873767852783\n",
      "step :  356 loss :  0.07939306646585464\n",
      "step :  357 loss :  0.079392209649086\n",
      "step :  358 loss :  0.07938919961452484\n",
      "step :  359 loss :  0.07938726991415024\n",
      "step :  360 loss :  0.07938921451568604\n",
      "step :  361 loss :  0.07938703149557114\n",
      "step :  362 loss :  0.07938780635595322\n",
      "step :  363 loss :  0.07938981801271439\n",
      "step :  364 loss :  0.07938899844884872\n",
      "step :  365 loss :  0.07938646525144577\n",
      "step :  366 loss :  0.07938674837350845\n",
      "step :  367 loss :  0.07938843220472336\n",
      "step :  368 loss :  0.07938764244318008\n",
      "step :  369 loss :  0.07938630133867264\n",
      "step :  370 loss :  0.07938674837350845\n",
      "step :  371 loss :  0.07938690483570099\n",
      "step :  372 loss :  0.07938683032989502\n",
      "step :  373 loss :  0.07938634604215622\n",
      "step :  374 loss :  0.0793873593211174\n",
      "step :  375 loss :  0.07938573509454727\n",
      "step :  376 loss :  0.07938758283853531\n",
      "step :  377 loss :  0.07938553392887115\n",
      "step :  378 loss :  0.0793861523270607\n",
      "step :  379 loss :  0.07938675582408905\n",
      "step :  380 loss :  0.0793866515159607\n",
      "step :  381 loss :  0.07938504964113235\n",
      "step :  382 loss :  0.07938605546951294\n",
      "step :  383 loss :  0.07938633114099503\n",
      "step :  384 loss :  0.07938611507415771\n",
      "step :  385 loss :  0.07938583195209503\n",
      "step :  386 loss :  0.07938480377197266\n",
      "step :  387 loss :  0.07938589155673981\n",
      "step :  388 loss :  0.07938499003648758\n",
      "step :  389 loss :  0.07938538491725922\n",
      "step :  390 loss :  0.07938655465841293\n",
      "step :  391 loss :  0.07938525080680847\n",
      "step :  392 loss :  0.07938514649868011\n",
      "step :  393 loss :  0.07938458770513535\n",
      "step :  394 loss :  0.07938437163829803\n",
      "step :  395 loss :  0.07938534766435623\n",
      "step :  396 loss :  0.07938516139984131\n",
      "step :  397 loss :  0.07938413321971893\n",
      "step :  398 loss :  0.0793851763010025\n",
      "step :  399 loss :  0.07938480377197266\n",
      "step :  400 loss :  0.07938426733016968\n",
      "step :  401 loss :  0.07938510179519653\n",
      "step :  402 loss :  0.07938532531261444\n",
      "step :  403 loss :  0.07938428223133087\n",
      "step :  404 loss :  0.07938437163829803\n",
      "step :  405 loss :  0.07938454300165176\n",
      "step :  406 loss :  0.07938461005687714\n",
      "step :  407 loss :  0.07938474416732788\n",
      "step :  408 loss :  0.07938433438539505\n",
      "step :  409 loss :  0.07938490062952042\n",
      "step :  410 loss :  0.07938438653945923\n",
      "step :  411 loss :  0.07938354462385178\n",
      "step :  412 loss :  0.07938379049301147\n",
      "step :  413 loss :  0.07938399165868759\n",
      "step :  414 loss :  0.07938377559185028\n",
      "step :  415 loss :  0.07938431948423386\n",
      "step :  416 loss :  0.07938356697559357\n",
      "step :  417 loss :  0.07938399165868759\n",
      "step :  418 loss :  0.07938362658023834\n",
      "step :  419 loss :  0.07938394695520401\n",
      "step :  420 loss :  0.07938390970230103\n",
      "step :  421 loss :  0.07938344031572342\n",
      "step :  422 loss :  0.07938382774591446\n",
      "step :  423 loss :  0.0793837383389473\n",
      "step :  424 loss :  0.07938343286514282\n",
      "step :  425 loss :  0.07938352972269058\n",
      "step :  426 loss :  0.07938344776630402\n",
      "step :  427 loss :  0.07938355952501297\n",
      "step :  428 loss :  0.07938413321971893\n",
      "step :  429 loss :  0.07938341796398163\n",
      "step :  430 loss :  0.07938366383314133\n",
      "step :  431 loss :  0.07938404381275177\n",
      "step :  432 loss :  0.07938303053379059\n",
      "step :  433 loss :  0.0793832316994667\n",
      "step :  434 loss :  0.07938382029533386\n",
      "step :  435 loss :  0.07938311249017715\n",
      "step :  436 loss :  0.07938318699598312\n",
      "step :  437 loss :  0.0793837159872055\n",
      "step :  438 loss :  0.0793829932808876\n",
      "step :  439 loss :  0.07938270270824432\n",
      "step :  440 loss :  0.07938375324010849\n",
      "step :  441 loss :  0.07938367873430252\n",
      "step :  442 loss :  0.07938279211521149\n",
      "step :  443 loss :  0.07938294112682343\n",
      "step :  444 loss :  0.07938332855701447\n",
      "step :  445 loss :  0.07938285171985626\n",
      "step :  446 loss :  0.0793827548623085\n",
      "step :  447 loss :  0.07938262820243835\n",
      "step :  448 loss :  0.07938267290592194\n",
      "step :  449 loss :  0.07938292622566223\n",
      "step :  450 loss :  0.07938278466463089\n",
      "step :  451 loss :  0.07938285917043686\n",
      "step :  452 loss :  0.07938286662101746\n",
      "step :  453 loss :  0.07938253879547119\n",
      "step :  454 loss :  0.07938273996114731\n",
      "step :  455 loss :  0.07938288897275925\n",
      "step :  456 loss :  0.07938273996114731\n",
      "step :  457 loss :  0.07938294112682343\n",
      "step :  458 loss :  0.0793830081820488\n",
      "step :  459 loss :  0.0793827548623085\n",
      "step :  460 loss :  0.07938282191753387\n",
      "step :  461 loss :  0.07938282191753387\n",
      "step :  462 loss :  0.07938263565301895\n",
      "step :  463 loss :  0.07938264310359955\n",
      "step :  464 loss :  0.07938287407159805\n",
      "step :  465 loss :  0.07938277721405029\n",
      "step :  466 loss :  0.07938270270824432\n",
      "step :  467 loss :  0.07938244193792343\n",
      "step :  468 loss :  0.07938259840011597\n",
      "step :  469 loss :  0.07938282191753387\n",
      "step :  470 loss :  0.07938262820243835\n",
      "step :  471 loss :  0.07938247919082642\n",
      "step :  472 loss :  0.07938273996114731\n",
      "step :  473 loss :  0.07938264310359955\n",
      "step :  474 loss :  0.07938252389431\n",
      "step :  475 loss :  0.07938285917043686\n",
      "step :  476 loss :  0.07938283681869507\n",
      "step :  477 loss :  0.07938239723443985\n",
      "step :  478 loss :  0.07938239723443985\n",
      "step :  479 loss :  0.07938256859779358\n",
      "step :  480 loss :  0.07938267290592194\n",
      "step :  481 loss :  0.07938257604837418\n",
      "step :  482 loss :  0.07938235998153687\n",
      "step :  483 loss :  0.07938238233327866\n",
      "step :  484 loss :  0.07938241213560104\n",
      "step :  485 loss :  0.07938235998153687\n",
      "step :  486 loss :  0.07938231527805328\n",
      "step :  487 loss :  0.07938249409198761\n",
      "step :  488 loss :  0.07938238233327866\n",
      "step :  489 loss :  0.07938238978385925\n",
      "step :  490 loss :  0.07938242703676224\n",
      "step :  491 loss :  0.07938239723443985\n",
      "step :  492 loss :  0.07938244938850403\n",
      "step :  493 loss :  0.07938234508037567\n",
      "step :  494 loss :  0.07938230037689209\n",
      "step :  495 loss :  0.07938238978385925\n",
      "step :  496 loss :  0.07938233762979507\n",
      "step :  497 loss :  0.07938235253095627\n",
      "step :  498 loss :  0.07938230782747269\n",
      "step :  499 loss :  0.07938237488269806\n",
      "Training model 4\n",
      "step :  0 loss :  0.48417550325393677\n",
      "step :  1 loss :  0.21288378536701202\n",
      "step :  2 loss :  0.22223640978336334\n",
      "step :  3 loss :  0.2590847313404083\n",
      "step :  4 loss :  0.2223280519247055\n",
      "step :  5 loss :  0.19298115372657776\n",
      "step :  6 loss :  0.2482997626066208\n",
      "step :  7 loss :  0.21717503666877747\n",
      "step :  8 loss :  0.21526062488555908\n",
      "step :  9 loss :  0.19721969962120056\n",
      "step :  10 loss :  0.2334059178829193\n",
      "step :  11 loss :  0.20346374809741974\n",
      "step :  12 loss :  0.18555521965026855\n",
      "step :  13 loss :  0.18666163086891174\n",
      "step :  14 loss :  0.17282108962535858\n",
      "step :  15 loss :  0.1662670522928238\n",
      "step :  16 loss :  0.1575004607439041\n",
      "step :  17 loss :  0.14783215522766113\n",
      "step :  18 loss :  0.13476984202861786\n",
      "step :  19 loss :  0.11910609900951385\n",
      "step :  20 loss :  0.11042407900094986\n",
      "step :  21 loss :  0.1061236709356308\n",
      "step :  22 loss :  0.1104317158460617\n",
      "step :  23 loss :  0.11816886812448502\n",
      "step :  24 loss :  0.1371852308511734\n",
      "step :  25 loss :  0.21406392753124237\n",
      "step :  26 loss :  0.12875774502754211\n",
      "step :  27 loss :  0.1186918243765831\n",
      "step :  28 loss :  0.13882867991924286\n",
      "step :  29 loss :  0.12717270851135254\n",
      "step :  30 loss :  0.12548208236694336\n",
      "step :  31 loss :  0.10529939830303192\n",
      "step :  32 loss :  0.11987079679965973\n",
      "step :  33 loss :  0.1019141897559166\n",
      "step :  34 loss :  0.11638464778661728\n",
      "step :  35 loss :  0.10439996421337128\n",
      "step :  36 loss :  0.10808456689119339\n",
      "step :  37 loss :  0.11311346292495728\n",
      "step :  38 loss :  0.14586319029331207\n",
      "step :  39 loss :  0.10794389247894287\n",
      "step :  40 loss :  0.0997941642999649\n",
      "step :  41 loss :  0.09511381387710571\n",
      "step :  42 loss :  0.09729766100645065\n",
      "step :  43 loss :  0.12737460434436798\n",
      "step :  44 loss :  0.10299979150295258\n",
      "step :  45 loss :  0.09248770028352737\n",
      "step :  46 loss :  0.09373632818460464\n",
      "step :  47 loss :  0.0979849323630333\n",
      "step :  48 loss :  0.09300456941127777\n",
      "step :  49 loss :  0.08936064690351486\n",
      "step :  50 loss :  0.08877629786729813\n",
      "step :  51 loss :  0.11305493861436844\n",
      "step :  52 loss :  0.0955844596028328\n",
      "step :  53 loss :  0.08813046663999557\n",
      "step :  54 loss :  0.09387598186731339\n",
      "step :  55 loss :  0.08649947494268417\n",
      "step :  56 loss :  0.0895485132932663\n",
      "step :  57 loss :  0.09729157388210297\n",
      "step :  58 loss :  0.08492238819599152\n",
      "step :  59 loss :  0.09243566542863846\n",
      "step :  60 loss :  0.08899730443954468\n",
      "step :  61 loss :  0.09726571291685104\n",
      "step :  62 loss :  0.09289636462926865\n",
      "step :  63 loss :  0.08688034117221832\n",
      "step :  64 loss :  0.08553556352853775\n",
      "step :  65 loss :  0.08666662126779556\n",
      "step :  66 loss :  0.08518648892641068\n",
      "step :  67 loss :  0.08352643251419067\n",
      "step :  68 loss :  0.08885461091995239\n",
      "step :  69 loss :  0.08445102721452713\n",
      "step :  70 loss :  0.09739959239959717\n",
      "step :  71 loss :  0.08878824859857559\n",
      "step :  72 loss :  0.08473806828260422\n",
      "step :  73 loss :  0.08426384627819061\n",
      "step :  74 loss :  0.08427289873361588\n",
      "step :  75 loss :  0.09298815578222275\n",
      "step :  76 loss :  0.08595506101846695\n",
      "step :  77 loss :  0.08416765183210373\n",
      "step :  78 loss :  0.08849548548460007\n",
      "step :  79 loss :  0.08898364752531052\n",
      "step :  80 loss :  0.08476083725690842\n",
      "step :  81 loss :  0.08307220786809921\n",
      "step :  82 loss :  0.08280481398105621\n",
      "step :  83 loss :  0.08618992567062378\n",
      "step :  84 loss :  0.08370070159435272\n",
      "step :  85 loss :  0.08243171125650406\n",
      "step :  86 loss :  0.08606816083192825\n",
      "step :  87 loss :  0.08524293452501297\n",
      "step :  88 loss :  0.08253097534179688\n",
      "step :  89 loss :  0.08296914398670197\n",
      "step :  90 loss :  0.08294264227151871\n",
      "step :  91 loss :  0.08709901571273804\n",
      "step :  92 loss :  0.0822567567229271\n",
      "step :  93 loss :  0.08312499523162842\n",
      "step :  94 loss :  0.08484909683465958\n",
      "step :  95 loss :  0.082005575299263\n",
      "step :  96 loss :  0.08298829197883606\n",
      "step :  97 loss :  0.0846160277724266\n",
      "step :  98 loss :  0.08177438378334045\n",
      "step :  99 loss :  0.08282506465911865\n",
      "step :  100 loss :  0.08304324001073837\n",
      "step :  101 loss :  0.08204501867294312\n",
      "step :  102 loss :  0.08302273601293564\n",
      "step :  103 loss :  0.08212613314390182\n",
      "step :  104 loss :  0.08337751030921936\n",
      "step :  105 loss :  0.0821026936173439\n",
      "step :  106 loss :  0.0822150856256485\n",
      "step :  107 loss :  0.08505623787641525\n",
      "step :  108 loss :  0.08321582525968552\n",
      "step :  109 loss :  0.081912562251091\n",
      "step :  110 loss :  0.08237266540527344\n",
      "step :  111 loss :  0.0824812799692154\n",
      "step :  112 loss :  0.08554843813180923\n",
      "step :  113 loss :  0.08473613113164902\n",
      "step :  114 loss :  0.08379724621772766\n",
      "step :  115 loss :  0.08382195234298706\n",
      "step :  116 loss :  0.08407554775476456\n",
      "step :  117 loss :  0.0840730294585228\n",
      "step :  118 loss :  0.08405017852783203\n",
      "step :  119 loss :  0.08357467502355576\n",
      "step :  120 loss :  0.08222801238298416\n",
      "step :  121 loss :  0.08212359994649887\n",
      "step :  122 loss :  0.08159640431404114\n",
      "step :  123 loss :  0.08145834505558014\n",
      "step :  124 loss :  0.08167405426502228\n",
      "step :  125 loss :  0.08133669197559357\n",
      "step :  126 loss :  0.08114667981863022\n",
      "step :  127 loss :  0.081228107213974\n",
      "step :  128 loss :  0.08126833289861679\n",
      "step :  129 loss :  0.08109357953071594\n",
      "step :  130 loss :  0.08121981471776962\n",
      "step :  131 loss :  0.08114870637655258\n",
      "step :  132 loss :  0.0812787413597107\n",
      "step :  133 loss :  0.08115250617265701\n",
      "step :  134 loss :  0.08123587816953659\n",
      "step :  135 loss :  0.08095507323741913\n",
      "step :  136 loss :  0.08089950680732727\n",
      "step :  137 loss :  0.08101856708526611\n",
      "step :  138 loss :  0.08113647252321243\n",
      "step :  139 loss :  0.08112239837646484\n",
      "step :  140 loss :  0.080886110663414\n",
      "step :  141 loss :  0.0807562917470932\n",
      "step :  142 loss :  0.08077048510313034\n",
      "step :  143 loss :  0.08070240914821625\n",
      "step :  144 loss :  0.08073616027832031\n",
      "step :  145 loss :  0.08068748563528061\n",
      "step :  146 loss :  0.08063935488462448\n",
      "step :  147 loss :  0.08093886077404022\n",
      "step :  148 loss :  0.08066809177398682\n",
      "step :  149 loss :  0.08073586225509644\n",
      "step :  150 loss :  0.08058681339025497\n",
      "step :  151 loss :  0.0805753692984581\n",
      "step :  152 loss :  0.08059538155794144\n",
      "step :  153 loss :  0.08060954511165619\n",
      "step :  154 loss :  0.08053325116634369\n",
      "step :  155 loss :  0.0808253362774849\n",
      "step :  156 loss :  0.08057499676942825\n",
      "step :  157 loss :  0.08049356937408447\n",
      "step :  158 loss :  0.08048427104949951\n",
      "step :  159 loss :  0.08045125752687454\n",
      "step :  160 loss :  0.08041979372501373\n",
      "step :  161 loss :  0.08040369302034378\n",
      "step :  162 loss :  0.08040303736925125\n",
      "step :  163 loss :  0.0804283395409584\n",
      "step :  164 loss :  0.08035062998533249\n",
      "step :  165 loss :  0.0804157480597496\n",
      "step :  166 loss :  0.08032768964767456\n",
      "step :  167 loss :  0.08058793842792511\n",
      "step :  168 loss :  0.08036334812641144\n",
      "step :  169 loss :  0.08042003959417343\n",
      "step :  170 loss :  0.08034873753786087\n",
      "step :  171 loss :  0.08036218583583832\n",
      "step :  172 loss :  0.08030755072832108\n",
      "step :  173 loss :  0.08043339103460312\n",
      "step :  174 loss :  0.08027855306863785\n",
      "step :  175 loss :  0.08031448721885681\n",
      "step :  176 loss :  0.08026358485221863\n",
      "step :  177 loss :  0.08031950145959854\n",
      "step :  178 loss :  0.08022620528936386\n",
      "step :  179 loss :  0.08041006326675415\n",
      "step :  180 loss :  0.08028331398963928\n",
      "step :  181 loss :  0.08034088462591171\n",
      "step :  182 loss :  0.08022493124008179\n",
      "step :  183 loss :  0.08026180416345596\n",
      "step :  184 loss :  0.08025476336479187\n",
      "step :  185 loss :  0.08020763099193573\n",
      "step :  186 loss :  0.08023813366889954\n",
      "step :  187 loss :  0.08022378385066986\n",
      "step :  188 loss :  0.08023927360773087\n",
      "step :  189 loss :  0.08021412789821625\n",
      "step :  190 loss :  0.08022385090589523\n",
      "step :  191 loss :  0.08021453768014908\n",
      "step :  192 loss :  0.08021960407495499\n",
      "step :  193 loss :  0.08031618595123291\n",
      "step :  194 loss :  0.08020778745412827\n",
      "step :  195 loss :  0.0802730917930603\n",
      "step :  196 loss :  0.08017504215240479\n",
      "step :  197 loss :  0.08026019483804703\n",
      "step :  198 loss :  0.08016747981309891\n",
      "step :  199 loss :  0.0802062526345253\n",
      "step :  200 loss :  0.080184705555439\n",
      "step :  201 loss :  0.08019018918275833\n",
      "step :  202 loss :  0.08015028387308121\n",
      "step :  203 loss :  0.080206498503685\n",
      "step :  204 loss :  0.08014782518148422\n",
      "step :  205 loss :  0.08015019446611404\n",
      "step :  206 loss :  0.08015912771224976\n",
      "step :  207 loss :  0.0802222490310669\n",
      "step :  208 loss :  0.08016928285360336\n",
      "step :  209 loss :  0.08018806576728821\n",
      "step :  210 loss :  0.0801270604133606\n",
      "step :  211 loss :  0.08023842424154282\n",
      "step :  212 loss :  0.08013616502285004\n",
      "step :  213 loss :  0.0801653265953064\n",
      "step :  214 loss :  0.08014830201864243\n",
      "step :  215 loss :  0.08015720546245575\n",
      "step :  216 loss :  0.0801607295870781\n",
      "step :  217 loss :  0.08011206239461899\n",
      "step :  218 loss :  0.08016145974397659\n",
      "step :  219 loss :  0.08010489493608475\n",
      "step :  220 loss :  0.08013087511062622\n",
      "step :  221 loss :  0.08012451976537704\n",
      "step :  222 loss :  0.08017844706773758\n",
      "step :  223 loss :  0.08010045439004898\n",
      "step :  224 loss :  0.08013667166233063\n",
      "step :  225 loss :  0.08010629564523697\n",
      "step :  226 loss :  0.08014186471700668\n",
      "step :  227 loss :  0.08010777086019516\n",
      "step :  228 loss :  0.08013975620269775\n",
      "step :  229 loss :  0.08009803295135498\n",
      "step :  230 loss :  0.080157570540905\n",
      "step :  231 loss :  0.08009456098079681\n",
      "step :  232 loss :  0.08014979958534241\n",
      "step :  233 loss :  0.08009292930364609\n",
      "step :  234 loss :  0.0801296979188919\n",
      "step :  235 loss :  0.0801035463809967\n",
      "step :  236 loss :  0.08008691668510437\n",
      "step :  237 loss :  0.08011262863874435\n",
      "step :  238 loss :  0.0800836980342865\n",
      "step :  239 loss :  0.08016297221183777\n",
      "step :  240 loss :  0.08007636666297913\n",
      "step :  241 loss :  0.08010520786046982\n",
      "step :  242 loss :  0.08011118322610855\n",
      "step :  243 loss :  0.08009126037359238\n",
      "step :  244 loss :  0.08009620755910873\n",
      "step :  245 loss :  0.08008735626935959\n",
      "step :  246 loss :  0.08008313924074173\n",
      "step :  247 loss :  0.08007775992155075\n",
      "step :  248 loss :  0.0800829753279686\n",
      "step :  249 loss :  0.08009785413742065\n",
      "step :  250 loss :  0.08008136600255966\n",
      "step :  251 loss :  0.08008322864770889\n",
      "step :  252 loss :  0.08007892221212387\n",
      "step :  253 loss :  0.08007556200027466\n",
      "step :  254 loss :  0.08007896691560745\n",
      "step :  255 loss :  0.08007829636335373\n",
      "step :  256 loss :  0.08006873726844788\n",
      "step :  257 loss :  0.08007697761058807\n",
      "step :  258 loss :  0.08007224649190903\n",
      "step :  259 loss :  0.08007654547691345\n",
      "step :  260 loss :  0.08008288592100143\n",
      "step :  261 loss :  0.08006367087364197\n",
      "step :  262 loss :  0.08007853478193283\n",
      "step :  263 loss :  0.08007793128490448\n",
      "step :  264 loss :  0.08008577674627304\n",
      "step :  265 loss :  0.08006621152162552\n",
      "step :  266 loss :  0.08008904755115509\n",
      "step :  267 loss :  0.08007564395666122\n",
      "step :  268 loss :  0.08006937056779861\n",
      "step :  269 loss :  0.0800769105553627\n",
      "step :  270 loss :  0.0800592452287674\n",
      "step :  271 loss :  0.08006923645734787\n",
      "step :  272 loss :  0.08006282895803452\n",
      "step :  273 loss :  0.08006118983030319\n",
      "step :  274 loss :  0.08007611334323883\n",
      "step :  275 loss :  0.0800616666674614\n",
      "step :  276 loss :  0.08006862550973892\n",
      "step :  277 loss :  0.08006599545478821\n",
      "step :  278 loss :  0.08006051182746887\n",
      "step :  279 loss :  0.0800628662109375\n",
      "step :  280 loss :  0.08006753027439117\n",
      "step :  281 loss :  0.08005502074956894\n",
      "step :  282 loss :  0.08005493879318237\n",
      "step :  283 loss :  0.08006680011749268\n",
      "step :  284 loss :  0.08006010949611664\n",
      "step :  285 loss :  0.080058254301548\n",
      "step :  286 loss :  0.08005733042955399\n",
      "step :  287 loss :  0.0800594687461853\n",
      "step :  288 loss :  0.08005401492118835\n",
      "step :  289 loss :  0.0800524428486824\n",
      "step :  290 loss :  0.08006352186203003\n",
      "step :  291 loss :  0.08004981279373169\n",
      "step :  292 loss :  0.0800568163394928\n",
      "step :  293 loss :  0.08005792647600174\n",
      "step :  294 loss :  0.08004771173000336\n",
      "step :  295 loss :  0.08005166798830032\n",
      "step :  296 loss :  0.08005427569150925\n",
      "step :  297 loss :  0.08004560321569443\n",
      "step :  298 loss :  0.08005586266517639\n",
      "step :  299 loss :  0.08005957305431366\n",
      "step :  300 loss :  0.08003734797239304\n",
      "step :  301 loss :  0.08005142956972122\n",
      "step :  302 loss :  0.08005882799625397\n",
      "step :  303 loss :  0.08004474639892578\n",
      "step :  304 loss :  0.08005037158727646\n",
      "step :  305 loss :  0.08005207777023315\n",
      "step :  306 loss :  0.08004661649465561\n",
      "step :  307 loss :  0.08005095273256302\n",
      "step :  308 loss :  0.08005490899085999\n",
      "step :  309 loss :  0.08004383742809296\n",
      "step :  310 loss :  0.08004671335220337\n",
      "step :  311 loss :  0.08005689084529877\n",
      "step :  312 loss :  0.08004806190729141\n",
      "step :  313 loss :  0.08004649728536606\n",
      "step :  314 loss :  0.08004829287528992\n",
      "step :  315 loss :  0.08004814386367798\n",
      "step :  316 loss :  0.0800442025065422\n",
      "step :  317 loss :  0.08004727214574814\n",
      "step :  318 loss :  0.0800492912530899\n",
      "step :  319 loss :  0.08004400879144669\n",
      "step :  320 loss :  0.08004680275917053\n",
      "step :  321 loss :  0.08005309849977493\n",
      "step :  322 loss :  0.0800374373793602\n",
      "step :  323 loss :  0.08004014939069748\n",
      "step :  324 loss :  0.08004841953516006\n",
      "step :  325 loss :  0.08004403859376907\n",
      "step :  326 loss :  0.08004235476255417\n",
      "step :  327 loss :  0.08004476875066757\n",
      "step :  328 loss :  0.08004502952098846\n",
      "step :  329 loss :  0.08004575967788696\n",
      "step :  330 loss :  0.08004521578550339\n",
      "step :  331 loss :  0.08004435151815414\n",
      "step :  332 loss :  0.08004441112279892\n",
      "step :  333 loss :  0.08004151284694672\n",
      "step :  334 loss :  0.08004659414291382\n",
      "step :  335 loss :  0.08004522323608398\n",
      "step :  336 loss :  0.08003649860620499\n",
      "step :  337 loss :  0.08004158735275269\n",
      "step :  338 loss :  0.08004647493362427\n",
      "step :  339 loss :  0.08003734052181244\n",
      "step :  340 loss :  0.08003915846347809\n",
      "step :  341 loss :  0.08004182577133179\n",
      "step :  342 loss :  0.08004163950681686\n",
      "step :  343 loss :  0.08004055917263031\n",
      "step :  344 loss :  0.08003956079483032\n",
      "step :  345 loss :  0.08004125952720642\n",
      "step :  346 loss :  0.0800444558262825\n",
      "step :  347 loss :  0.08004286140203476\n",
      "step :  348 loss :  0.0800405889749527\n",
      "step :  349 loss :  0.08004283159971237\n",
      "step :  350 loss :  0.08004600554704666\n",
      "step :  351 loss :  0.08003872632980347\n",
      "step :  352 loss :  0.0800400972366333\n",
      "step :  353 loss :  0.08004316687583923\n",
      "step :  354 loss :  0.08004119247198105\n",
      "step :  355 loss :  0.08003881573677063\n",
      "step :  356 loss :  0.08004038035869598\n",
      "step :  357 loss :  0.0800417885184288\n",
      "step :  358 loss :  0.08004038035869598\n",
      "step :  359 loss :  0.08004039525985718\n",
      "step :  360 loss :  0.08004126697778702\n",
      "step :  361 loss :  0.08004184067249298\n",
      "step :  362 loss :  0.0800430104136467\n",
      "step :  363 loss :  0.08004013448953629\n",
      "step :  364 loss :  0.08004066348075867\n",
      "step :  365 loss :  0.08004365116357803\n",
      "step :  366 loss :  0.08003883808851242\n",
      "step :  367 loss :  0.08004029840230942\n",
      "step :  368 loss :  0.08004118502140045\n",
      "step :  369 loss :  0.0800388753414154\n",
      "step :  370 loss :  0.08003944158554077\n",
      "step :  371 loss :  0.08004169911146164\n",
      "step :  372 loss :  0.08003895729780197\n",
      "step :  373 loss :  0.08004027605056763\n",
      "step :  374 loss :  0.08004143089056015\n",
      "step :  375 loss :  0.08003758639097214\n",
      "step :  376 loss :  0.08003876358270645\n",
      "step :  377 loss :  0.08004028350114822\n",
      "step :  378 loss :  0.08004048466682434\n",
      "step :  379 loss :  0.08003950864076614\n",
      "step :  380 loss :  0.0800395980477333\n",
      "step :  381 loss :  0.08003943413496017\n",
      "step :  382 loss :  0.08004070073366165\n",
      "step :  383 loss :  0.08004001528024673\n",
      "step :  384 loss :  0.08003980666399002\n",
      "step :  385 loss :  0.08004038035869598\n",
      "step :  386 loss :  0.08003809303045273\n",
      "step :  387 loss :  0.08003895729780197\n",
      "step :  388 loss :  0.08003994822502136\n",
      "step :  389 loss :  0.08004017174243927\n",
      "step :  390 loss :  0.08003921806812286\n",
      "step :  391 loss :  0.08003957569599152\n",
      "step :  392 loss :  0.08004048466682434\n",
      "step :  393 loss :  0.08003807067871094\n",
      "step :  394 loss :  0.08003877848386765\n",
      "step :  395 loss :  0.08004022389650345\n",
      "step :  396 loss :  0.08003856986761093\n",
      "step :  397 loss :  0.08003905415534973\n",
      "step :  398 loss :  0.08003895729780197\n",
      "step :  399 loss :  0.08003829419612885\n",
      "step :  400 loss :  0.08003921806812286\n",
      "step :  401 loss :  0.08003938943147659\n",
      "step :  402 loss :  0.08003652840852737\n",
      "step :  403 loss :  0.08003759384155273\n",
      "step :  404 loss :  0.08003973215818405\n",
      "step :  405 loss :  0.08003810793161392\n",
      "step :  406 loss :  0.08003796637058258\n",
      "step :  407 loss :  0.08003868162631989\n",
      "step :  408 loss :  0.08003900200128555\n",
      "step :  409 loss :  0.08003678172826767\n",
      "step :  410 loss :  0.08003698289394379\n",
      "step :  411 loss :  0.08003787696361542\n",
      "step :  412 loss :  0.0800384134054184\n",
      "step :  413 loss :  0.08003829419612885\n",
      "step :  414 loss :  0.08003778755664825\n",
      "step :  415 loss :  0.08003868907690048\n",
      "step :  416 loss :  0.08003843575716019\n",
      "step :  417 loss :  0.08003800362348557\n",
      "step :  418 loss :  0.08003848791122437\n",
      "step :  419 loss :  0.08003883808851242\n",
      "step :  420 loss :  0.08003780245780945\n",
      "step :  421 loss :  0.08003807812929153\n",
      "step :  422 loss :  0.0800386518239975\n",
      "step :  423 loss :  0.08003761619329453\n",
      "step :  424 loss :  0.08003780990839005\n",
      "step :  425 loss :  0.08003803342580795\n",
      "step :  426 loss :  0.08003818988800049\n",
      "step :  427 loss :  0.08003826439380646\n",
      "step :  428 loss :  0.08003774285316467\n",
      "step :  429 loss :  0.08003876358270645\n",
      "step :  430 loss :  0.0800383910536766\n",
      "step :  431 loss :  0.08003713935613632\n",
      "step :  432 loss :  0.08003698289394379\n",
      "step :  433 loss :  0.08003756403923035\n",
      "step :  434 loss :  0.08003795892000198\n",
      "step :  435 loss :  0.08003769814968109\n",
      "step :  436 loss :  0.08003777265548706\n",
      "step :  437 loss :  0.0800381526350975\n",
      "step :  438 loss :  0.08003758639097214\n",
      "step :  439 loss :  0.08003797382116318\n",
      "step :  440 loss :  0.08003835380077362\n",
      "step :  441 loss :  0.08003729581832886\n",
      "step :  442 loss :  0.08003757148981094\n",
      "step :  443 loss :  0.08003802597522736\n",
      "step :  444 loss :  0.08003772795200348\n",
      "step :  445 loss :  0.08003765344619751\n",
      "step :  446 loss :  0.08003778755664825\n",
      "step :  447 loss :  0.08003752678632736\n",
      "step :  448 loss :  0.08003813028335571\n",
      "step :  449 loss :  0.08003778755664825\n",
      "step :  450 loss :  0.08003697544336319\n",
      "step :  451 loss :  0.08003702014684677\n",
      "step :  452 loss :  0.08003753423690796\n",
      "step :  453 loss :  0.08003762364387512\n",
      "step :  454 loss :  0.08003732562065125\n",
      "step :  455 loss :  0.08003747463226318\n",
      "step :  456 loss :  0.08003804832696915\n",
      "step :  457 loss :  0.08003755658864975\n",
      "step :  458 loss :  0.08003722131252289\n",
      "step :  459 loss :  0.08003736287355423\n",
      "step :  460 loss :  0.08003770560026169\n",
      "step :  461 loss :  0.08003748208284378\n",
      "step :  462 loss :  0.08003745228052139\n",
      "step :  463 loss :  0.08003760129213333\n",
      "step :  464 loss :  0.080037422478199\n",
      "step :  465 loss :  0.08003756403923035\n",
      "step :  466 loss :  0.08003734797239304\n",
      "step :  467 loss :  0.0800376906991005\n",
      "step :  468 loss :  0.08003737777471542\n",
      "step :  469 loss :  0.08003709465265274\n",
      "step :  470 loss :  0.08003716170787811\n",
      "step :  471 loss :  0.08003762364387512\n",
      "step :  472 loss :  0.08003749698400497\n",
      "step :  473 loss :  0.08003708720207214\n",
      "step :  474 loss :  0.08003732562065125\n",
      "step :  475 loss :  0.08003763854503632\n",
      "step :  476 loss :  0.0800371766090393\n",
      "step :  477 loss :  0.08003690838813782\n",
      "step :  478 loss :  0.0800371915102005\n",
      "step :  479 loss :  0.08003752678632736\n",
      "step :  480 loss :  0.08003740012645721\n",
      "step :  481 loss :  0.0800371840596199\n",
      "step :  482 loss :  0.0800371840596199\n",
      "step :  483 loss :  0.08003727346658707\n",
      "step :  484 loss :  0.0800374373793602\n",
      "step :  485 loss :  0.08003707975149155\n",
      "step :  486 loss :  0.08003715425729752\n",
      "step :  487 loss :  0.08003727346658707\n",
      "step :  488 loss :  0.08003716170787811\n",
      "step :  489 loss :  0.0800371989607811\n",
      "step :  490 loss :  0.0800369456410408\n",
      "step :  491 loss :  0.08003708720207214\n",
      "step :  492 loss :  0.08003738522529602\n",
      "step :  493 loss :  0.08003707975149155\n",
      "step :  494 loss :  0.08003711700439453\n",
      "step :  495 loss :  0.08003725111484528\n",
      "step :  496 loss :  0.08003710955381393\n",
      "step :  497 loss :  0.08003722876310349\n",
      "step :  498 loss :  0.08003701269626617\n",
      "step :  499 loss :  0.08003698289394379\n",
      "Training model 5\n",
      "step :  0 loss :  0.31232208013534546\n",
      "step :  1 loss :  0.26899254322052\n",
      "step :  2 loss :  0.20440058410167694\n",
      "step :  3 loss :  0.21823470294475555\n",
      "step :  4 loss :  0.19164791703224182\n",
      "step :  5 loss :  0.22100138664245605\n",
      "step :  6 loss :  0.21620450913906097\n",
      "step :  7 loss :  0.21573558449745178\n",
      "step :  8 loss :  0.18578551709651947\n",
      "step :  9 loss :  0.21211402118206024\n",
      "step :  10 loss :  0.18665385246276855\n",
      "step :  11 loss :  0.16903693974018097\n",
      "step :  12 loss :  0.1636868268251419\n",
      "step :  13 loss :  0.15298229455947876\n",
      "step :  14 loss :  0.14142683148384094\n",
      "step :  15 loss :  0.14486858248710632\n",
      "step :  16 loss :  0.1407485455274582\n",
      "step :  17 loss :  0.3416330814361572\n",
      "step :  18 loss :  0.10904142260551453\n",
      "step :  19 loss :  0.15316584706306458\n",
      "step :  20 loss :  0.17575789988040924\n",
      "step :  21 loss :  0.12295995652675629\n",
      "step :  22 loss :  0.13417315483093262\n",
      "step :  23 loss :  0.12198786437511444\n",
      "step :  24 loss :  0.11105432361364365\n",
      "step :  25 loss :  0.10238111764192581\n",
      "step :  26 loss :  0.09639699012041092\n",
      "step :  27 loss :  0.10722999274730682\n",
      "step :  28 loss :  0.11911403387784958\n",
      "step :  29 loss :  0.10923568159341812\n",
      "step :  30 loss :  0.10574110597372055\n",
      "step :  31 loss :  0.0977887362241745\n",
      "step :  32 loss :  0.09842699021100998\n",
      "step :  33 loss :  0.09593862295150757\n",
      "step :  34 loss :  0.10100408643484116\n",
      "step :  35 loss :  0.1442318707704544\n",
      "step :  36 loss :  0.08977923542261124\n",
      "step :  37 loss :  0.09291292726993561\n",
      "step :  38 loss :  0.08637483417987823\n",
      "step :  39 loss :  0.12619920074939728\n",
      "step :  40 loss :  0.10826235264539719\n",
      "step :  41 loss :  0.08611986041069031\n",
      "step :  42 loss :  0.08663855493068695\n",
      "step :  43 loss :  0.09994146227836609\n",
      "step :  44 loss :  0.08612647652626038\n",
      "step :  45 loss :  0.09114191681146622\n",
      "step :  46 loss :  0.08495080471038818\n",
      "step :  47 loss :  0.08493934571743011\n",
      "step :  48 loss :  0.0841783732175827\n",
      "step :  49 loss :  0.09896906465291977\n",
      "step :  50 loss :  0.10547987371683121\n",
      "step :  51 loss :  0.08563779294490814\n",
      "step :  52 loss :  0.08593995124101639\n",
      "step :  53 loss :  0.08541328459978104\n",
      "step :  54 loss :  0.09113899618387222\n",
      "step :  55 loss :  0.09217551350593567\n",
      "step :  56 loss :  0.08534187078475952\n",
      "step :  57 loss :  0.09070983529090881\n",
      "step :  58 loss :  0.08525748550891876\n",
      "step :  59 loss :  0.08360015600919724\n",
      "step :  60 loss :  0.08528324216604233\n",
      "step :  61 loss :  0.09225393086671829\n",
      "step :  62 loss :  0.08312277495861053\n",
      "step :  63 loss :  0.08631178736686707\n",
      "step :  64 loss :  0.08302248269319534\n",
      "step :  65 loss :  0.0830993577837944\n",
      "step :  66 loss :  0.08215942233800888\n",
      "step :  67 loss :  0.08242607116699219\n",
      "step :  68 loss :  0.0916767343878746\n",
      "step :  69 loss :  0.08539621531963348\n",
      "step :  70 loss :  0.08581902086734772\n",
      "step :  71 loss :  0.08194226026535034\n",
      "step :  72 loss :  0.08346384763717651\n",
      "step :  73 loss :  0.08357279747724533\n",
      "step :  74 loss :  0.08109045773744583\n",
      "step :  75 loss :  0.08413783460855484\n",
      "step :  76 loss :  0.08232969045639038\n",
      "step :  77 loss :  0.08167055249214172\n",
      "step :  78 loss :  0.08462511003017426\n",
      "step :  79 loss :  0.08135425299406052\n",
      "step :  80 loss :  0.08188360184431076\n",
      "step :  81 loss :  0.08125071227550507\n",
      "step :  82 loss :  0.08743693679571152\n",
      "step :  83 loss :  0.08292660862207413\n",
      "step :  84 loss :  0.08205801248550415\n",
      "step :  85 loss :  0.08356098085641861\n",
      "step :  86 loss :  0.08532212674617767\n",
      "step :  87 loss :  0.08493070304393768\n",
      "step :  88 loss :  0.08258488774299622\n",
      "step :  89 loss :  0.08279682695865631\n",
      "step :  90 loss :  0.08296813070774078\n",
      "step :  91 loss :  0.08447854965925217\n",
      "step :  92 loss :  0.08195571601390839\n",
      "step :  93 loss :  0.08147233724594116\n",
      "step :  94 loss :  0.08112037181854248\n",
      "step :  95 loss :  0.08474965393543243\n",
      "step :  96 loss :  0.08215726912021637\n",
      "step :  97 loss :  0.08100920915603638\n",
      "step :  98 loss :  0.08207672834396362\n",
      "step :  99 loss :  0.081882543861866\n",
      "step :  100 loss :  0.0828387439250946\n",
      "step :  101 loss :  0.0821908712387085\n",
      "step :  102 loss :  0.08147910982370377\n",
      "step :  103 loss :  0.08186247944831848\n",
      "step :  104 loss :  0.08175119757652283\n",
      "step :  105 loss :  0.08285495638847351\n",
      "step :  106 loss :  0.08326398581266403\n",
      "step :  107 loss :  0.08245109021663666\n",
      "step :  108 loss :  0.08145970106124878\n",
      "step :  109 loss :  0.08198375254869461\n",
      "step :  110 loss :  0.08057601004838943\n",
      "step :  111 loss :  0.08073204755783081\n",
      "step :  112 loss :  0.08033468574285507\n",
      "step :  113 loss :  0.08033983409404755\n",
      "step :  114 loss :  0.08140897005796432\n",
      "step :  115 loss :  0.08028603345155716\n",
      "step :  116 loss :  0.08022168278694153\n",
      "step :  117 loss :  0.08045566827058792\n",
      "step :  118 loss :  0.0803377702832222\n",
      "step :  119 loss :  0.08059542626142502\n",
      "step :  120 loss :  0.08105497807264328\n",
      "step :  121 loss :  0.08043527603149414\n",
      "step :  122 loss :  0.08059757202863693\n",
      "step :  123 loss :  0.08016866445541382\n",
      "step :  124 loss :  0.08041341602802277\n",
      "step :  125 loss :  0.08001285791397095\n",
      "step :  126 loss :  0.0802234560251236\n",
      "step :  127 loss :  0.080661840736866\n",
      "step :  128 loss :  0.08042466640472412\n",
      "step :  129 loss :  0.08126138895750046\n",
      "step :  130 loss :  0.08009626716375351\n",
      "step :  131 loss :  0.0806855857372284\n",
      "step :  132 loss :  0.07997934520244598\n",
      "step :  133 loss :  0.08070066571235657\n",
      "step :  134 loss :  0.07990704476833344\n",
      "step :  135 loss :  0.0803588256239891\n",
      "step :  136 loss :  0.08014880865812302\n",
      "step :  137 loss :  0.08045031875371933\n",
      "step :  138 loss :  0.0798589214682579\n",
      "step :  139 loss :  0.08060779422521591\n",
      "step :  140 loss :  0.08018224686384201\n",
      "step :  141 loss :  0.08091586083173752\n",
      "step :  142 loss :  0.07991322129964828\n",
      "step :  143 loss :  0.08038938045501709\n",
      "step :  144 loss :  0.07981325685977936\n",
      "step :  145 loss :  0.0805601254105568\n",
      "step :  146 loss :  0.07987286150455475\n",
      "step :  147 loss :  0.08062439411878586\n",
      "step :  148 loss :  0.0798773542046547\n",
      "step :  149 loss :  0.080130435526371\n",
      "step :  150 loss :  0.07977554947137833\n",
      "step :  151 loss :  0.07992345839738846\n",
      "step :  152 loss :  0.07973286509513855\n",
      "step :  153 loss :  0.07979691028594971\n",
      "step :  154 loss :  0.08001097291707993\n",
      "step :  155 loss :  0.07982627302408218\n",
      "step :  156 loss :  0.08003373444080353\n",
      "step :  157 loss :  0.07977166026830673\n",
      "step :  158 loss :  0.07983456552028656\n",
      "step :  159 loss :  0.07977299392223358\n",
      "step :  160 loss :  0.07969833165407181\n",
      "step :  161 loss :  0.07969123125076294\n",
      "step :  162 loss :  0.0796717032790184\n",
      "step :  163 loss :  0.07971540093421936\n",
      "step :  164 loss :  0.07976971566677094\n",
      "step :  165 loss :  0.07969077676534653\n",
      "step :  166 loss :  0.07965736091136932\n",
      "step :  167 loss :  0.07965736091136932\n",
      "step :  168 loss :  0.0797717273235321\n",
      "step :  169 loss :  0.07962428778409958\n",
      "step :  170 loss :  0.07995829731225967\n",
      "step :  171 loss :  0.07963837683200836\n",
      "step :  172 loss :  0.07975471019744873\n",
      "step :  173 loss :  0.07966528087854385\n",
      "step :  174 loss :  0.07959891110658646\n",
      "step :  175 loss :  0.07960920035839081\n",
      "step :  176 loss :  0.07957255840301514\n",
      "step :  177 loss :  0.07961666584014893\n",
      "step :  178 loss :  0.07958081364631653\n",
      "step :  179 loss :  0.0797678604722023\n",
      "step :  180 loss :  0.07956691086292267\n",
      "step :  181 loss :  0.07964048534631729\n",
      "step :  182 loss :  0.07967685163021088\n",
      "step :  183 loss :  0.07955346256494522\n",
      "step :  184 loss :  0.0797135978937149\n",
      "step :  185 loss :  0.07957260310649872\n",
      "step :  186 loss :  0.07966826111078262\n",
      "step :  187 loss :  0.07964485883712769\n",
      "step :  188 loss :  0.07954349368810654\n",
      "step :  189 loss :  0.07961972802877426\n",
      "step :  190 loss :  0.07954532653093338\n",
      "step :  191 loss :  0.0795903429389\n",
      "step :  192 loss :  0.07963162660598755\n",
      "step :  193 loss :  0.0795028805732727\n",
      "step :  194 loss :  0.0797378420829773\n",
      "step :  195 loss :  0.07959850132465363\n",
      "step :  196 loss :  0.07950391620397568\n",
      "step :  197 loss :  0.07957422733306885\n",
      "step :  198 loss :  0.0795355811715126\n",
      "step :  199 loss :  0.07951691001653671\n",
      "step :  200 loss :  0.07951369136571884\n",
      "step :  201 loss :  0.07951810956001282\n",
      "step :  202 loss :  0.07959301024675369\n",
      "step :  203 loss :  0.07949253916740417\n",
      "step :  204 loss :  0.07952320575714111\n",
      "step :  205 loss :  0.07948289066553116\n",
      "step :  206 loss :  0.07947760820388794\n",
      "step :  207 loss :  0.0794774666428566\n",
      "step :  208 loss :  0.07946275174617767\n",
      "step :  209 loss :  0.07956448942422867\n",
      "step :  210 loss :  0.07946779578924179\n",
      "step :  211 loss :  0.07948903739452362\n",
      "step :  212 loss :  0.07952037453651428\n",
      "step :  213 loss :  0.07945732772350311\n",
      "step :  214 loss :  0.0795583724975586\n",
      "step :  215 loss :  0.07944713532924652\n",
      "step :  216 loss :  0.07954726368188858\n",
      "step :  217 loss :  0.07950880378484726\n",
      "step :  218 loss :  0.07944466918706894\n",
      "step :  219 loss :  0.07952212542295456\n",
      "step :  220 loss :  0.07944927364587784\n",
      "step :  221 loss :  0.07944376021623611\n",
      "step :  222 loss :  0.07944822311401367\n",
      "step :  223 loss :  0.07944919914007187\n",
      "step :  224 loss :  0.07943934202194214\n",
      "step :  225 loss :  0.07943149656057358\n",
      "step :  226 loss :  0.07948439568281174\n",
      "step :  227 loss :  0.07943975925445557\n",
      "step :  228 loss :  0.07944381237030029\n",
      "step :  229 loss :  0.07944557070732117\n",
      "step :  230 loss :  0.07944308966398239\n",
      "step :  231 loss :  0.07944383472204208\n",
      "step :  232 loss :  0.07943861186504364\n",
      "step :  233 loss :  0.07942807674407959\n",
      "step :  234 loss :  0.07945849746465683\n",
      "step :  235 loss :  0.07942068576812744\n",
      "step :  236 loss :  0.07946472615003586\n",
      "step :  237 loss :  0.07941457629203796\n",
      "step :  238 loss :  0.07945426553487778\n",
      "step :  239 loss :  0.07945158332586288\n",
      "step :  240 loss :  0.07944216579198837\n",
      "step :  241 loss :  0.07942400872707367\n",
      "step :  242 loss :  0.07944212853908539\n",
      "step :  243 loss :  0.07942076027393341\n",
      "step :  244 loss :  0.07941176742315292\n",
      "step :  245 loss :  0.07942552864551544\n",
      "step :  246 loss :  0.0794423297047615\n",
      "step :  247 loss :  0.07940684258937836\n",
      "step :  248 loss :  0.07942929118871689\n",
      "step :  249 loss :  0.07941774278879166\n",
      "step :  250 loss :  0.07940246909856796\n",
      "step :  251 loss :  0.07942599803209305\n",
      "step :  252 loss :  0.07941991835832596\n",
      "step :  253 loss :  0.07940685003995895\n",
      "step :  254 loss :  0.07943596690893173\n",
      "step :  255 loss :  0.07940702885389328\n",
      "step :  256 loss :  0.07940410077571869\n",
      "step :  257 loss :  0.07940530776977539\n",
      "step :  258 loss :  0.07940516620874405\n",
      "step :  259 loss :  0.07939521223306656\n",
      "step :  260 loss :  0.07941699773073196\n",
      "step :  261 loss :  0.07940831780433655\n",
      "step :  262 loss :  0.07939313352108002\n",
      "step :  263 loss :  0.07942932099103928\n",
      "step :  264 loss :  0.07941017299890518\n",
      "step :  265 loss :  0.0793948620557785\n",
      "step :  266 loss :  0.07940250635147095\n",
      "step :  267 loss :  0.07940443605184555\n",
      "step :  268 loss :  0.07939239591360092\n",
      "step :  269 loss :  0.07939926534891129\n",
      "step :  270 loss :  0.07939018309116364\n",
      "step :  271 loss :  0.07939375936985016\n",
      "step :  272 loss :  0.07939060032367706\n",
      "step :  273 loss :  0.07939755916595459\n",
      "step :  274 loss :  0.0793895423412323\n",
      "step :  275 loss :  0.07941989600658417\n",
      "step :  276 loss :  0.07939772307872772\n",
      "step :  277 loss :  0.07938960939645767\n",
      "step :  278 loss :  0.07940465956926346\n",
      "step :  279 loss :  0.07939232885837555\n",
      "step :  280 loss :  0.07939228415489197\n",
      "step :  281 loss :  0.07939989119768143\n",
      "step :  282 loss :  0.07938635349273682\n",
      "step :  283 loss :  0.07938671112060547\n",
      "step :  284 loss :  0.0793905034661293\n",
      "step :  285 loss :  0.0793839618563652\n",
      "step :  286 loss :  0.07938645035028458\n",
      "step :  287 loss :  0.07938499748706818\n",
      "step :  288 loss :  0.07938046753406525\n",
      "step :  289 loss :  0.07937853783369064\n",
      "step :  290 loss :  0.07939180731773376\n",
      "step :  291 loss :  0.07937931269407272\n",
      "step :  292 loss :  0.07937806844711304\n",
      "step :  293 loss :  0.07938695698976517\n",
      "step :  294 loss :  0.07938762754201889\n",
      "step :  295 loss :  0.07938135415315628\n",
      "step :  296 loss :  0.0793795958161354\n",
      "step :  297 loss :  0.07938617467880249\n",
      "step :  298 loss :  0.07938215136528015\n",
      "step :  299 loss :  0.07937848567962646\n",
      "step :  300 loss :  0.07937667518854141\n",
      "step :  301 loss :  0.07937881350517273\n",
      "step :  302 loss :  0.07937637716531754\n",
      "step :  303 loss :  0.07937560975551605\n",
      "step :  304 loss :  0.0793786346912384\n",
      "step :  305 loss :  0.07937276363372803\n",
      "step :  306 loss :  0.07938146591186523\n",
      "step :  307 loss :  0.07938377559185028\n",
      "step :  308 loss :  0.07937214523553848\n",
      "step :  309 loss :  0.07938124239444733\n",
      "step :  310 loss :  0.07938037812709808\n",
      "step :  311 loss :  0.07937442511320114\n",
      "step :  312 loss :  0.07937131822109222\n",
      "step :  313 loss :  0.07938161492347717\n",
      "step :  314 loss :  0.07937246561050415\n",
      "step :  315 loss :  0.07937369495630264\n",
      "step :  316 loss :  0.07938158512115479\n",
      "step :  317 loss :  0.07937302440404892\n",
      "step :  318 loss :  0.07937128096818924\n",
      "step :  319 loss :  0.07937351614236832\n",
      "step :  320 loss :  0.07937595993280411\n",
      "step :  321 loss :  0.07936983555555344\n",
      "step :  322 loss :  0.07937398552894592\n",
      "step :  323 loss :  0.07937520742416382\n",
      "step :  324 loss :  0.07937245070934296\n",
      "step :  325 loss :  0.07937485724687576\n",
      "step :  326 loss :  0.07936975359916687\n",
      "step :  327 loss :  0.07937226444482803\n",
      "step :  328 loss :  0.07936912775039673\n",
      "step :  329 loss :  0.07937361299991608\n",
      "step :  330 loss :  0.07936933636665344\n",
      "step :  331 loss :  0.07937312871217728\n",
      "step :  332 loss :  0.0793711319565773\n",
      "step :  333 loss :  0.07936907559633255\n",
      "step :  334 loss :  0.0793733298778534\n",
      "step :  335 loss :  0.07937437295913696\n",
      "step :  336 loss :  0.07936766743659973\n",
      "step :  337 loss :  0.07936769723892212\n",
      "step :  338 loss :  0.0793725922703743\n",
      "step :  339 loss :  0.07937370240688324\n",
      "step :  340 loss :  0.07937118411064148\n",
      "step :  341 loss :  0.07936760038137436\n",
      "step :  342 loss :  0.07937061041593552\n",
      "step :  343 loss :  0.07937425374984741\n",
      "step :  344 loss :  0.07936763018369675\n",
      "step :  345 loss :  0.07936893403530121\n",
      "step :  346 loss :  0.07936745882034302\n",
      "step :  347 loss :  0.07936573028564453\n",
      "step :  348 loss :  0.07936765998601913\n",
      "step :  349 loss :  0.07936837524175644\n",
      "step :  350 loss :  0.07936562597751617\n",
      "step :  351 loss :  0.07937109470367432\n",
      "step :  352 loss :  0.0793682336807251\n",
      "step :  353 loss :  0.07936449348926544\n",
      "step :  354 loss :  0.07936634868383408\n",
      "step :  355 loss :  0.07936998456716537\n",
      "step :  356 loss :  0.0793701782822609\n",
      "step :  357 loss :  0.07936601340770721\n",
      "step :  358 loss :  0.07936470955610275\n",
      "step :  359 loss :  0.07936430722475052\n",
      "step :  360 loss :  0.0793684720993042\n",
      "step :  361 loss :  0.07936529070138931\n",
      "step :  362 loss :  0.07936487346887589\n",
      "step :  363 loss :  0.07936672121286392\n",
      "step :  364 loss :  0.07936518639326096\n",
      "step :  365 loss :  0.07936408370733261\n",
      "step :  366 loss :  0.07936718314886093\n",
      "step :  367 loss :  0.07936788350343704\n",
      "step :  368 loss :  0.07936432957649231\n",
      "step :  369 loss :  0.07936561852693558\n",
      "step :  370 loss :  0.07936642318964005\n",
      "step :  371 loss :  0.07936756312847137\n",
      "step :  372 loss :  0.07936547696590424\n",
      "step :  373 loss :  0.07936374098062515\n",
      "step :  374 loss :  0.07936554402112961\n",
      "step :  375 loss :  0.0793633684515953\n",
      "step :  376 loss :  0.0793653354048729\n",
      "step :  377 loss :  0.07936624437570572\n",
      "step :  378 loss :  0.07936480641365051\n",
      "step :  379 loss :  0.07936443388462067\n",
      "step :  380 loss :  0.07936598360538483\n",
      "step :  381 loss :  0.07936342805624008\n",
      "step :  382 loss :  0.07936400920152664\n",
      "step :  383 loss :  0.0793626606464386\n",
      "step :  384 loss :  0.0793643370270729\n",
      "step :  385 loss :  0.0793662741780281\n",
      "step :  386 loss :  0.07936392724514008\n",
      "step :  387 loss :  0.0793643593788147\n",
      "step :  388 loss :  0.07936364412307739\n",
      "step :  389 loss :  0.07936345785856247\n",
      "step :  390 loss :  0.07936280220746994\n",
      "step :  391 loss :  0.07936307787895203\n",
      "step :  392 loss :  0.07936419546604156\n",
      "step :  393 loss :  0.07936251163482666\n",
      "step :  394 loss :  0.07936308532953262\n",
      "step :  395 loss :  0.07936269789934158\n",
      "step :  396 loss :  0.07936375588178635\n",
      "step :  397 loss :  0.07936394214630127\n",
      "step :  398 loss :  0.07936340570449829\n",
      "step :  399 loss :  0.07936367392539978\n",
      "step :  400 loss :  0.0793621689081192\n",
      "step :  401 loss :  0.07936371862888336\n",
      "step :  402 loss :  0.07936349511146545\n",
      "step :  403 loss :  0.07936210930347443\n",
      "step :  404 loss :  0.07936359941959381\n",
      "step :  405 loss :  0.07936207205057144\n",
      "step :  406 loss :  0.07936328649520874\n",
      "step :  407 loss :  0.07936257123947144\n",
      "step :  408 loss :  0.0793629065155983\n",
      "step :  409 loss :  0.07936328649520874\n",
      "step :  410 loss :  0.07936311513185501\n",
      "step :  411 loss :  0.07936229556798935\n",
      "step :  412 loss :  0.07936202734708786\n",
      "step :  413 loss :  0.07936321198940277\n",
      "step :  414 loss :  0.07936294376850128\n",
      "step :  415 loss :  0.07936125993728638\n",
      "step :  416 loss :  0.0793619155883789\n",
      "step :  417 loss :  0.07936292886734009\n",
      "step :  418 loss :  0.07936245203018188\n",
      "step :  419 loss :  0.07936165481805801\n",
      "step :  420 loss :  0.0793614387512207\n",
      "step :  421 loss :  0.0793628916144371\n",
      "step :  422 loss :  0.07936243712902069\n",
      "step :  423 loss :  0.07936202734708786\n",
      "step :  424 loss :  0.0793614611029625\n",
      "step :  425 loss :  0.07936152070760727\n",
      "step :  426 loss :  0.07936203479766846\n",
      "step :  427 loss :  0.07936172932386398\n",
      "step :  428 loss :  0.07936129719018936\n",
      "step :  429 loss :  0.0793621689081192\n",
      "step :  430 loss :  0.07936229556798935\n",
      "step :  431 loss :  0.0793614313006401\n",
      "step :  432 loss :  0.0793624222278595\n",
      "step :  433 loss :  0.07936195284128189\n",
      "step :  434 loss :  0.07936133444309235\n",
      "step :  435 loss :  0.07936207205057144\n",
      "step :  436 loss :  0.07936269789934158\n",
      "step :  437 loss :  0.07936214655637741\n",
      "step :  438 loss :  0.07936117053031921\n",
      "step :  439 loss :  0.07936153560876846\n",
      "step :  440 loss :  0.07936139404773712\n",
      "step :  441 loss :  0.07936104387044907\n",
      "step :  442 loss :  0.07936159521341324\n",
      "step :  443 loss :  0.07936140149831772\n",
      "step :  444 loss :  0.07936130464076996\n",
      "step :  445 loss :  0.07936133444309235\n",
      "step :  446 loss :  0.07936102896928787\n",
      "step :  447 loss :  0.07936161011457443\n",
      "step :  448 loss :  0.07936109602451324\n",
      "step :  449 loss :  0.07936114072799683\n",
      "step :  450 loss :  0.07936103641986847\n",
      "step :  451 loss :  0.07936141639947891\n",
      "step :  452 loss :  0.0793612152338028\n",
      "step :  453 loss :  0.07936100661754608\n",
      "step :  454 loss :  0.07936134189367294\n",
      "step :  455 loss :  0.07936141639947891\n",
      "step :  456 loss :  0.07936103641986847\n",
      "step :  457 loss :  0.07936128228902817\n",
      "step :  458 loss :  0.07936098426580429\n",
      "step :  459 loss :  0.07936159521341324\n",
      "step :  460 loss :  0.07936099171638489\n",
      "step :  461 loss :  0.0793607160449028\n",
      "step :  462 loss :  0.07936102151870728\n",
      "step :  463 loss :  0.07936146855354309\n",
      "step :  464 loss :  0.07936114817857742\n",
      "step :  465 loss :  0.0793607160449028\n",
      "step :  466 loss :  0.079361192882061\n",
      "step :  467 loss :  0.07936100661754608\n",
      "step :  468 loss :  0.07936081290245056\n",
      "step :  469 loss :  0.07936090230941772\n",
      "step :  470 loss :  0.07936109602451324\n",
      "step :  471 loss :  0.07936079055070877\n",
      "step :  472 loss :  0.0793609619140625\n",
      "step :  473 loss :  0.0793609619140625\n",
      "step :  474 loss :  0.07936102151870728\n",
      "step :  475 loss :  0.07936100661754608\n",
      "step :  476 loss :  0.07936066389083862\n",
      "step :  477 loss :  0.07936091721057892\n",
      "step :  478 loss :  0.07936075329780579\n",
      "step :  479 loss :  0.07936060428619385\n",
      "step :  480 loss :  0.07936086505651474\n",
      "step :  481 loss :  0.07936067134141922\n",
      "step :  482 loss :  0.07936074584722519\n",
      "step :  483 loss :  0.07936069369316101\n",
      "step :  484 loss :  0.079360730946064\n",
      "step :  485 loss :  0.07936082035303116\n",
      "step :  486 loss :  0.07936080545186996\n",
      "step :  487 loss :  0.07936059683561325\n",
      "step :  488 loss :  0.07936093956232071\n",
      "step :  489 loss :  0.07936073839664459\n",
      "step :  490 loss :  0.07936055213212967\n",
      "step :  491 loss :  0.07936086505651474\n",
      "step :  492 loss :  0.07936107367277145\n",
      "step :  493 loss :  0.07936078310012817\n",
      "step :  494 loss :  0.07936052232980728\n",
      "step :  495 loss :  0.07936062663793564\n",
      "step :  496 loss :  0.0793607160449028\n",
      "step :  497 loss :  0.0793607160449028\n",
      "step :  498 loss :  0.07936052978038788\n",
      "step :  499 loss :  0.0793604850769043\n",
      "Training model 6\n",
      "step :  0 loss :  0.26822227239608765\n",
      "step :  1 loss :  0.27817168831825256\n",
      "step :  2 loss :  0.21751125156879425\n",
      "step :  3 loss :  0.2260512113571167\n",
      "step :  4 loss :  0.18341988325119019\n",
      "step :  5 loss :  0.21333864331245422\n",
      "step :  6 loss :  0.22154448926448822\n",
      "step :  7 loss :  0.22418856620788574\n",
      "step :  8 loss :  0.19830095767974854\n",
      "step :  9 loss :  0.21910345554351807\n",
      "step :  10 loss :  0.19170036911964417\n",
      "step :  11 loss :  0.16974598169326782\n",
      "step :  12 loss :  0.16738072037696838\n",
      "step :  13 loss :  0.16268181800842285\n",
      "step :  14 loss :  0.14660106599330902\n",
      "step :  15 loss :  0.1318073719739914\n",
      "step :  16 loss :  0.134874626994133\n",
      "step :  17 loss :  0.18360942602157593\n",
      "step :  18 loss :  0.28031426668167114\n",
      "step :  19 loss :  0.1646943986415863\n",
      "step :  20 loss :  0.11935565620660782\n",
      "step :  21 loss :  0.13484585285186768\n",
      "step :  22 loss :  0.13546115159988403\n",
      "step :  23 loss :  0.11276718229055405\n",
      "step :  24 loss :  0.10958191752433777\n",
      "step :  25 loss :  0.10413423925638199\n",
      "step :  26 loss :  0.10211152583360672\n",
      "step :  27 loss :  0.13204367458820343\n",
      "step :  28 loss :  0.1026771292090416\n",
      "step :  29 loss :  0.10108207911252975\n",
      "step :  30 loss :  0.09926725178956985\n",
      "step :  31 loss :  0.10077033936977386\n",
      "step :  32 loss :  0.10085111856460571\n",
      "step :  33 loss :  0.10877877473831177\n",
      "step :  34 loss :  0.14639189839363098\n",
      "step :  35 loss :  0.0951850637793541\n",
      "step :  36 loss :  0.09360796213150024\n",
      "step :  37 loss :  0.08884396404027939\n",
      "step :  38 loss :  0.09130100905895233\n",
      "step :  39 loss :  0.0907641127705574\n",
      "step :  40 loss :  0.10814815759658813\n",
      "step :  41 loss :  0.10861910879611969\n",
      "step :  42 loss :  0.08646364510059357\n",
      "step :  43 loss :  0.0938691794872284\n",
      "step :  44 loss :  0.09021705389022827\n",
      "step :  45 loss :  0.08647849410772324\n",
      "step :  46 loss :  0.09379120171070099\n",
      "step :  47 loss :  0.09370972216129303\n",
      "step :  48 loss :  0.1190216988325119\n",
      "step :  49 loss :  0.09193132072687149\n",
      "step :  50 loss :  0.10506170243024826\n",
      "step :  51 loss :  0.1034512147307396\n",
      "step :  52 loss :  0.09219583123922348\n",
      "step :  53 loss :  0.0865294486284256\n",
      "step :  54 loss :  0.08897380530834198\n",
      "step :  55 loss :  0.09752566367387772\n",
      "step :  56 loss :  0.09552940726280212\n",
      "step :  57 loss :  0.08610508590936661\n",
      "step :  58 loss :  0.08512186259031296\n",
      "step :  59 loss :  0.08523450046777725\n",
      "step :  60 loss :  0.0946083664894104\n",
      "step :  61 loss :  0.0843839943408966\n",
      "step :  62 loss :  0.08409519493579865\n",
      "step :  63 loss :  0.08923941850662231\n",
      "step :  64 loss :  0.08344533294439316\n",
      "step :  65 loss :  0.08560438454151154\n",
      "step :  66 loss :  0.08369028568267822\n",
      "step :  67 loss :  0.08379412442445755\n",
      "step :  68 loss :  0.0827232077717781\n",
      "step :  69 loss :  0.08497867733240128\n",
      "step :  70 loss :  0.08333642035722733\n",
      "step :  71 loss :  0.0847429558634758\n",
      "step :  72 loss :  0.08418126404285431\n",
      "step :  73 loss :  0.08378608524799347\n",
      "step :  74 loss :  0.08899795264005661\n",
      "step :  75 loss :  0.08288687467575073\n",
      "step :  76 loss :  0.08318687230348587\n",
      "step :  77 loss :  0.08352285623550415\n",
      "step :  78 loss :  0.0823434367775917\n",
      "step :  79 loss :  0.08328885585069656\n",
      "step :  80 loss :  0.08744566887617111\n",
      "step :  81 loss :  0.08527525514364243\n",
      "step :  82 loss :  0.08694854378700256\n",
      "step :  83 loss :  0.08468333631753922\n",
      "step :  84 loss :  0.08451952785253525\n",
      "step :  85 loss :  0.08287805318832397\n",
      "step :  86 loss :  0.08673107624053955\n",
      "step :  87 loss :  0.08566596359014511\n",
      "step :  88 loss :  0.08228690177202225\n",
      "step :  89 loss :  0.08264327794313431\n",
      "step :  90 loss :  0.0820213034749031\n",
      "step :  91 loss :  0.08303294330835342\n",
      "step :  92 loss :  0.081733837723732\n",
      "step :  93 loss :  0.08295973390340805\n",
      "step :  94 loss :  0.08372005075216293\n",
      "step :  95 loss :  0.08184143155813217\n",
      "step :  96 loss :  0.08178134262561798\n",
      "step :  97 loss :  0.08168579638004303\n",
      "step :  98 loss :  0.08152851462364197\n",
      "step :  99 loss :  0.08132009953260422\n",
      "step :  100 loss :  0.08471309393644333\n",
      "step :  101 loss :  0.08143025636672974\n",
      "step :  102 loss :  0.0814189687371254\n",
      "step :  103 loss :  0.08137936890125275\n",
      "step :  104 loss :  0.08127249777317047\n",
      "step :  105 loss :  0.08117092400789261\n",
      "step :  106 loss :  0.08123646676540375\n",
      "step :  107 loss :  0.08147477358579636\n",
      "step :  108 loss :  0.081093929708004\n",
      "step :  109 loss :  0.08168712258338928\n",
      "step :  110 loss :  0.08279620856046677\n",
      "step :  111 loss :  0.08101484179496765\n",
      "step :  112 loss :  0.08225282281637192\n",
      "step :  113 loss :  0.08158797025680542\n",
      "step :  114 loss :  0.08103479444980621\n",
      "step :  115 loss :  0.08240297436714172\n",
      "step :  116 loss :  0.08162581920623779\n",
      "step :  117 loss :  0.08149219304323196\n",
      "step :  118 loss :  0.08115240186452866\n",
      "step :  119 loss :  0.08070384711027145\n",
      "step :  120 loss :  0.08073021471500397\n",
      "step :  121 loss :  0.08094905316829681\n",
      "step :  122 loss :  0.08199397474527359\n",
      "step :  123 loss :  0.08100265264511108\n",
      "step :  124 loss :  0.08098626881837845\n",
      "step :  125 loss :  0.08102919906377792\n",
      "step :  126 loss :  0.08066654950380325\n",
      "step :  127 loss :  0.0808195024728775\n",
      "step :  128 loss :  0.08097106963396072\n",
      "step :  129 loss :  0.08158493787050247\n",
      "step :  130 loss :  0.08061760663986206\n",
      "step :  131 loss :  0.08067560195922852\n",
      "step :  132 loss :  0.08097109943628311\n",
      "step :  133 loss :  0.08049134910106659\n",
      "step :  134 loss :  0.08060553669929504\n",
      "step :  135 loss :  0.08149372041225433\n",
      "step :  136 loss :  0.0805804654955864\n",
      "step :  137 loss :  0.08047322928905487\n",
      "step :  138 loss :  0.0805695652961731\n",
      "step :  139 loss :  0.08093062788248062\n",
      "step :  140 loss :  0.0814826563000679\n",
      "step :  141 loss :  0.08089908212423325\n",
      "step :  142 loss :  0.08150433748960495\n",
      "step :  143 loss :  0.08055337518453598\n",
      "step :  144 loss :  0.08053386211395264\n",
      "step :  145 loss :  0.08078181743621826\n",
      "step :  146 loss :  0.08056614547967911\n",
      "step :  147 loss :  0.08088222146034241\n",
      "step :  148 loss :  0.08039453625679016\n",
      "step :  149 loss :  0.08092319965362549\n",
      "step :  150 loss :  0.08032898604869843\n",
      "step :  151 loss :  0.0808270126581192\n",
      "step :  152 loss :  0.08033342659473419\n",
      "step :  153 loss :  0.08041534572839737\n",
      "step :  154 loss :  0.08057183027267456\n",
      "step :  155 loss :  0.08047810941934586\n",
      "step :  156 loss :  0.08102923631668091\n",
      "step :  157 loss :  0.08031716197729111\n",
      "step :  158 loss :  0.08042293787002563\n",
      "step :  159 loss :  0.08050626516342163\n",
      "step :  160 loss :  0.08024079352617264\n",
      "step :  161 loss :  0.08041117340326309\n",
      "step :  162 loss :  0.08026967942714691\n",
      "step :  163 loss :  0.08037012815475464\n",
      "step :  164 loss :  0.08027860522270203\n",
      "step :  165 loss :  0.0801989957690239\n",
      "step :  166 loss :  0.08078636974096298\n",
      "step :  167 loss :  0.08018801361322403\n",
      "step :  168 loss :  0.08026168495416641\n",
      "step :  169 loss :  0.0802249163389206\n",
      "step :  170 loss :  0.08019369840621948\n",
      "step :  171 loss :  0.0802687257528305\n",
      "step :  172 loss :  0.08025097101926804\n",
      "step :  173 loss :  0.0801917165517807\n",
      "step :  174 loss :  0.08016954362392426\n",
      "step :  175 loss :  0.08019515872001648\n",
      "step :  176 loss :  0.08014324307441711\n",
      "step :  177 loss :  0.08018060028553009\n",
      "step :  178 loss :  0.08012848347425461\n",
      "step :  179 loss :  0.08040786534547806\n",
      "step :  180 loss :  0.08016450703144073\n",
      "step :  181 loss :  0.08011278510093689\n",
      "step :  182 loss :  0.08013710379600525\n",
      "step :  183 loss :  0.08010200411081314\n",
      "step :  184 loss :  0.08022884279489517\n",
      "step :  185 loss :  0.08015107363462448\n",
      "step :  186 loss :  0.0801120400428772\n",
      "step :  187 loss :  0.08018998801708221\n",
      "step :  188 loss :  0.08007211983203888\n",
      "step :  189 loss :  0.0801927000284195\n",
      "step :  190 loss :  0.08014526218175888\n",
      "step :  191 loss :  0.08006557077169418\n",
      "step :  192 loss :  0.08009649813175201\n",
      "step :  193 loss :  0.0801343247294426\n",
      "step :  194 loss :  0.08009438216686249\n",
      "step :  195 loss :  0.08006957173347473\n",
      "step :  196 loss :  0.08008509129285812\n",
      "step :  197 loss :  0.0800914540886879\n",
      "step :  198 loss :  0.08008600026369095\n",
      "step :  199 loss :  0.08011822402477264\n",
      "step :  200 loss :  0.08005597442388535\n",
      "step :  201 loss :  0.08006063103675842\n",
      "step :  202 loss :  0.08007274568080902\n",
      "step :  203 loss :  0.08008690178394318\n",
      "step :  204 loss :  0.08004402369260788\n",
      "step :  205 loss :  0.08007913827896118\n",
      "step :  206 loss :  0.08003103733062744\n",
      "step :  207 loss :  0.08010251075029373\n",
      "step :  208 loss :  0.08003270626068115\n",
      "step :  209 loss :  0.08009984344244003\n",
      "step :  210 loss :  0.080006442964077\n",
      "step :  211 loss :  0.08001486212015152\n",
      "step :  212 loss :  0.08003615587949753\n",
      "step :  213 loss :  0.08002638071775436\n",
      "step :  214 loss :  0.08000072836875916\n",
      "step :  215 loss :  0.08001445978879929\n",
      "step :  216 loss :  0.07999050617218018\n",
      "step :  217 loss :  0.08009273558855057\n",
      "step :  218 loss :  0.07999120652675629\n",
      "step :  219 loss :  0.07998093217611313\n",
      "step :  220 loss :  0.07999947667121887\n",
      "step :  221 loss :  0.08003871887922287\n",
      "step :  222 loss :  0.07997336983680725\n",
      "step :  223 loss :  0.08000501245260239\n",
      "step :  224 loss :  0.07997937500476837\n",
      "step :  225 loss :  0.07998214662075043\n",
      "step :  226 loss :  0.07998117059469223\n",
      "step :  227 loss :  0.08001001179218292\n",
      "step :  228 loss :  0.07997661828994751\n",
      "step :  229 loss :  0.08003813028335571\n",
      "step :  230 loss :  0.08000960946083069\n",
      "step :  231 loss :  0.07995714247226715\n",
      "step :  232 loss :  0.07997544854879379\n",
      "step :  233 loss :  0.0799567773938179\n",
      "step :  234 loss :  0.07995788753032684\n",
      "step :  235 loss :  0.07994840294122696\n",
      "step :  236 loss :  0.07994803041219711\n",
      "step :  237 loss :  0.07996181398630142\n",
      "step :  238 loss :  0.07994549721479416\n",
      "step :  239 loss :  0.07996048033237457\n",
      "step :  240 loss :  0.07996596395969391\n",
      "step :  241 loss :  0.07993613183498383\n",
      "step :  242 loss :  0.07993648201227188\n",
      "step :  243 loss :  0.07993825525045395\n",
      "step :  244 loss :  0.07995307445526123\n",
      "step :  245 loss :  0.07996237277984619\n",
      "step :  246 loss :  0.07995638251304626\n",
      "step :  247 loss :  0.07995782792568207\n",
      "step :  248 loss :  0.07993375509977341\n",
      "step :  249 loss :  0.07992596924304962\n",
      "step :  250 loss :  0.07993698120117188\n",
      "step :  251 loss :  0.07993198931217194\n",
      "step :  252 loss :  0.0799376368522644\n",
      "step :  253 loss :  0.07994266599416733\n",
      "step :  254 loss :  0.07992482930421829\n",
      "step :  255 loss :  0.07993972301483154\n",
      "step :  256 loss :  0.0799281895160675\n",
      "step :  257 loss :  0.07993737608194351\n",
      "step :  258 loss :  0.07993179559707642\n",
      "step :  259 loss :  0.07992567121982574\n",
      "step :  260 loss :  0.07991661876440048\n",
      "step :  261 loss :  0.0799209401011467\n",
      "step :  262 loss :  0.07991773635149002\n",
      "step :  263 loss :  0.07993265241384506\n",
      "step :  264 loss :  0.07992097735404968\n",
      "step :  265 loss :  0.07992037385702133\n",
      "step :  266 loss :  0.07990583032369614\n",
      "step :  267 loss :  0.07991071790456772\n",
      "step :  268 loss :  0.07990715652704239\n",
      "step :  269 loss :  0.07990588247776031\n",
      "step :  270 loss :  0.07990454882383347\n",
      "step :  271 loss :  0.07990162819623947\n",
      "step :  272 loss :  0.07991363853216171\n",
      "step :  273 loss :  0.07990029454231262\n",
      "step :  274 loss :  0.07990264892578125\n",
      "step :  275 loss :  0.07991256564855576\n",
      "step :  276 loss :  0.079900361597538\n",
      "step :  277 loss :  0.07989993691444397\n",
      "step :  278 loss :  0.07990749925374985\n",
      "step :  279 loss :  0.07989317923784256\n",
      "step :  280 loss :  0.07989810407161713\n",
      "step :  281 loss :  0.07989516854286194\n",
      "step :  282 loss :  0.0798928290605545\n",
      "step :  283 loss :  0.07989197969436646\n",
      "step :  284 loss :  0.07989570498466492\n",
      "step :  285 loss :  0.07989170402288437\n",
      "step :  286 loss :  0.07990030199289322\n",
      "step :  287 loss :  0.0798923447728157\n",
      "step :  288 loss :  0.07989275455474854\n",
      "step :  289 loss :  0.07988641411066055\n",
      "step :  290 loss :  0.07988739013671875\n",
      "step :  291 loss :  0.07988721877336502\n",
      "step :  292 loss :  0.07988511770963669\n",
      "step :  293 loss :  0.07988709956407547\n",
      "step :  294 loss :  0.07988549023866653\n",
      "step :  295 loss :  0.07988438010215759\n",
      "step :  296 loss :  0.0798829048871994\n",
      "step :  297 loss :  0.07988081127405167\n",
      "step :  298 loss :  0.07988182455301285\n",
      "step :  299 loss :  0.07988985627889633\n",
      "step :  300 loss :  0.07988132536411285\n",
      "step :  301 loss :  0.07988452166318893\n",
      "step :  302 loss :  0.07988257706165314\n",
      "step :  303 loss :  0.07988127321004868\n",
      "step :  304 loss :  0.07988017052412033\n",
      "step :  305 loss :  0.07988794147968292\n",
      "step :  306 loss :  0.07987795025110245\n",
      "step :  307 loss :  0.07987645268440247\n",
      "step :  308 loss :  0.07989025861024857\n",
      "step :  309 loss :  0.0798780620098114\n",
      "step :  310 loss :  0.0798785537481308\n",
      "step :  311 loss :  0.07988087087869644\n",
      "step :  312 loss :  0.0798768624663353\n",
      "step :  313 loss :  0.07987704873085022\n",
      "step :  314 loss :  0.0798751637339592\n",
      "step :  315 loss :  0.07987329363822937\n",
      "step :  316 loss :  0.07988129556179047\n",
      "step :  317 loss :  0.07987212389707565\n",
      "step :  318 loss :  0.0798763632774353\n",
      "step :  319 loss :  0.07987707108259201\n",
      "step :  320 loss :  0.07987450808286667\n",
      "step :  321 loss :  0.07987389713525772\n",
      "step :  322 loss :  0.07987165451049805\n",
      "step :  323 loss :  0.0798715129494667\n",
      "step :  324 loss :  0.07987615466117859\n",
      "step :  325 loss :  0.07986883819103241\n",
      "step :  326 loss :  0.07986758649349213\n",
      "step :  327 loss :  0.07988224178552628\n",
      "step :  328 loss :  0.07987634837627411\n",
      "step :  329 loss :  0.07986894994974136\n",
      "step :  330 loss :  0.07987722009420395\n",
      "step :  331 loss :  0.07987773418426514\n",
      "step :  332 loss :  0.0798683911561966\n",
      "step :  333 loss :  0.07986867427825928\n",
      "step :  334 loss :  0.07987245172262192\n",
      "step :  335 loss :  0.07986929267644882\n",
      "step :  336 loss :  0.07986751943826675\n",
      "step :  337 loss :  0.0798678994178772\n",
      "step :  338 loss :  0.07986705750226974\n",
      "step :  339 loss :  0.07987052202224731\n",
      "step :  340 loss :  0.07986915856599808\n",
      "step :  341 loss :  0.07986821979284286\n",
      "step :  342 loss :  0.07986579090356827\n",
      "step :  343 loss :  0.0798681378364563\n",
      "step :  344 loss :  0.07986736297607422\n",
      "step :  345 loss :  0.07986471801996231\n",
      "step :  346 loss :  0.07986670732498169\n",
      "step :  347 loss :  0.07986748218536377\n",
      "step :  348 loss :  0.0798640176653862\n",
      "step :  349 loss :  0.07986476272344589\n",
      "step :  350 loss :  0.07986711710691452\n",
      "step :  351 loss :  0.07986573874950409\n",
      "step :  352 loss :  0.07986560463905334\n",
      "step :  353 loss :  0.07986333221197128\n",
      "step :  354 loss :  0.07986527681350708\n",
      "step :  355 loss :  0.07986698299646378\n",
      "step :  356 loss :  0.07986553013324738\n",
      "step :  357 loss :  0.07986491173505783\n",
      "step :  358 loss :  0.0798654779791832\n",
      "step :  359 loss :  0.07986245304346085\n",
      "step :  360 loss :  0.07986379414796829\n",
      "step :  361 loss :  0.07986482232809067\n",
      "step :  362 loss :  0.0798613578081131\n",
      "step :  363 loss :  0.07986278831958771\n",
      "step :  364 loss :  0.07986190170049667\n",
      "step :  365 loss :  0.079861119389534\n",
      "step :  366 loss :  0.07986097037792206\n",
      "step :  367 loss :  0.07986098527908325\n",
      "step :  368 loss :  0.07986120134592056\n",
      "step :  369 loss :  0.0798606127500534\n",
      "step :  370 loss :  0.07986093312501907\n",
      "step :  371 loss :  0.07986282557249069\n",
      "step :  372 loss :  0.07986069470643997\n",
      "step :  373 loss :  0.07986094802618027\n",
      "step :  374 loss :  0.07986339181661606\n",
      "step :  375 loss :  0.0798608660697937\n",
      "step :  376 loss :  0.07986133545637131\n",
      "step :  377 loss :  0.07986234128475189\n",
      "step :  378 loss :  0.07986105978488922\n",
      "step :  379 loss :  0.07986050099134445\n",
      "step :  380 loss :  0.07986060529947281\n",
      "step :  381 loss :  0.07985937595367432\n",
      "step :  382 loss :  0.07985980808734894\n",
      "step :  383 loss :  0.07985914498567581\n",
      "step :  384 loss :  0.07986091077327728\n",
      "step :  385 loss :  0.07986053824424744\n",
      "step :  386 loss :  0.07985924929380417\n",
      "step :  387 loss :  0.07985910028219223\n",
      "step :  388 loss :  0.07986173778772354\n",
      "step :  389 loss :  0.07986009865999222\n",
      "step :  390 loss :  0.07985866069793701\n",
      "step :  391 loss :  0.07986031472682953\n",
      "step :  392 loss :  0.0798599123954773\n",
      "step :  393 loss :  0.07985950261354446\n",
      "step :  394 loss :  0.07985948771238327\n",
      "step :  395 loss :  0.07985922694206238\n",
      "step :  396 loss :  0.07985926419496536\n",
      "step :  397 loss :  0.07985823601484299\n",
      "step :  398 loss :  0.07986094057559967\n",
      "step :  399 loss :  0.07986082136631012\n",
      "step :  400 loss :  0.07985895127058029\n",
      "step :  401 loss :  0.07985882461071014\n",
      "step :  402 loss :  0.07986049354076385\n",
      "step :  403 loss :  0.07985910028219223\n",
      "step :  404 loss :  0.07985848933458328\n",
      "step :  405 loss :  0.07985973358154297\n",
      "step :  406 loss :  0.07985848188400269\n",
      "step :  407 loss :  0.07985760271549225\n",
      "step :  408 loss :  0.07985895872116089\n",
      "step :  409 loss :  0.07985880225896835\n",
      "step :  410 loss :  0.07985812425613403\n",
      "step :  411 loss :  0.07985753566026688\n",
      "step :  412 loss :  0.07985761016607285\n",
      "step :  413 loss :  0.0798579677939415\n",
      "step :  414 loss :  0.07985857129096985\n",
      "step :  415 loss :  0.07985753566026688\n",
      "step :  416 loss :  0.0798579752445221\n",
      "step :  417 loss :  0.07985794544219971\n",
      "step :  418 loss :  0.0798574835062027\n",
      "step :  419 loss :  0.07985855638980865\n",
      "step :  420 loss :  0.0798582062125206\n",
      "step :  421 loss :  0.07985749840736389\n",
      "step :  422 loss :  0.0798572450876236\n",
      "step :  423 loss :  0.0798574909567833\n",
      "step :  424 loss :  0.07985809445381165\n",
      "step :  425 loss :  0.07985720038414001\n",
      "step :  426 loss :  0.0798572301864624\n",
      "step :  427 loss :  0.07985832542181015\n",
      "step :  428 loss :  0.07985790073871613\n",
      "step :  429 loss :  0.07985717058181763\n",
      "step :  430 loss :  0.07985757291316986\n",
      "step :  431 loss :  0.07985757291316986\n",
      "step :  432 loss :  0.07985726743936539\n",
      "step :  433 loss :  0.07985734194517136\n",
      "step :  434 loss :  0.07985737174749374\n",
      "step :  435 loss :  0.0798572301864624\n",
      "step :  436 loss :  0.07985759526491165\n",
      "step :  437 loss :  0.0798569843173027\n",
      "step :  438 loss :  0.07985685020685196\n",
      "step :  439 loss :  0.07985753566026688\n",
      "step :  440 loss :  0.07985720783472061\n",
      "step :  441 loss :  0.07985669374465942\n",
      "step :  442 loss :  0.07985737174749374\n",
      "step :  443 loss :  0.07985764741897583\n",
      "step :  444 loss :  0.07985708117485046\n",
      "step :  445 loss :  0.07985705882310867\n",
      "step :  446 loss :  0.07985759526491165\n",
      "step :  447 loss :  0.07985721528530121\n",
      "step :  448 loss :  0.0798567533493042\n",
      "step :  449 loss :  0.07985684275627136\n",
      "step :  450 loss :  0.0798567682504654\n",
      "step :  451 loss :  0.07985688000917435\n",
      "step :  452 loss :  0.07985661178827286\n",
      "step :  453 loss :  0.07985670119524002\n",
      "step :  454 loss :  0.07985679060220718\n",
      "step :  455 loss :  0.07985671609640121\n",
      "step :  456 loss :  0.07985709607601166\n",
      "step :  457 loss :  0.079856738448143\n",
      "step :  458 loss :  0.07985646277666092\n",
      "step :  459 loss :  0.07985691726207733\n",
      "step :  460 loss :  0.07985685020685196\n",
      "step :  461 loss :  0.07985646277666092\n",
      "step :  462 loss :  0.07985667884349823\n",
      "step :  463 loss :  0.07985664904117584\n",
      "step :  464 loss :  0.07985653728246689\n",
      "step :  465 loss :  0.07985666394233704\n",
      "step :  466 loss :  0.07985713332891464\n",
      "step :  467 loss :  0.07985687255859375\n",
      "step :  468 loss :  0.07985653728246689\n",
      "step :  469 loss :  0.07985647767782211\n",
      "step :  470 loss :  0.0798565149307251\n",
      "step :  471 loss :  0.07985658943653107\n",
      "step :  472 loss :  0.07985640317201614\n",
      "step :  473 loss :  0.07985637336969376\n",
      "step :  474 loss :  0.07985668629407883\n",
      "step :  475 loss :  0.07985655218362808\n",
      "step :  476 loss :  0.079856276512146\n",
      "step :  477 loss :  0.07985661178827286\n",
      "step :  478 loss :  0.07985661178827286\n",
      "step :  479 loss :  0.07985641807317734\n",
      "step :  480 loss :  0.07985633611679077\n",
      "step :  481 loss :  0.07985660433769226\n",
      "step :  482 loss :  0.0798565074801445\n",
      "step :  483 loss :  0.07985623925924301\n",
      "step :  484 loss :  0.07985660433769226\n",
      "step :  485 loss :  0.07985668629407883\n",
      "step :  486 loss :  0.0798565149307251\n",
      "step :  487 loss :  0.07985641807317734\n",
      "step :  488 loss :  0.07985632121562958\n",
      "step :  489 loss :  0.07985635846853256\n",
      "step :  490 loss :  0.07985633611679077\n",
      "step :  491 loss :  0.07985635846853256\n",
      "step :  492 loss :  0.07985642552375793\n",
      "step :  493 loss :  0.07985629886388779\n",
      "step :  494 loss :  0.07985623925924301\n",
      "step :  495 loss :  0.07985639572143555\n",
      "step :  496 loss :  0.07985621690750122\n",
      "step :  497 loss :  0.07985613495111465\n",
      "step :  498 loss :  0.07985632121562958\n",
      "step :  499 loss :  0.07985628396272659\n",
      "Training model 7\n",
      "step :  0 loss :  0.39724835753440857\n",
      "step :  1 loss :  0.2440359890460968\n",
      "step :  2 loss :  0.20358774065971375\n",
      "step :  3 loss :  0.24749699234962463\n",
      "step :  4 loss :  0.2083258181810379\n",
      "step :  5 loss :  0.19800181686878204\n",
      "step :  6 loss :  0.2083379477262497\n",
      "step :  7 loss :  0.2137531191110611\n",
      "step :  8 loss :  0.19506967067718506\n",
      "step :  9 loss :  0.20227496325969696\n",
      "step :  10 loss :  0.19070249795913696\n",
      "step :  11 loss :  0.1774846613407135\n",
      "step :  12 loss :  0.17048394680023193\n",
      "step :  13 loss :  0.1613200455904007\n",
      "step :  14 loss :  0.14379623532295227\n",
      "step :  15 loss :  0.15891370177268982\n",
      "step :  16 loss :  0.1422480195760727\n",
      "step :  17 loss :  0.19790582358837128\n",
      "step :  18 loss :  0.21351072192192078\n",
      "step :  19 loss :  0.15500633418560028\n",
      "step :  20 loss :  0.12522713840007782\n",
      "step :  21 loss :  0.11829588562250137\n",
      "step :  22 loss :  0.13849946856498718\n",
      "step :  23 loss :  0.12708783149719238\n",
      "step :  24 loss :  0.13565772771835327\n",
      "step :  25 loss :  0.123089499771595\n",
      "step :  26 loss :  0.1271713525056839\n",
      "step :  27 loss :  0.10268787294626236\n",
      "step :  28 loss :  0.12516295909881592\n",
      "step :  29 loss :  0.135914146900177\n",
      "step :  30 loss :  0.09902774542570114\n",
      "step :  31 loss :  0.1189781054854393\n",
      "step :  32 loss :  0.10698976367712021\n",
      "step :  33 loss :  0.09497173875570297\n",
      "step :  34 loss :  0.10334651917219162\n",
      "step :  35 loss :  0.11635039001703262\n",
      "step :  36 loss :  0.09170274436473846\n",
      "step :  37 loss :  0.10680525749921799\n",
      "step :  38 loss :  0.11576380580663681\n",
      "step :  39 loss :  0.0939977616071701\n",
      "step :  40 loss :  0.08670968562364578\n",
      "step :  41 loss :  0.08840873837471008\n",
      "step :  42 loss :  0.09169821441173553\n",
      "step :  43 loss :  0.10199645906686783\n",
      "step :  44 loss :  0.1218385323882103\n",
      "step :  45 loss :  0.08757876604795456\n",
      "step :  46 loss :  0.08473370224237442\n",
      "step :  47 loss :  0.08393389731645584\n",
      "step :  48 loss :  0.08973663300275803\n",
      "step :  49 loss :  0.08715703338384628\n",
      "step :  50 loss :  0.10059753805398941\n",
      "step :  51 loss :  0.08757629245519638\n",
      "step :  52 loss :  0.09155542403459549\n",
      "step :  53 loss :  0.08726321905851364\n",
      "step :  54 loss :  0.08493710309267044\n",
      "step :  55 loss :  0.08978454023599625\n",
      "step :  56 loss :  0.08326879888772964\n",
      "step :  57 loss :  0.08858173340559006\n",
      "step :  58 loss :  0.08444218337535858\n",
      "step :  59 loss :  0.09790582209825516\n",
      "step :  60 loss :  0.0825095847249031\n",
      "step :  61 loss :  0.08317084610462189\n",
      "step :  62 loss :  0.08346980810165405\n",
      "step :  63 loss :  0.08280576765537262\n",
      "step :  64 loss :  0.10042297095060349\n",
      "step :  65 loss :  0.08814037591218948\n",
      "step :  66 loss :  0.08221205323934555\n",
      "step :  67 loss :  0.08643323928117752\n",
      "step :  68 loss :  0.08599739521741867\n",
      "step :  69 loss :  0.08593026548624039\n",
      "step :  70 loss :  0.08295748382806778\n",
      "step :  71 loss :  0.08180207759141922\n",
      "step :  72 loss :  0.08848509937524796\n",
      "step :  73 loss :  0.08475728332996368\n",
      "step :  74 loss :  0.08141259849071503\n",
      "step :  75 loss :  0.08337493240833282\n",
      "step :  76 loss :  0.08317404240369797\n",
      "step :  77 loss :  0.08181370794773102\n",
      "step :  78 loss :  0.0883566290140152\n",
      "step :  79 loss :  0.08379693329334259\n",
      "step :  80 loss :  0.08097753673791885\n",
      "step :  81 loss :  0.0811789482831955\n",
      "step :  82 loss :  0.08061254769563675\n",
      "step :  83 loss :  0.08081132918596268\n",
      "step :  84 loss :  0.08227787166833878\n",
      "step :  85 loss :  0.08290282636880875\n",
      "step :  86 loss :  0.0807340070605278\n",
      "step :  87 loss :  0.08395519107580185\n",
      "step :  88 loss :  0.08178798109292984\n",
      "step :  89 loss :  0.08163388818502426\n",
      "step :  90 loss :  0.08119572699069977\n",
      "step :  91 loss :  0.08042760193347931\n",
      "step :  92 loss :  0.08017859607934952\n",
      "step :  93 loss :  0.0818362608551979\n",
      "step :  94 loss :  0.0818081945180893\n",
      "step :  95 loss :  0.08220242708921432\n",
      "step :  96 loss :  0.08043612539768219\n",
      "step :  97 loss :  0.08153566718101501\n",
      "step :  98 loss :  0.08056802302598953\n",
      "step :  99 loss :  0.0802740827202797\n",
      "step :  100 loss :  0.08221122622489929\n",
      "step :  101 loss :  0.08084867149591446\n",
      "step :  102 loss :  0.08025775104761124\n",
      "step :  103 loss :  0.08208990842103958\n",
      "step :  104 loss :  0.08137556165456772\n",
      "step :  105 loss :  0.0804893895983696\n",
      "step :  106 loss :  0.08003216981887817\n",
      "step :  107 loss :  0.0810052752494812\n",
      "step :  108 loss :  0.08121172338724136\n",
      "step :  109 loss :  0.07984074205160141\n",
      "step :  110 loss :  0.07993407547473907\n",
      "step :  111 loss :  0.08134184777736664\n",
      "step :  112 loss :  0.07966247200965881\n",
      "step :  113 loss :  0.07955942302942276\n",
      "step :  114 loss :  0.08030865341424942\n",
      "step :  115 loss :  0.07951801270246506\n",
      "step :  116 loss :  0.0794297382235527\n",
      "step :  117 loss :  0.07978220283985138\n",
      "step :  118 loss :  0.08097139745950699\n",
      "step :  119 loss :  0.08065354824066162\n",
      "step :  120 loss :  0.08128724992275238\n",
      "step :  121 loss :  0.07975557446479797\n",
      "step :  122 loss :  0.07970926910638809\n",
      "step :  123 loss :  0.08060309290885925\n",
      "step :  124 loss :  0.08046804368495941\n",
      "step :  125 loss :  0.08101882040500641\n",
      "step :  126 loss :  0.08005407452583313\n",
      "step :  127 loss :  0.08099778741598129\n",
      "step :  128 loss :  0.07942374795675278\n",
      "step :  129 loss :  0.08058525621891022\n",
      "step :  130 loss :  0.07995323091745377\n",
      "step :  131 loss :  0.08158430457115173\n",
      "step :  132 loss :  0.08004357665777206\n",
      "step :  133 loss :  0.08080291748046875\n",
      "step :  134 loss :  0.07970312237739563\n",
      "step :  135 loss :  0.08128752559423447\n",
      "step :  136 loss :  0.07966558635234833\n",
      "step :  137 loss :  0.08157195150852203\n",
      "step :  138 loss :  0.07958831638097763\n",
      "step :  139 loss :  0.08076748251914978\n",
      "step :  140 loss :  0.07945431768894196\n",
      "step :  141 loss :  0.08023297786712646\n",
      "step :  142 loss :  0.07934761792421341\n",
      "step :  143 loss :  0.07950097322463989\n",
      "step :  144 loss :  0.07942565530538559\n",
      "step :  145 loss :  0.07922568172216415\n",
      "step :  146 loss :  0.07926470786333084\n",
      "step :  147 loss :  0.07932020723819733\n",
      "step :  148 loss :  0.07924024015665054\n",
      "step :  149 loss :  0.07936909794807434\n",
      "step :  150 loss :  0.07915132492780685\n",
      "step :  151 loss :  0.07949479669332504\n",
      "step :  152 loss :  0.07912051677703857\n",
      "step :  153 loss :  0.07945564389228821\n",
      "step :  154 loss :  0.079180046916008\n",
      "step :  155 loss :  0.0792083740234375\n",
      "step :  156 loss :  0.07911369204521179\n",
      "step :  157 loss :  0.07916931062936783\n",
      "step :  158 loss :  0.07906869053840637\n",
      "step :  159 loss :  0.0793089047074318\n",
      "step :  160 loss :  0.0790444016456604\n",
      "step :  161 loss :  0.07931911945343018\n",
      "step :  162 loss :  0.07905706763267517\n",
      "step :  163 loss :  0.07910492271184921\n",
      "step :  164 loss :  0.07904325425624847\n",
      "step :  165 loss :  0.07902497053146362\n",
      "step :  166 loss :  0.07912331074476242\n",
      "step :  167 loss :  0.07905304431915283\n",
      "step :  168 loss :  0.07902295887470245\n",
      "step :  169 loss :  0.0790172666311264\n",
      "step :  170 loss :  0.07915061712265015\n",
      "step :  171 loss :  0.07902856171131134\n",
      "step :  172 loss :  0.07900473475456238\n",
      "step :  173 loss :  0.07898320257663727\n",
      "step :  174 loss :  0.07925346493721008\n",
      "step :  175 loss :  0.0789639875292778\n",
      "step :  176 loss :  0.0790146216750145\n",
      "step :  177 loss :  0.0789816826581955\n",
      "step :  178 loss :  0.07915984839200974\n",
      "step :  179 loss :  0.0789952203631401\n",
      "step :  180 loss :  0.07905438542366028\n",
      "step :  181 loss :  0.0790204331278801\n",
      "step :  182 loss :  0.07896322011947632\n",
      "step :  183 loss :  0.07911655306816101\n",
      "step :  184 loss :  0.07895097881555557\n",
      "step :  185 loss :  0.07893838733434677\n",
      "step :  186 loss :  0.07894903421401978\n",
      "step :  187 loss :  0.07902134954929352\n",
      "step :  188 loss :  0.07897768169641495\n",
      "step :  189 loss :  0.07897574454545975\n",
      "step :  190 loss :  0.07898111641407013\n",
      "step :  191 loss :  0.07892509549856186\n",
      "step :  192 loss :  0.07896798849105835\n",
      "step :  193 loss :  0.07893147319555283\n",
      "step :  194 loss :  0.07898803800344467\n",
      "step :  195 loss :  0.07896743714809418\n",
      "step :  196 loss :  0.07897365093231201\n",
      "step :  197 loss :  0.07890595495700836\n",
      "step :  198 loss :  0.0789070725440979\n",
      "step :  199 loss :  0.0789269208908081\n",
      "step :  200 loss :  0.0788908451795578\n",
      "step :  201 loss :  0.07896378636360168\n",
      "step :  202 loss :  0.0789036676287651\n",
      "step :  203 loss :  0.07898373901844025\n",
      "step :  204 loss :  0.07891523838043213\n",
      "step :  205 loss :  0.07892796397209167\n",
      "step :  206 loss :  0.07889435440301895\n",
      "step :  207 loss :  0.07891514897346497\n",
      "step :  208 loss :  0.07889266312122345\n",
      "step :  209 loss :  0.07892536371946335\n",
      "step :  210 loss :  0.0789162665605545\n",
      "step :  211 loss :  0.07893042266368866\n",
      "step :  212 loss :  0.0788908451795578\n",
      "step :  213 loss :  0.07889088243246078\n",
      "step :  214 loss :  0.07889824360609055\n",
      "step :  215 loss :  0.0788990929722786\n",
      "step :  216 loss :  0.07892076671123505\n",
      "step :  217 loss :  0.0788724347949028\n",
      "step :  218 loss :  0.078911192715168\n",
      "step :  219 loss :  0.07888872176408768\n",
      "step :  220 loss :  0.07889902591705322\n",
      "step :  221 loss :  0.07888730615377426\n",
      "step :  222 loss :  0.07890074700117111\n",
      "step :  223 loss :  0.07887370139360428\n",
      "step :  224 loss :  0.0788891538977623\n",
      "step :  225 loss :  0.0788535326719284\n",
      "step :  226 loss :  0.07889476418495178\n",
      "step :  227 loss :  0.07885345071554184\n",
      "step :  228 loss :  0.07887109369039536\n",
      "step :  229 loss :  0.07887755334377289\n",
      "step :  230 loss :  0.07889900356531143\n",
      "step :  231 loss :  0.07885816693305969\n",
      "step :  232 loss :  0.07886463403701782\n",
      "step :  233 loss :  0.07885255664587021\n",
      "step :  234 loss :  0.0788583979010582\n",
      "step :  235 loss :  0.07884860783815384\n",
      "step :  236 loss :  0.07886014878749847\n",
      "step :  237 loss :  0.07886214554309845\n",
      "step :  238 loss :  0.07887058705091476\n",
      "step :  239 loss :  0.07884223759174347\n",
      "step :  240 loss :  0.07887714356184006\n",
      "step :  241 loss :  0.07882925868034363\n",
      "step :  242 loss :  0.07884892076253891\n",
      "step :  243 loss :  0.07885283976793289\n",
      "step :  244 loss :  0.07883809506893158\n",
      "step :  245 loss :  0.07885038107633591\n",
      "step :  246 loss :  0.07888958603143692\n",
      "step :  247 loss :  0.0788293182849884\n",
      "step :  248 loss :  0.07883677631616592\n",
      "step :  249 loss :  0.07885029911994934\n",
      "step :  250 loss :  0.0788261741399765\n",
      "step :  251 loss :  0.07884035259485245\n",
      "step :  252 loss :  0.07883638143539429\n",
      "step :  253 loss :  0.0788448303937912\n",
      "step :  254 loss :  0.07883897423744202\n",
      "step :  255 loss :  0.07883533835411072\n",
      "step :  256 loss :  0.07885509729385376\n",
      "step :  257 loss :  0.07883447408676147\n",
      "step :  258 loss :  0.0788252055644989\n",
      "step :  259 loss :  0.07883421331644058\n",
      "step :  260 loss :  0.07881910353899002\n",
      "step :  261 loss :  0.07883938401937485\n",
      "step :  262 loss :  0.07881620526313782\n",
      "step :  263 loss :  0.07882747799158096\n",
      "step :  264 loss :  0.0788264200091362\n",
      "step :  265 loss :  0.07882609218358994\n",
      "step :  266 loss :  0.07883324474096298\n",
      "step :  267 loss :  0.07882311195135117\n",
      "step :  268 loss :  0.07882460206747055\n",
      "step :  269 loss :  0.07882072776556015\n",
      "step :  270 loss :  0.07883571088314056\n",
      "step :  271 loss :  0.07881352305412292\n",
      "step :  272 loss :  0.07882016897201538\n",
      "step :  273 loss :  0.07881684601306915\n",
      "step :  274 loss :  0.07881702482700348\n",
      "step :  275 loss :  0.07882386445999146\n",
      "step :  276 loss :  0.07881166785955429\n",
      "step :  277 loss :  0.07881395518779755\n",
      "step :  278 loss :  0.07880955189466476\n",
      "step :  279 loss :  0.07881134748458862\n",
      "step :  280 loss :  0.07881197333335876\n",
      "step :  281 loss :  0.0788111761212349\n",
      "step :  282 loss :  0.07882563024759293\n",
      "step :  283 loss :  0.07880786061286926\n",
      "step :  284 loss :  0.07881784439086914\n",
      "step :  285 loss :  0.07882430404424667\n",
      "step :  286 loss :  0.07880254834890366\n",
      "step :  287 loss :  0.0788055956363678\n",
      "step :  288 loss :  0.07881492376327515\n",
      "step :  289 loss :  0.07880895584821701\n",
      "step :  290 loss :  0.07880683243274689\n",
      "step :  291 loss :  0.07881252467632294\n",
      "step :  292 loss :  0.07880863547325134\n",
      "step :  293 loss :  0.0788092240691185\n",
      "step :  294 loss :  0.07881108671426773\n",
      "step :  295 loss :  0.07881101965904236\n",
      "step :  296 loss :  0.07881069928407669\n",
      "step :  297 loss :  0.07881216704845428\n",
      "step :  298 loss :  0.0788043811917305\n",
      "step :  299 loss :  0.0788065567612648\n",
      "step :  300 loss :  0.07880285382270813\n",
      "step :  301 loss :  0.07881017029285431\n",
      "step :  302 loss :  0.07882469892501831\n",
      "step :  303 loss :  0.07879983633756638\n",
      "step :  304 loss :  0.0787978544831276\n",
      "step :  305 loss :  0.07881626486778259\n",
      "step :  306 loss :  0.07880751043558121\n",
      "step :  307 loss :  0.07880114763975143\n",
      "step :  308 loss :  0.0788043886423111\n",
      "step :  309 loss :  0.07880646735429764\n",
      "step :  310 loss :  0.07880308479070663\n",
      "step :  311 loss :  0.07880346477031708\n",
      "step :  312 loss :  0.07880650460720062\n",
      "step :  313 loss :  0.07879887521266937\n",
      "step :  314 loss :  0.07879994064569473\n",
      "step :  315 loss :  0.07880118489265442\n",
      "step :  316 loss :  0.07879739999771118\n",
      "step :  317 loss :  0.07880336046218872\n",
      "step :  318 loss :  0.07879949361085892\n",
      "step :  319 loss :  0.07879694551229477\n",
      "step :  320 loss :  0.0788004919886589\n",
      "step :  321 loss :  0.07880093157291412\n",
      "step :  322 loss :  0.07879984378814697\n",
      "step :  323 loss :  0.07879994064569473\n",
      "step :  324 loss :  0.0788014754652977\n",
      "step :  325 loss :  0.07879576086997986\n",
      "step :  326 loss :  0.07880314439535141\n",
      "step :  327 loss :  0.07879745960235596\n",
      "step :  328 loss :  0.07879891991615295\n",
      "step :  329 loss :  0.07880059629678726\n",
      "step :  330 loss :  0.07879610359668732\n",
      "step :  331 loss :  0.07879991084337234\n",
      "step :  332 loss :  0.078800268471241\n",
      "step :  333 loss :  0.07879936695098877\n",
      "step :  334 loss :  0.07880090177059174\n",
      "step :  335 loss :  0.07879897952079773\n",
      "step :  336 loss :  0.07880213856697083\n",
      "step :  337 loss :  0.07880184799432755\n",
      "step :  338 loss :  0.07879696786403656\n",
      "step :  339 loss :  0.0787973701953888\n",
      "step :  340 loss :  0.07879679650068283\n",
      "step :  341 loss :  0.07879668474197388\n",
      "step :  342 loss :  0.07879865169525146\n",
      "step :  343 loss :  0.07880180329084396\n",
      "step :  344 loss :  0.07879560440778732\n",
      "step :  345 loss :  0.07879631221294403\n",
      "step :  346 loss :  0.07879708707332611\n",
      "step :  347 loss :  0.07879866659641266\n",
      "step :  348 loss :  0.07879532873630524\n",
      "step :  349 loss :  0.0787944421172142\n",
      "step :  350 loss :  0.07879512757062912\n",
      "step :  351 loss :  0.07879636436700821\n",
      "step :  352 loss :  0.07879393547773361\n",
      "step :  353 loss :  0.07879621535539627\n",
      "step :  354 loss :  0.07879465818405151\n",
      "step :  355 loss :  0.07879367470741272\n",
      "step :  356 loss :  0.07879701256752014\n",
      "step :  357 loss :  0.07879766076803207\n",
      "step :  358 loss :  0.07879606634378433\n",
      "step :  359 loss :  0.07879771292209625\n",
      "step :  360 loss :  0.07879357039928436\n",
      "step :  361 loss :  0.07879392802715302\n",
      "step :  362 loss :  0.07879561185836792\n",
      "step :  363 loss :  0.07879481464624405\n",
      "step :  364 loss :  0.07879342138767242\n",
      "step :  365 loss :  0.07879523932933807\n",
      "step :  366 loss :  0.07879481464624405\n",
      "step :  367 loss :  0.07879330217838287\n",
      "step :  368 loss :  0.07879408448934555\n",
      "step :  369 loss :  0.07879461348056793\n",
      "step :  370 loss :  0.07879362255334854\n",
      "step :  371 loss :  0.07879465818405151\n",
      "step :  372 loss :  0.07879474759101868\n",
      "step :  373 loss :  0.07879429310560226\n",
      "step :  374 loss :  0.07879411429166794\n",
      "step :  375 loss :  0.07879509031772614\n",
      "step :  376 loss :  0.07879383116960526\n",
      "step :  377 loss :  0.07879415899515152\n",
      "step :  378 loss :  0.07879406213760376\n",
      "step :  379 loss :  0.07879327982664108\n",
      "step :  380 loss :  0.07879374176263809\n",
      "step :  381 loss :  0.07879329472780228\n",
      "step :  382 loss :  0.07879365980625153\n",
      "step :  383 loss :  0.07879555225372314\n",
      "step :  384 loss :  0.07879292964935303\n",
      "step :  385 loss :  0.07879186421632767\n",
      "step :  386 loss :  0.07879353314638138\n",
      "step :  387 loss :  0.07879551500082016\n",
      "step :  388 loss :  0.0787925124168396\n",
      "step :  389 loss :  0.07879320532083511\n",
      "step :  390 loss :  0.0787949189543724\n",
      "step :  391 loss :  0.07879415899515152\n",
      "step :  392 loss :  0.07879384607076645\n",
      "step :  393 loss :  0.0787927582859993\n",
      "step :  394 loss :  0.07879342883825302\n",
      "step :  395 loss :  0.07879243046045303\n",
      "step :  396 loss :  0.07879411429166794\n",
      "step :  397 loss :  0.0787925124168396\n",
      "step :  398 loss :  0.0787927582859993\n",
      "step :  399 loss :  0.07879243791103363\n",
      "step :  400 loss :  0.07879220694303513\n",
      "step :  401 loss :  0.07879260927438736\n",
      "step :  402 loss :  0.07879186421632767\n",
      "step :  403 loss :  0.07879279553890228\n",
      "step :  404 loss :  0.07879307866096497\n",
      "step :  405 loss :  0.0787920132279396\n",
      "step :  406 loss :  0.07879366725683212\n",
      "step :  407 loss :  0.0787922814488411\n",
      "step :  408 loss :  0.0787924975156784\n",
      "step :  409 loss :  0.0787924975156784\n",
      "step :  410 loss :  0.07879297435283661\n",
      "step :  411 loss :  0.07879246771335602\n",
      "step :  412 loss :  0.07879196107387543\n",
      "step :  413 loss :  0.07879281789064407\n",
      "step :  414 loss :  0.07879173010587692\n",
      "step :  415 loss :  0.07879172265529633\n",
      "step :  416 loss :  0.07879341393709183\n",
      "step :  417 loss :  0.07879176735877991\n",
      "step :  418 loss :  0.07879126071929932\n",
      "step :  419 loss :  0.07879293709993362\n",
      "step :  420 loss :  0.07879254221916199\n",
      "step :  421 loss :  0.07879197597503662\n",
      "step :  422 loss :  0.07879211753606796\n",
      "step :  423 loss :  0.07879195362329483\n",
      "step :  424 loss :  0.07879150658845901\n",
      "step :  425 loss :  0.0787920355796814\n",
      "step :  426 loss :  0.0787920132279396\n",
      "step :  427 loss :  0.07879194617271423\n",
      "step :  428 loss :  0.07879215478897095\n",
      "step :  429 loss :  0.07879191637039185\n",
      "step :  430 loss :  0.07879182696342468\n",
      "step :  431 loss :  0.07879220694303513\n",
      "step :  432 loss :  0.07879176735877991\n",
      "step :  433 loss :  0.07879210263490677\n",
      "step :  434 loss :  0.07879140228033066\n",
      "step :  435 loss :  0.07879213243722916\n",
      "step :  436 loss :  0.07879149913787842\n",
      "step :  437 loss :  0.07879122346639633\n",
      "step :  438 loss :  0.07879166305065155\n",
      "step :  439 loss :  0.07879172265529633\n",
      "step :  440 loss :  0.07879176735877991\n",
      "step :  441 loss :  0.07879168540239334\n",
      "step :  442 loss :  0.07879135757684708\n",
      "step :  443 loss :  0.07879146933555603\n",
      "step :  444 loss :  0.07879199087619781\n",
      "step :  445 loss :  0.07879162579774857\n",
      "step :  446 loss :  0.07879161834716797\n",
      "step :  447 loss :  0.07879176735877991\n",
      "step :  448 loss :  0.07879140973091125\n",
      "step :  449 loss :  0.07879157364368439\n",
      "step :  450 loss :  0.07879184931516647\n",
      "step :  451 loss :  0.07879111915826797\n",
      "step :  452 loss :  0.07879125326871872\n",
      "step :  453 loss :  0.07879111170768738\n",
      "step :  454 loss :  0.0787910670042038\n",
      "step :  455 loss :  0.07879127562046051\n",
      "step :  456 loss :  0.07879132032394409\n",
      "step :  457 loss :  0.07879137247800827\n",
      "step :  458 loss :  0.07879148423671722\n",
      "step :  459 loss :  0.07879142463207245\n",
      "step :  460 loss :  0.07879132777452469\n",
      "step :  461 loss :  0.07879146933555603\n",
      "step :  462 loss :  0.07879128307104111\n",
      "step :  463 loss :  0.0787913128733635\n",
      "step :  464 loss :  0.07879135757684708\n",
      "step :  465 loss :  0.07879120111465454\n",
      "step :  466 loss :  0.07879117876291275\n",
      "step :  467 loss :  0.0787920281291008\n",
      "step :  468 loss :  0.07879120111465454\n",
      "step :  469 loss :  0.07879096269607544\n",
      "step :  470 loss :  0.07879159599542618\n",
      "step :  471 loss :  0.0787917971611023\n",
      "step :  472 loss :  0.07879120111465454\n",
      "step :  473 loss :  0.07879151403903961\n",
      "step :  474 loss :  0.07879151403903961\n",
      "step :  475 loss :  0.078791044652462\n",
      "step :  476 loss :  0.07879111915826797\n",
      "step :  477 loss :  0.07879117876291275\n",
      "step :  478 loss :  0.07879099994897842\n",
      "step :  479 loss :  0.07879114151000977\n",
      "step :  480 loss :  0.07879118621349335\n",
      "step :  481 loss :  0.07879103720188141\n",
      "step :  482 loss :  0.07879120111465454\n",
      "step :  483 loss :  0.07879102975130081\n",
      "step :  484 loss :  0.07879094779491425\n",
      "step :  485 loss :  0.07879139482975006\n",
      "step :  486 loss :  0.07879108190536499\n",
      "step :  487 loss :  0.07879100739955902\n",
      "step :  488 loss :  0.07879120856523514\n",
      "step :  489 loss :  0.07879102230072021\n",
      "step :  490 loss :  0.07879098504781723\n",
      "step :  491 loss :  0.07879092544317245\n",
      "step :  492 loss :  0.07879098504781723\n",
      "step :  493 loss :  0.07879109680652618\n",
      "step :  494 loss :  0.078791044652462\n",
      "step :  495 loss :  0.0787910595536232\n",
      "step :  496 loss :  0.07879102975130081\n",
      "step :  497 loss :  0.07879110425710678\n",
      "step :  498 loss :  0.07879095524549484\n",
      "step :  499 loss :  0.07879113405942917\n",
      "Training model 8\n",
      "step :  0 loss :  0.36204540729522705\n",
      "step :  1 loss :  0.25351130962371826\n",
      "step :  2 loss :  0.20477771759033203\n",
      "step :  3 loss :  0.22447100281715393\n",
      "step :  4 loss :  0.1895042061805725\n",
      "step :  5 loss :  0.2017637938261032\n",
      "step :  6 loss :  0.21287454664707184\n",
      "step :  7 loss :  0.24428316950798035\n",
      "step :  8 loss :  0.2001747041940689\n",
      "step :  9 loss :  0.2160940021276474\n",
      "step :  10 loss :  0.19457420706748962\n",
      "step :  11 loss :  0.17936648428440094\n",
      "step :  12 loss :  0.172378808259964\n",
      "step :  13 loss :  0.16238893568515778\n",
      "step :  14 loss :  0.15196284651756287\n",
      "step :  15 loss :  0.14506486058235168\n",
      "step :  16 loss :  0.12126719951629639\n",
      "step :  17 loss :  0.10885526984930038\n",
      "step :  18 loss :  0.13678905367851257\n",
      "step :  19 loss :  0.11045844852924347\n",
      "step :  20 loss :  0.133474200963974\n",
      "step :  21 loss :  0.11290546506643295\n",
      "step :  22 loss :  0.15593396127223969\n",
      "step :  23 loss :  0.124984510242939\n",
      "step :  24 loss :  0.1175733208656311\n",
      "step :  25 loss :  0.1288757026195526\n",
      "step :  26 loss :  0.1127176582813263\n",
      "step :  27 loss :  0.10432214289903641\n",
      "step :  28 loss :  0.0992056280374527\n",
      "step :  29 loss :  0.09313524514436722\n",
      "step :  30 loss :  0.12269088625907898\n",
      "step :  31 loss :  0.12555928528308868\n",
      "step :  32 loss :  0.11926767230033875\n",
      "step :  33 loss :  0.09723853319883347\n",
      "step :  34 loss :  0.09533008933067322\n",
      "step :  35 loss :  0.09181351959705353\n",
      "step :  36 loss :  0.09428812563419342\n",
      "step :  37 loss :  0.10252965986728668\n",
      "step :  38 loss :  0.09355086833238602\n",
      "step :  39 loss :  0.08851443976163864\n",
      "step :  40 loss :  0.08905147016048431\n",
      "step :  41 loss :  0.11011496931314468\n",
      "step :  42 loss :  0.09433716535568237\n",
      "step :  43 loss :  0.10254521667957306\n",
      "step :  44 loss :  0.09552496671676636\n",
      "step :  45 loss :  0.1028682142496109\n",
      "step :  46 loss :  0.08649829030036926\n",
      "step :  47 loss :  0.0872679054737091\n",
      "step :  48 loss :  0.08973484486341476\n",
      "step :  49 loss :  0.10612919181585312\n",
      "step :  50 loss :  0.08538705855607986\n",
      "step :  51 loss :  0.08534271270036697\n",
      "step :  52 loss :  0.08516747504472733\n",
      "step :  53 loss :  0.08427567780017853\n",
      "step :  54 loss :  0.09896692633628845\n",
      "step :  55 loss :  0.08764693140983582\n",
      "step :  56 loss :  0.10355228185653687\n",
      "step :  57 loss :  0.08590972423553467\n",
      "step :  58 loss :  0.08804214745759964\n",
      "step :  59 loss :  0.08558125793933868\n",
      "step :  60 loss :  0.09543344378471375\n",
      "step :  61 loss :  0.08324165642261505\n",
      "step :  62 loss :  0.09281781315803528\n",
      "step :  63 loss :  0.084213986992836\n",
      "step :  64 loss :  0.08303528279066086\n",
      "step :  65 loss :  0.08633691072463989\n",
      "step :  66 loss :  0.08508143573999405\n",
      "step :  67 loss :  0.08296164125204086\n",
      "step :  68 loss :  0.08389284461736679\n",
      "step :  69 loss :  0.08424674719572067\n",
      "step :  70 loss :  0.08214103430509567\n",
      "step :  71 loss :  0.08697231113910675\n",
      "step :  72 loss :  0.08246183395385742\n",
      "step :  73 loss :  0.09083035588264465\n",
      "step :  74 loss :  0.08294971287250519\n",
      "step :  75 loss :  0.08358967304229736\n",
      "step :  76 loss :  0.08536039292812347\n",
      "step :  77 loss :  0.08206833153963089\n",
      "step :  78 loss :  0.08185908943414688\n",
      "step :  79 loss :  0.08333628624677658\n",
      "step :  80 loss :  0.0824296697974205\n",
      "step :  81 loss :  0.08443783968687057\n",
      "step :  82 loss :  0.08252506703138351\n",
      "step :  83 loss :  0.08705424517393112\n",
      "step :  84 loss :  0.08457707613706589\n",
      "step :  85 loss :  0.08437083661556244\n",
      "step :  86 loss :  0.08163046091794968\n",
      "step :  87 loss :  0.08291620016098022\n",
      "step :  88 loss :  0.08129893988370895\n",
      "step :  89 loss :  0.08262556046247482\n",
      "step :  90 loss :  0.08385851979255676\n",
      "step :  91 loss :  0.083476223051548\n",
      "step :  92 loss :  0.08362583070993423\n",
      "step :  93 loss :  0.08137216418981552\n",
      "step :  94 loss :  0.08385799080133438\n",
      "step :  95 loss :  0.08103726804256439\n",
      "step :  96 loss :  0.08103249222040176\n",
      "step :  97 loss :  0.08229201287031174\n",
      "step :  98 loss :  0.08135675638914108\n",
      "step :  99 loss :  0.0808190330862999\n",
      "step :  100 loss :  0.08241748064756393\n",
      "step :  101 loss :  0.08292381465435028\n",
      "step :  102 loss :  0.08324451744556427\n",
      "step :  103 loss :  0.08344731479883194\n",
      "step :  104 loss :  0.08149232715368271\n",
      "step :  105 loss :  0.0825435072183609\n",
      "step :  106 loss :  0.08388083428144455\n",
      "step :  107 loss :  0.08262459188699722\n",
      "step :  108 loss :  0.08080806583166122\n",
      "step :  109 loss :  0.08090701699256897\n",
      "step :  110 loss :  0.08100838959217072\n",
      "step :  111 loss :  0.083012655377388\n",
      "step :  112 loss :  0.08255030959844589\n",
      "step :  113 loss :  0.08059873431921005\n",
      "step :  114 loss :  0.08146888762712479\n",
      "step :  115 loss :  0.08078035712242126\n",
      "step :  116 loss :  0.08039598166942596\n",
      "step :  117 loss :  0.08053039759397507\n",
      "step :  118 loss :  0.08064830303192139\n",
      "step :  119 loss :  0.08339037001132965\n",
      "step :  120 loss :  0.082476407289505\n",
      "step :  121 loss :  0.08075857907533646\n",
      "step :  122 loss :  0.08054611086845398\n",
      "step :  123 loss :  0.08090231567621231\n",
      "step :  124 loss :  0.08089111000299454\n",
      "step :  125 loss :  0.08161576837301254\n",
      "step :  126 loss :  0.08097949624061584\n",
      "step :  127 loss :  0.08032047003507614\n",
      "step :  128 loss :  0.08040126413106918\n",
      "step :  129 loss :  0.08036137372255325\n",
      "step :  130 loss :  0.08034178614616394\n",
      "step :  131 loss :  0.08027073740959167\n",
      "step :  132 loss :  0.08021243661642075\n",
      "step :  133 loss :  0.08131537586450577\n",
      "step :  134 loss :  0.08082838356494904\n",
      "step :  135 loss :  0.0805995911359787\n",
      "step :  136 loss :  0.08018738031387329\n",
      "step :  137 loss :  0.08052040636539459\n",
      "step :  138 loss :  0.08011725544929504\n",
      "step :  139 loss :  0.08010305464267731\n",
      "step :  140 loss :  0.08017593622207642\n",
      "step :  141 loss :  0.0809093564748764\n",
      "step :  142 loss :  0.08019472658634186\n",
      "step :  143 loss :  0.08026471734046936\n",
      "step :  144 loss :  0.0807463526725769\n",
      "step :  145 loss :  0.08042892068624496\n",
      "step :  146 loss :  0.08085913956165314\n",
      "step :  147 loss :  0.08006880432367325\n",
      "step :  148 loss :  0.08040548861026764\n",
      "step :  149 loss :  0.08046030253171921\n",
      "step :  150 loss :  0.08020202815532684\n",
      "step :  151 loss :  0.08017484098672867\n",
      "step :  152 loss :  0.08024469017982483\n",
      "step :  153 loss :  0.07997962832450867\n",
      "step :  154 loss :  0.07999318838119507\n",
      "step :  155 loss :  0.08027436584234238\n",
      "step :  156 loss :  0.07994931936264038\n",
      "step :  157 loss :  0.08007653802633286\n",
      "step :  158 loss :  0.07996247708797455\n",
      "step :  159 loss :  0.08002159744501114\n",
      "step :  160 loss :  0.08009541034698486\n",
      "step :  161 loss :  0.08006835728883743\n",
      "step :  162 loss :  0.07998073846101761\n",
      "step :  163 loss :  0.08027072250843048\n",
      "step :  164 loss :  0.07997110486030579\n",
      "step :  165 loss :  0.08002867549657822\n",
      "step :  166 loss :  0.07991863042116165\n",
      "step :  167 loss :  0.07994157075881958\n",
      "step :  168 loss :  0.0800069198012352\n",
      "step :  169 loss :  0.08026567846536636\n",
      "step :  170 loss :  0.07989468425512314\n",
      "step :  171 loss :  0.08023259788751602\n",
      "step :  172 loss :  0.07991524040699005\n",
      "step :  173 loss :  0.08034966140985489\n",
      "step :  174 loss :  0.07994505763053894\n",
      "step :  175 loss :  0.07995607703924179\n",
      "step :  176 loss :  0.07993929833173752\n",
      "step :  177 loss :  0.0799088254570961\n",
      "step :  178 loss :  0.08005423843860626\n",
      "step :  179 loss :  0.07994621992111206\n",
      "step :  180 loss :  0.0798511877655983\n",
      "step :  181 loss :  0.07990453392267227\n",
      "step :  182 loss :  0.0799313634634018\n",
      "step :  183 loss :  0.07988126575946808\n",
      "step :  184 loss :  0.07994119822978973\n",
      "step :  185 loss :  0.07986333221197128\n",
      "step :  186 loss :  0.07987560331821442\n",
      "step :  187 loss :  0.07996413111686707\n",
      "step :  188 loss :  0.07987896353006363\n",
      "step :  189 loss :  0.07984646409749985\n",
      "step :  190 loss :  0.07986090332269669\n",
      "step :  191 loss :  0.07988330721855164\n",
      "step :  192 loss :  0.07987909764051437\n",
      "step :  193 loss :  0.07982789725065231\n",
      "step :  194 loss :  0.07985585927963257\n",
      "step :  195 loss :  0.07988956570625305\n",
      "step :  196 loss :  0.07982301712036133\n",
      "step :  197 loss :  0.07984941452741623\n",
      "step :  198 loss :  0.07983984798192978\n",
      "step :  199 loss :  0.07991358637809753\n",
      "step :  200 loss :  0.07983732223510742\n",
      "step :  201 loss :  0.07983379065990448\n",
      "step :  202 loss :  0.07982256263494492\n",
      "step :  203 loss :  0.07983603328466415\n",
      "step :  204 loss :  0.07986698299646378\n",
      "step :  205 loss :  0.07986090332269669\n",
      "step :  206 loss :  0.07981643825769424\n",
      "step :  207 loss :  0.07980339974164963\n",
      "step :  208 loss :  0.07983918488025665\n",
      "step :  209 loss :  0.07980972528457642\n",
      "step :  210 loss :  0.07983545958995819\n",
      "step :  211 loss :  0.07980063557624817\n",
      "step :  212 loss :  0.07980810105800629\n",
      "step :  213 loss :  0.07984352856874466\n",
      "step :  214 loss :  0.07982146739959717\n",
      "step :  215 loss :  0.07979466766119003\n",
      "step :  216 loss :  0.07983166724443436\n",
      "step :  217 loss :  0.0797886922955513\n",
      "step :  218 loss :  0.07990016788244247\n",
      "step :  219 loss :  0.07979506254196167\n",
      "step :  220 loss :  0.0798693373799324\n",
      "step :  221 loss :  0.07979259639978409\n",
      "step :  222 loss :  0.07986491918563843\n",
      "step :  223 loss :  0.07978425174951553\n",
      "step :  224 loss :  0.0798473134636879\n",
      "step :  225 loss :  0.07979271560907364\n",
      "step :  226 loss :  0.07983391731977463\n",
      "step :  227 loss :  0.07977952063083649\n",
      "step :  228 loss :  0.07978492975234985\n",
      "step :  229 loss :  0.07981124520301819\n",
      "step :  230 loss :  0.07978060841560364\n",
      "step :  231 loss :  0.07989780604839325\n",
      "step :  232 loss :  0.07977530360221863\n",
      "step :  233 loss :  0.07983963936567307\n",
      "step :  234 loss :  0.07976976782083511\n",
      "step :  235 loss :  0.07977661490440369\n",
      "step :  236 loss :  0.07977510988712311\n",
      "step :  237 loss :  0.07978234440088272\n",
      "step :  238 loss :  0.07979805022478104\n",
      "step :  239 loss :  0.07977348566055298\n",
      "step :  240 loss :  0.07978910207748413\n",
      "step :  241 loss :  0.07976507395505905\n",
      "step :  242 loss :  0.0797797292470932\n",
      "step :  243 loss :  0.07977210730314255\n",
      "step :  244 loss :  0.07977546751499176\n",
      "step :  245 loss :  0.0797821506857872\n",
      "step :  246 loss :  0.07979716360569\n",
      "step :  247 loss :  0.07974988967180252\n",
      "step :  248 loss :  0.07980754226446152\n",
      "step :  249 loss :  0.0797509178519249\n",
      "step :  250 loss :  0.07978160679340363\n",
      "step :  251 loss :  0.07975707948207855\n",
      "step :  252 loss :  0.07979806512594223\n",
      "step :  253 loss :  0.07974925637245178\n",
      "step :  254 loss :  0.07980808615684509\n",
      "step :  255 loss :  0.07974836230278015\n",
      "step :  256 loss :  0.07978585362434387\n",
      "step :  257 loss :  0.07975738495588303\n",
      "step :  258 loss :  0.07976336777210236\n",
      "step :  259 loss :  0.07976822555065155\n",
      "step :  260 loss :  0.07976585626602173\n",
      "step :  261 loss :  0.07976871728897095\n",
      "step :  262 loss :  0.07975824922323227\n",
      "step :  263 loss :  0.0797678530216217\n",
      "step :  264 loss :  0.07975941151380539\n",
      "step :  265 loss :  0.07976709306240082\n",
      "step :  266 loss :  0.079765684902668\n",
      "step :  267 loss :  0.07975953072309494\n",
      "step :  268 loss :  0.07975202798843384\n",
      "step :  269 loss :  0.07978295534849167\n",
      "step :  270 loss :  0.07975384593009949\n",
      "step :  271 loss :  0.07975070923566818\n",
      "step :  272 loss :  0.07975564152002335\n",
      "step :  273 loss :  0.07974790781736374\n",
      "step :  274 loss :  0.07977521419525146\n",
      "step :  275 loss :  0.07974060624837875\n",
      "step :  276 loss :  0.07976655662059784\n",
      "step :  277 loss :  0.07974553853273392\n",
      "step :  278 loss :  0.07974342256784439\n",
      "step :  279 loss :  0.0797555223107338\n",
      "step :  280 loss :  0.07975705713033676\n",
      "step :  281 loss :  0.07975368946790695\n",
      "step :  282 loss :  0.07974518835544586\n",
      "step :  283 loss :  0.07975757867097855\n",
      "step :  284 loss :  0.07974550127983093\n",
      "step :  285 loss :  0.07976335287094116\n",
      "step :  286 loss :  0.07974191009998322\n",
      "step :  287 loss :  0.07974988967180252\n",
      "step :  288 loss :  0.07974797487258911\n",
      "step :  289 loss :  0.07973194122314453\n",
      "step :  290 loss :  0.07974269986152649\n",
      "step :  291 loss :  0.0797654241323471\n",
      "step :  292 loss :  0.07973138242959976\n",
      "step :  293 loss :  0.07976813614368439\n",
      "step :  294 loss :  0.0797448381781578\n",
      "step :  295 loss :  0.07973527908325195\n",
      "step :  296 loss :  0.07973609119653702\n",
      "step :  297 loss :  0.07974810898303986\n",
      "step :  298 loss :  0.07973609119653702\n",
      "step :  299 loss :  0.07974295318126678\n",
      "step :  300 loss :  0.07974041998386383\n",
      "step :  301 loss :  0.0797460675239563\n",
      "step :  302 loss :  0.07973716408014297\n",
      "step :  303 loss :  0.07975046336650848\n",
      "step :  304 loss :  0.07973930984735489\n",
      "step :  305 loss :  0.07974102348089218\n",
      "step :  306 loss :  0.07974749058485031\n",
      "step :  307 loss :  0.0797332376241684\n",
      "step :  308 loss :  0.079740971326828\n",
      "step :  309 loss :  0.07974198460578918\n",
      "step :  310 loss :  0.07973194122314453\n",
      "step :  311 loss :  0.07973909378051758\n",
      "step :  312 loss :  0.07975087314844131\n",
      "step :  313 loss :  0.07973449677228928\n",
      "step :  314 loss :  0.07973659038543701\n",
      "step :  315 loss :  0.07973891496658325\n",
      "step :  316 loss :  0.07974598556756973\n",
      "step :  317 loss :  0.07973110675811768\n",
      "step :  318 loss :  0.07973527163267136\n",
      "step :  319 loss :  0.07974597811698914\n",
      "step :  320 loss :  0.0797353982925415\n",
      "step :  321 loss :  0.07973675429821014\n",
      "step :  322 loss :  0.07974504679441452\n",
      "step :  323 loss :  0.07973410934209824\n",
      "step :  324 loss :  0.07973916083574295\n",
      "step :  325 loss :  0.07974302768707275\n",
      "step :  326 loss :  0.07973382622003555\n",
      "step :  327 loss :  0.07973755151033401\n",
      "step :  328 loss :  0.07974064350128174\n",
      "step :  329 loss :  0.07973376661539078\n",
      "step :  330 loss :  0.0797344297170639\n",
      "step :  331 loss :  0.07974334061145782\n",
      "step :  332 loss :  0.07973167300224304\n",
      "step :  333 loss :  0.07973241060972214\n",
      "step :  334 loss :  0.07974550873041153\n",
      "step :  335 loss :  0.07973098754882812\n",
      "step :  336 loss :  0.07973421365022659\n",
      "step :  337 loss :  0.07975032180547714\n",
      "step :  338 loss :  0.07973048090934753\n",
      "step :  339 loss :  0.07972848415374756\n",
      "step :  340 loss :  0.07973523437976837\n",
      "step :  341 loss :  0.07974107563495636\n",
      "step :  342 loss :  0.07973083853721619\n",
      "step :  343 loss :  0.07973268628120422\n",
      "step :  344 loss :  0.07973305135965347\n",
      "step :  345 loss :  0.07973548769950867\n",
      "step :  346 loss :  0.07972904294729233\n",
      "step :  347 loss :  0.07973239570856094\n",
      "step :  348 loss :  0.07974301278591156\n",
      "step :  349 loss :  0.07973019033670425\n",
      "step :  350 loss :  0.07972866296768188\n",
      "step :  351 loss :  0.079734206199646\n",
      "step :  352 loss :  0.07973653823137283\n",
      "step :  353 loss :  0.07973050326108932\n",
      "step :  354 loss :  0.07973282039165497\n",
      "step :  355 loss :  0.07973575592041016\n",
      "step :  356 loss :  0.07973134517669678\n",
      "step :  357 loss :  0.07973077148199081\n",
      "step :  358 loss :  0.07974020391702652\n",
      "step :  359 loss :  0.07973167300224304\n",
      "step :  360 loss :  0.0797310471534729\n",
      "step :  361 loss :  0.0797354206442833\n",
      "step :  362 loss :  0.0797329917550087\n",
      "step :  363 loss :  0.07973022013902664\n",
      "step :  364 loss :  0.0797356441617012\n",
      "step :  365 loss :  0.07973085343837738\n",
      "step :  366 loss :  0.07973583042621613\n",
      "step :  367 loss :  0.07973044365644455\n",
      "step :  368 loss :  0.07972995191812515\n",
      "step :  369 loss :  0.07973406463861465\n",
      "step :  370 loss :  0.07973150908946991\n",
      "step :  371 loss :  0.07973062247037888\n",
      "step :  372 loss :  0.07973403483629227\n",
      "step :  373 loss :  0.07973005622625351\n",
      "step :  374 loss :  0.07973209023475647\n",
      "step :  375 loss :  0.07972952723503113\n",
      "step :  376 loss :  0.07973083853721619\n",
      "step :  377 loss :  0.07973399013280869\n",
      "step :  378 loss :  0.07972995936870575\n",
      "step :  379 loss :  0.07973048090934753\n",
      "step :  380 loss :  0.07973554730415344\n",
      "step :  381 loss :  0.07973044365644455\n",
      "step :  382 loss :  0.07973185926675797\n",
      "step :  383 loss :  0.07973024249076843\n",
      "step :  384 loss :  0.07973061501979828\n",
      "step :  385 loss :  0.07973036915063858\n",
      "step :  386 loss :  0.07973244041204453\n",
      "step :  387 loss :  0.07972978800535202\n",
      "step :  388 loss :  0.07973349094390869\n",
      "step :  389 loss :  0.07972981035709381\n",
      "step :  390 loss :  0.07973125576972961\n",
      "step :  391 loss :  0.07972975075244904\n",
      "step :  392 loss :  0.07972926646471024\n",
      "step :  393 loss :  0.07973191142082214\n",
      "step :  394 loss :  0.07972961664199829\n",
      "step :  395 loss :  0.07973134517669678\n",
      "step :  396 loss :  0.07972907274961472\n",
      "step :  397 loss :  0.07973136007785797\n",
      "step :  398 loss :  0.07972917705774307\n",
      "step :  399 loss :  0.07973087579011917\n",
      "step :  400 loss :  0.079729825258255\n",
      "step :  401 loss :  0.07972940802574158\n",
      "step :  402 loss :  0.0797295868396759\n",
      "step :  403 loss :  0.0797315388917923\n",
      "step :  404 loss :  0.07972890883684158\n",
      "step :  405 loss :  0.0797303169965744\n",
      "step :  406 loss :  0.0797288715839386\n",
      "step :  407 loss :  0.07973074167966843\n",
      "step :  408 loss :  0.07972870767116547\n",
      "step :  409 loss :  0.07973029464483261\n",
      "step :  410 loss :  0.079729363322258\n",
      "step :  411 loss :  0.07972868531942368\n",
      "step :  412 loss :  0.07972942292690277\n",
      "step :  413 loss :  0.07972848415374756\n",
      "step :  414 loss :  0.07972951233386993\n",
      "step :  415 loss :  0.07972922176122665\n",
      "step :  416 loss :  0.07972832024097443\n",
      "step :  417 loss :  0.07973068207502365\n",
      "step :  418 loss :  0.07972832024097443\n",
      "step :  419 loss :  0.07972833514213562\n",
      "step :  420 loss :  0.07973013073205948\n",
      "step :  421 loss :  0.07972842454910278\n",
      "step :  422 loss :  0.07972826808691025\n",
      "step :  423 loss :  0.07973062992095947\n",
      "step :  424 loss :  0.07972855865955353\n",
      "step :  425 loss :  0.07972857356071472\n",
      "step :  426 loss :  0.07973168790340424\n",
      "step :  427 loss :  0.07972842454910278\n",
      "step :  428 loss :  0.07972809672355652\n",
      "step :  429 loss :  0.07972918450832367\n",
      "step :  430 loss :  0.07972969859838486\n",
      "step :  431 loss :  0.07972808182239532\n",
      "step :  432 loss :  0.07972878962755203\n",
      "step :  433 loss :  0.07972856611013412\n",
      "step :  434 loss :  0.07972808927297592\n",
      "step :  435 loss :  0.07972826808691025\n",
      "step :  436 loss :  0.07973000407218933\n",
      "step :  437 loss :  0.07972818613052368\n",
      "step :  438 loss :  0.07972775399684906\n",
      "step :  439 loss :  0.07972923666238785\n",
      "step :  440 loss :  0.07972830533981323\n",
      "step :  441 loss :  0.0797281265258789\n",
      "step :  442 loss :  0.07972902059555054\n",
      "step :  443 loss :  0.07972834259271622\n",
      "step :  444 loss :  0.07972794771194458\n",
      "step :  445 loss :  0.07972810417413712\n",
      "step :  446 loss :  0.07972867786884308\n",
      "step :  447 loss :  0.07972782850265503\n",
      "step :  448 loss :  0.07972799241542816\n",
      "step :  449 loss :  0.07972928881645203\n",
      "step :  450 loss :  0.07972791790962219\n",
      "step :  451 loss :  0.07972800731658936\n",
      "step :  452 loss :  0.0797291174530983\n",
      "step :  453 loss :  0.07972840964794159\n",
      "step :  454 loss :  0.07972775399684906\n",
      "step :  455 loss :  0.07972860336303711\n",
      "step :  456 loss :  0.07972799241542816\n",
      "step :  457 loss :  0.07972852140665054\n",
      "step :  458 loss :  0.07972779124975204\n",
      "step :  459 loss :  0.07972797006368637\n",
      "step :  460 loss :  0.07972850650548935\n",
      "step :  461 loss :  0.0797279104590416\n",
      "step :  462 loss :  0.07972844690084457\n",
      "step :  463 loss :  0.079727903008461\n",
      "step :  464 loss :  0.07972782850265503\n",
      "step :  465 loss :  0.0797291100025177\n",
      "step :  466 loss :  0.07972809672355652\n",
      "step :  467 loss :  0.07972757518291473\n",
      "step :  468 loss :  0.07972794771194458\n",
      "step :  469 loss :  0.0797281563282013\n",
      "step :  470 loss :  0.07972830533981323\n",
      "step :  471 loss :  0.07972776889801025\n",
      "step :  472 loss :  0.07972778379917145\n",
      "step :  473 loss :  0.0797286257147789\n",
      "step :  474 loss :  0.07972779870033264\n",
      "step :  475 loss :  0.07972801476716995\n",
      "step :  476 loss :  0.07972771674394608\n",
      "step :  477 loss :  0.07972844690084457\n",
      "step :  478 loss :  0.07972776889801025\n",
      "step :  479 loss :  0.07972761988639832\n",
      "step :  480 loss :  0.07972842454910278\n",
      "step :  481 loss :  0.07972776144742966\n",
      "step :  482 loss :  0.0797276422381401\n",
      "step :  483 loss :  0.07972794026136398\n",
      "step :  484 loss :  0.0797276496887207\n",
      "step :  485 loss :  0.07972782105207443\n",
      "step :  486 loss :  0.07972776889801025\n",
      "step :  487 loss :  0.07972770929336548\n",
      "step :  488 loss :  0.07972784340381622\n",
      "step :  489 loss :  0.07972757518291473\n",
      "step :  490 loss :  0.07972786575555801\n",
      "step :  491 loss :  0.07972783595323563\n",
      "step :  492 loss :  0.07972761988639832\n",
      "step :  493 loss :  0.07972771674394608\n",
      "step :  494 loss :  0.07972816377878189\n",
      "step :  495 loss :  0.0797276422381401\n",
      "step :  496 loss :  0.07972779870033264\n",
      "step :  497 loss :  0.0797276720404625\n",
      "step :  498 loss :  0.07972780615091324\n",
      "step :  499 loss :  0.07972760498523712\n",
      "Training model 9\n",
      "step :  0 loss :  0.36360597610473633\n",
      "step :  1 loss :  0.285788893699646\n",
      "step :  2 loss :  0.20390860736370087\n",
      "step :  3 loss :  0.2225194275379181\n",
      "step :  4 loss :  0.1897432655096054\n",
      "step :  5 loss :  0.20558270812034607\n",
      "step :  6 loss :  0.20984645187854767\n",
      "step :  7 loss :  0.2324891984462738\n",
      "step :  8 loss :  0.19606074690818787\n",
      "step :  9 loss :  0.22210603952407837\n",
      "step :  10 loss :  0.19579224288463593\n",
      "step :  11 loss :  0.17346791923046112\n",
      "step :  12 loss :  0.16765384376049042\n",
      "step :  13 loss :  0.16047222912311554\n",
      "step :  14 loss :  0.1427813470363617\n",
      "step :  15 loss :  0.13156536221504211\n",
      "step :  16 loss :  0.1419539451599121\n",
      "step :  17 loss :  0.13420991599559784\n",
      "step :  18 loss :  0.14215721189975739\n",
      "step :  19 loss :  0.12526114284992218\n",
      "step :  20 loss :  0.23894177377223969\n",
      "step :  21 loss :  0.13281479477882385\n",
      "step :  22 loss :  0.11683076620101929\n",
      "step :  23 loss :  0.13711009919643402\n",
      "step :  24 loss :  0.1465582400560379\n",
      "step :  25 loss :  0.13829553127288818\n",
      "step :  26 loss :  0.12138853222131729\n",
      "step :  27 loss :  0.13929696381092072\n",
      "step :  28 loss :  0.10926593840122223\n",
      "step :  29 loss :  0.11524355411529541\n",
      "step :  30 loss :  0.12432035058736801\n",
      "step :  31 loss :  0.10579061508178711\n",
      "step :  32 loss :  0.14116604626178741\n",
      "step :  33 loss :  0.11670547723770142\n",
      "step :  34 loss :  0.10738920420408249\n",
      "step :  35 loss :  0.10037475824356079\n",
      "step :  36 loss :  0.11782092601060867\n",
      "step :  37 loss :  0.11569735407829285\n",
      "step :  38 loss :  0.09695148468017578\n",
      "step :  39 loss :  0.09990793466567993\n",
      "step :  40 loss :  0.09620744735002518\n",
      "step :  41 loss :  0.09455244243144989\n",
      "step :  42 loss :  0.10322477668523788\n",
      "step :  43 loss :  0.09422622621059418\n",
      "step :  44 loss :  0.12371467053890228\n",
      "step :  45 loss :  0.09056659787893295\n",
      "step :  46 loss :  0.09362170845270157\n",
      "step :  47 loss :  0.08942963927984238\n",
      "step :  48 loss :  0.08782818913459778\n",
      "step :  49 loss :  0.08707387745380402\n",
      "step :  50 loss :  0.09177129715681076\n",
      "step :  51 loss :  0.09432707726955414\n",
      "step :  52 loss :  0.1149028018116951\n",
      "step :  53 loss :  0.09161148220300674\n",
      "step :  54 loss :  0.093462735414505\n",
      "step :  55 loss :  0.08513176441192627\n",
      "step :  56 loss :  0.08571512252092361\n",
      "step :  57 loss :  0.08216187357902527\n",
      "step :  58 loss :  0.09745825082063675\n",
      "step :  59 loss :  0.09640290588140488\n",
      "step :  60 loss :  0.09022245556116104\n",
      "step :  61 loss :  0.10027631372213364\n",
      "step :  62 loss :  0.08350290358066559\n",
      "step :  63 loss :  0.08184923231601715\n",
      "step :  64 loss :  0.08228184282779694\n",
      "step :  65 loss :  0.09807682782411575\n",
      "step :  66 loss :  0.09278079867362976\n",
      "step :  67 loss :  0.08386535942554474\n",
      "step :  68 loss :  0.08474169671535492\n",
      "step :  69 loss :  0.08342224359512329\n",
      "step :  70 loss :  0.08835569024085999\n",
      "step :  71 loss :  0.08559367060661316\n",
      "step :  72 loss :  0.0818566158413887\n",
      "step :  73 loss :  0.09194686263799667\n",
      "step :  74 loss :  0.08482084423303604\n",
      "step :  75 loss :  0.08259940892457962\n",
      "step :  76 loss :  0.08195082098245621\n",
      "step :  77 loss :  0.08335127681493759\n",
      "step :  78 loss :  0.08073263615369797\n",
      "step :  79 loss :  0.08404339849948883\n",
      "step :  80 loss :  0.08227678388357162\n",
      "step :  81 loss :  0.08424868434667587\n",
      "step :  82 loss :  0.08690282702445984\n",
      "step :  83 loss :  0.08369721472263336\n",
      "step :  84 loss :  0.0831374004483223\n",
      "step :  85 loss :  0.0808231383562088\n",
      "step :  86 loss :  0.08063067495822906\n",
      "step :  87 loss :  0.08000901341438293\n",
      "step :  88 loss :  0.08464104682207108\n",
      "step :  89 loss :  0.0819280818104744\n",
      "step :  90 loss :  0.0845421776175499\n",
      "step :  91 loss :  0.08348753303289413\n",
      "step :  92 loss :  0.08023114502429962\n",
      "step :  93 loss :  0.08106517046689987\n",
      "step :  94 loss :  0.07993251830339432\n",
      "step :  95 loss :  0.08336196839809418\n",
      "step :  96 loss :  0.08112437278032303\n",
      "step :  97 loss :  0.08297945559024811\n",
      "step :  98 loss :  0.08092817664146423\n",
      "step :  99 loss :  0.08041729032993317\n",
      "step :  100 loss :  0.08193856477737427\n",
      "step :  101 loss :  0.0822053775191307\n",
      "step :  102 loss :  0.07930224388837814\n",
      "step :  103 loss :  0.07937908172607422\n",
      "step :  104 loss :  0.07974020391702652\n",
      "step :  105 loss :  0.08068528026342392\n",
      "step :  106 loss :  0.08000390976667404\n",
      "step :  107 loss :  0.07943262904882431\n",
      "step :  108 loss :  0.08131749927997589\n",
      "step :  109 loss :  0.07979249209165573\n",
      "step :  110 loss :  0.08020643144845963\n",
      "step :  111 loss :  0.08167559653520584\n",
      "step :  112 loss :  0.08241520076990128\n",
      "step :  113 loss :  0.0799449011683464\n",
      "step :  114 loss :  0.07908590883016586\n",
      "step :  115 loss :  0.07950180023908615\n",
      "step :  116 loss :  0.08073145896196365\n",
      "step :  117 loss :  0.0807122066617012\n",
      "step :  118 loss :  0.07982195913791656\n",
      "step :  119 loss :  0.0793313980102539\n",
      "step :  120 loss :  0.07879243046045303\n",
      "step :  121 loss :  0.07961016893386841\n",
      "step :  122 loss :  0.07913196831941605\n",
      "step :  123 loss :  0.07876700162887573\n",
      "step :  124 loss :  0.07929347455501556\n",
      "step :  125 loss :  0.07991490513086319\n",
      "step :  126 loss :  0.0801415666937828\n",
      "step :  127 loss :  0.0794588178396225\n",
      "step :  128 loss :  0.07910078018903732\n",
      "step :  129 loss :  0.0788273811340332\n",
      "step :  130 loss :  0.0793517455458641\n",
      "step :  131 loss :  0.07937322556972504\n",
      "step :  132 loss :  0.0794859305024147\n",
      "step :  133 loss :  0.07888904958963394\n",
      "step :  134 loss :  0.07892629504203796\n",
      "step :  135 loss :  0.07865630090236664\n",
      "step :  136 loss :  0.07897626608610153\n",
      "step :  137 loss :  0.0788770467042923\n",
      "step :  138 loss :  0.07871521264314651\n",
      "step :  139 loss :  0.07861248403787613\n",
      "step :  140 loss :  0.07894953340291977\n",
      "step :  141 loss :  0.07866628468036652\n",
      "step :  142 loss :  0.0790574848651886\n",
      "step :  143 loss :  0.07872465252876282\n",
      "step :  144 loss :  0.07873248308897018\n",
      "step :  145 loss :  0.07852048426866531\n",
      "step :  146 loss :  0.07900317758321762\n",
      "step :  147 loss :  0.07861433178186417\n",
      "step :  148 loss :  0.07902055978775024\n",
      "step :  149 loss :  0.07856471836566925\n",
      "step :  150 loss :  0.07885800302028656\n",
      "step :  151 loss :  0.0784294530749321\n",
      "step :  152 loss :  0.07906344532966614\n",
      "step :  153 loss :  0.07845667004585266\n",
      "step :  154 loss :  0.07933801412582397\n",
      "step :  155 loss :  0.07841451466083527\n",
      "step :  156 loss :  0.07904742658138275\n",
      "step :  157 loss :  0.07835926860570908\n",
      "step :  158 loss :  0.07895573228597641\n",
      "step :  159 loss :  0.07847683876752853\n",
      "step :  160 loss :  0.0788881927728653\n",
      "step :  161 loss :  0.07846102863550186\n",
      "step :  162 loss :  0.07852993160486221\n",
      "step :  163 loss :  0.07856948673725128\n",
      "step :  164 loss :  0.07835755497217178\n",
      "step :  165 loss :  0.07875358313322067\n",
      "step :  166 loss :  0.07840286195278168\n",
      "step :  167 loss :  0.0788005068898201\n",
      "step :  168 loss :  0.07836342602968216\n",
      "step :  169 loss :  0.07869545370340347\n",
      "step :  170 loss :  0.0783572793006897\n",
      "step :  171 loss :  0.07865050435066223\n",
      "step :  172 loss :  0.07846923917531967\n",
      "step :  173 loss :  0.07851780951023102\n",
      "step :  174 loss :  0.07833303511142731\n",
      "step :  175 loss :  0.078466035425663\n",
      "step :  176 loss :  0.07835329324007034\n",
      "step :  177 loss :  0.07872650772333145\n",
      "step :  178 loss :  0.07842528074979782\n",
      "step :  179 loss :  0.0783236101269722\n",
      "step :  180 loss :  0.07846549153327942\n",
      "step :  181 loss :  0.07837604731321335\n",
      "step :  182 loss :  0.07831741869449615\n",
      "step :  183 loss :  0.07832679152488708\n",
      "step :  184 loss :  0.07848681509494781\n",
      "step :  185 loss :  0.07830586284399033\n",
      "step :  186 loss :  0.07845944911241531\n",
      "step :  187 loss :  0.07840126007795334\n",
      "step :  188 loss :  0.07833484560251236\n",
      "step :  189 loss :  0.0783415287733078\n",
      "step :  190 loss :  0.07832672446966171\n",
      "step :  191 loss :  0.07834076136350632\n",
      "step :  192 loss :  0.07837328314781189\n",
      "step :  193 loss :  0.07836084067821503\n",
      "step :  194 loss :  0.07838226109743118\n",
      "step :  195 loss :  0.07829935103654861\n",
      "step :  196 loss :  0.07831966876983643\n",
      "step :  197 loss :  0.07831073552370071\n",
      "step :  198 loss :  0.07828548550605774\n",
      "step :  199 loss :  0.078439861536026\n",
      "step :  200 loss :  0.07838239520788193\n",
      "step :  201 loss :  0.07828667759895325\n",
      "step :  202 loss :  0.07838143408298492\n",
      "step :  203 loss :  0.0782802402973175\n",
      "step :  204 loss :  0.07830002903938293\n",
      "step :  205 loss :  0.07830099761486053\n",
      "step :  206 loss :  0.0782817006111145\n",
      "step :  207 loss :  0.07827898114919662\n",
      "step :  208 loss :  0.07827692478895187\n",
      "step :  209 loss :  0.07842431217432022\n",
      "step :  210 loss :  0.07831905037164688\n",
      "step :  211 loss :  0.07831131666898727\n",
      "step :  212 loss :  0.07838127017021179\n",
      "step :  213 loss :  0.07830428332090378\n",
      "step :  214 loss :  0.07828086614608765\n",
      "step :  215 loss :  0.07830607891082764\n",
      "step :  216 loss :  0.07826115190982819\n",
      "step :  217 loss :  0.07827991247177124\n",
      "step :  218 loss :  0.0782429575920105\n",
      "step :  219 loss :  0.07833888381719589\n",
      "step :  220 loss :  0.07832764834165573\n",
      "step :  221 loss :  0.07832396030426025\n",
      "step :  222 loss :  0.07828370481729507\n",
      "step :  223 loss :  0.07827772200107574\n",
      "step :  224 loss :  0.07827331125736237\n",
      "step :  225 loss :  0.07823965698480606\n",
      "step :  226 loss :  0.0783003717660904\n",
      "step :  227 loss :  0.07823698222637177\n",
      "step :  228 loss :  0.07838018983602524\n",
      "step :  229 loss :  0.07828047126531601\n",
      "step :  230 loss :  0.07834485918283463\n",
      "step :  231 loss :  0.07825523614883423\n",
      "step :  232 loss :  0.07827020436525345\n",
      "step :  233 loss :  0.07827883958816528\n",
      "step :  234 loss :  0.07823321968317032\n",
      "step :  235 loss :  0.07832212001085281\n",
      "step :  236 loss :  0.07824590057134628\n",
      "step :  237 loss :  0.07827435433864594\n",
      "step :  238 loss :  0.07826893776655197\n",
      "step :  239 loss :  0.07823846489191055\n",
      "step :  240 loss :  0.07826760411262512\n",
      "step :  241 loss :  0.07823295891284943\n",
      "step :  242 loss :  0.07824826240539551\n",
      "step :  243 loss :  0.07823740690946579\n",
      "step :  244 loss :  0.07825229316949844\n",
      "step :  245 loss :  0.07825663685798645\n",
      "step :  246 loss :  0.07825957238674164\n",
      "step :  247 loss :  0.07822810858488083\n",
      "step :  248 loss :  0.07824405282735825\n",
      "step :  249 loss :  0.07822524011135101\n",
      "step :  250 loss :  0.07822977006435394\n",
      "step :  251 loss :  0.07823425531387329\n",
      "step :  252 loss :  0.07822359353303909\n",
      "step :  253 loss :  0.07825577259063721\n",
      "step :  254 loss :  0.07822371274232864\n",
      "step :  255 loss :  0.07822749018669128\n",
      "step :  256 loss :  0.07824856787919998\n",
      "step :  257 loss :  0.07821141183376312\n",
      "step :  258 loss :  0.07824598252773285\n",
      "step :  259 loss :  0.07822217792272568\n",
      "step :  260 loss :  0.07822459936141968\n",
      "step :  261 loss :  0.07822327315807343\n",
      "step :  262 loss :  0.0782361626625061\n",
      "step :  263 loss :  0.07822535932064056\n",
      "step :  264 loss :  0.0782226100564003\n",
      "step :  265 loss :  0.07821212708950043\n",
      "step :  266 loss :  0.07822544127702713\n",
      "step :  267 loss :  0.07821405678987503\n",
      "step :  268 loss :  0.0782189816236496\n",
      "step :  269 loss :  0.0782179906964302\n",
      "step :  270 loss :  0.07822458446025848\n",
      "step :  271 loss :  0.07821618020534515\n",
      "step :  272 loss :  0.07821595668792725\n",
      "step :  273 loss :  0.07821226865053177\n",
      "step :  274 loss :  0.0782105103135109\n",
      "step :  275 loss :  0.07821934670209885\n",
      "step :  276 loss :  0.0782012864947319\n",
      "step :  277 loss :  0.07821927219629288\n",
      "step :  278 loss :  0.0782075896859169\n",
      "step :  279 loss :  0.07820454239845276\n",
      "step :  280 loss :  0.07820697128772736\n",
      "step :  281 loss :  0.0782134085893631\n",
      "step :  282 loss :  0.07820478826761246\n",
      "step :  283 loss :  0.07821200042963028\n",
      "step :  284 loss :  0.07820673286914825\n",
      "step :  285 loss :  0.07821101695299149\n",
      "step :  286 loss :  0.078225277364254\n",
      "step :  287 loss :  0.07820536941289902\n",
      "step :  288 loss :  0.07821562141180038\n",
      "step :  289 loss :  0.07820824533700943\n",
      "step :  290 loss :  0.07820136100053787\n",
      "step :  291 loss :  0.07821168005466461\n",
      "step :  292 loss :  0.07819739729166031\n",
      "step :  293 loss :  0.0781964659690857\n",
      "step :  294 loss :  0.0782102718949318\n",
      "step :  295 loss :  0.07819969207048416\n",
      "step :  296 loss :  0.07819642871618271\n",
      "step :  297 loss :  0.0781983807682991\n",
      "step :  298 loss :  0.0781928151845932\n",
      "step :  299 loss :  0.07818780094385147\n",
      "step :  300 loss :  0.07820697128772736\n",
      "step :  301 loss :  0.07819131016731262\n",
      "step :  302 loss :  0.07818067073822021\n",
      "step :  303 loss :  0.07819633185863495\n",
      "step :  304 loss :  0.07819732278585434\n",
      "step :  305 loss :  0.07819046825170517\n",
      "step :  306 loss :  0.0781959816813469\n",
      "step :  307 loss :  0.07819491624832153\n",
      "step :  308 loss :  0.07819510251283646\n",
      "step :  309 loss :  0.07820579409599304\n",
      "step :  310 loss :  0.07818908244371414\n",
      "step :  311 loss :  0.07820423692464828\n",
      "step :  312 loss :  0.07818791270256042\n",
      "step :  313 loss :  0.07818738371133804\n",
      "step :  314 loss :  0.07818982750177383\n",
      "step :  315 loss :  0.0781896635890007\n",
      "step :  316 loss :  0.07819011807441711\n",
      "step :  317 loss :  0.0781918540596962\n",
      "step :  318 loss :  0.0781896784901619\n",
      "step :  319 loss :  0.07818900048732758\n",
      "step :  320 loss :  0.07818524539470673\n",
      "step :  321 loss :  0.07818826287984848\n",
      "step :  322 loss :  0.07818835973739624\n",
      "step :  323 loss :  0.07818686962127686\n",
      "step :  324 loss :  0.07818855345249176\n",
      "step :  325 loss :  0.0781896635890007\n",
      "step :  326 loss :  0.07818698137998581\n",
      "step :  327 loss :  0.07818930596113205\n",
      "step :  328 loss :  0.07819005846977234\n",
      "step :  329 loss :  0.07818679511547089\n",
      "step :  330 loss :  0.07818947732448578\n",
      "step :  331 loss :  0.07818553596735\n",
      "step :  332 loss :  0.07818739861249924\n",
      "step :  333 loss :  0.07818742096424103\n",
      "step :  334 loss :  0.0781872570514679\n",
      "step :  335 loss :  0.07818714529275894\n",
      "step :  336 loss :  0.07818511128425598\n",
      "step :  337 loss :  0.07818788290023804\n",
      "step :  338 loss :  0.07818550616502762\n",
      "step :  339 loss :  0.07818751037120819\n",
      "step :  340 loss :  0.07818521559238434\n",
      "step :  341 loss :  0.07818616926670074\n",
      "step :  342 loss :  0.07818654179573059\n",
      "step :  343 loss :  0.07818181067705154\n",
      "step :  344 loss :  0.07818855345249176\n",
      "step :  345 loss :  0.07818732410669327\n",
      "step :  346 loss :  0.07818133383989334\n",
      "step :  347 loss :  0.07818697392940521\n",
      "step :  348 loss :  0.07818896323442459\n",
      "step :  349 loss :  0.07818110287189484\n",
      "step :  350 loss :  0.07818517833948135\n",
      "step :  351 loss :  0.07818776369094849\n",
      "step :  352 loss :  0.078179731965065\n",
      "step :  353 loss :  0.07818222790956497\n",
      "step :  354 loss :  0.0781828984618187\n",
      "step :  355 loss :  0.07818097621202469\n",
      "step :  356 loss :  0.07818128168582916\n",
      "step :  357 loss :  0.07818131148815155\n",
      "step :  358 loss :  0.07818351686000824\n",
      "step :  359 loss :  0.0781819224357605\n",
      "step :  360 loss :  0.07818306982517242\n",
      "step :  361 loss :  0.0781826302409172\n",
      "step :  362 loss :  0.07818502187728882\n",
      "step :  363 loss :  0.07818379253149033\n",
      "step :  364 loss :  0.07818196713924408\n",
      "step :  365 loss :  0.07818160206079483\n",
      "step :  366 loss :  0.0781807079911232\n",
      "step :  367 loss :  0.0781809538602829\n",
      "step :  368 loss :  0.07818114757537842\n",
      "step :  369 loss :  0.07818002253770828\n",
      "step :  370 loss :  0.07818140834569931\n",
      "step :  371 loss :  0.0781819298863411\n",
      "step :  372 loss :  0.07818030565977097\n",
      "step :  373 loss :  0.07818210870027542\n",
      "step :  374 loss :  0.0781812071800232\n",
      "step :  375 loss :  0.07818076014518738\n",
      "step :  376 loss :  0.07818103581666946\n",
      "step :  377 loss :  0.07818128913640976\n",
      "step :  378 loss :  0.07818031311035156\n",
      "step :  379 loss :  0.07817962020635605\n",
      "step :  380 loss :  0.07818108797073364\n",
      "step :  381 loss :  0.07818010449409485\n",
      "step :  382 loss :  0.07818037271499634\n",
      "step :  383 loss :  0.07818040996789932\n",
      "step :  384 loss :  0.07818206399679184\n",
      "step :  385 loss :  0.07818156480789185\n",
      "step :  386 loss :  0.0781802088022232\n",
      "step :  387 loss :  0.07818049192428589\n",
      "step :  388 loss :  0.07818013429641724\n",
      "step :  389 loss :  0.07818040251731873\n",
      "step :  390 loss :  0.07817984372377396\n",
      "step :  391 loss :  0.07818038016557693\n",
      "step :  392 loss :  0.07818145304918289\n",
      "step :  393 loss :  0.07818131148815155\n",
      "step :  394 loss :  0.07817954570055008\n",
      "step :  395 loss :  0.07817980647087097\n",
      "step :  396 loss :  0.07818041741847992\n",
      "step :  397 loss :  0.07818080484867096\n",
      "step :  398 loss :  0.07817958295345306\n",
      "step :  399 loss :  0.07817967981100082\n",
      "step :  400 loss :  0.0781799852848053\n",
      "step :  401 loss :  0.07817912846803665\n",
      "step :  402 loss :  0.07817942649126053\n",
      "step :  403 loss :  0.07818035036325455\n",
      "step :  404 loss :  0.07817979902029037\n",
      "step :  405 loss :  0.0781790241599083\n",
      "step :  406 loss :  0.07817967981100082\n",
      "step :  407 loss :  0.07818014174699783\n",
      "step :  408 loss :  0.07817959040403366\n",
      "step :  409 loss :  0.07817962765693665\n",
      "step :  410 loss :  0.07817929983139038\n",
      "step :  411 loss :  0.07817962765693665\n",
      "step :  412 loss :  0.07817932218313217\n",
      "step :  413 loss :  0.07818005234003067\n",
      "step :  414 loss :  0.07817959040403366\n",
      "step :  415 loss :  0.07817872613668442\n",
      "step :  416 loss :  0.07818005979061127\n",
      "step :  417 loss :  0.07818061858415604\n",
      "step :  418 loss :  0.07817961275577545\n",
      "step :  419 loss :  0.07817858457565308\n",
      "step :  420 loss :  0.07817905396223068\n",
      "step :  421 loss :  0.07817958295345306\n",
      "step :  422 loss :  0.07817927747964859\n",
      "step :  423 loss :  0.07817928493022919\n",
      "step :  424 loss :  0.07817928493022919\n",
      "step :  425 loss :  0.07817880064249039\n",
      "step :  426 loss :  0.07817903906106949\n",
      "step :  427 loss :  0.07817907631397247\n",
      "step :  428 loss :  0.07817909121513367\n",
      "step :  429 loss :  0.07817905396223068\n",
      "step :  430 loss :  0.07817928493022919\n",
      "step :  431 loss :  0.07817935943603516\n",
      "step :  432 loss :  0.07817891240119934\n",
      "step :  433 loss :  0.07817896455526352\n",
      "step :  434 loss :  0.07817929238080978\n",
      "step :  435 loss :  0.07817903906106949\n",
      "step :  436 loss :  0.07817882299423218\n",
      "step :  437 loss :  0.07817882299423218\n",
      "step :  438 loss :  0.07817903906106949\n",
      "step :  439 loss :  0.07817837595939636\n",
      "step :  440 loss :  0.07817891240119934\n",
      "step :  441 loss :  0.07817907631397247\n",
      "step :  442 loss :  0.07817882299423218\n",
      "step :  443 loss :  0.0781780406832695\n",
      "step :  444 loss :  0.07817858457565308\n",
      "step :  445 loss :  0.07817904651165009\n",
      "step :  446 loss :  0.07817861437797546\n",
      "step :  447 loss :  0.0781783014535904\n",
      "step :  448 loss :  0.07817858457565308\n",
      "step :  449 loss :  0.07817899435758591\n",
      "step :  450 loss :  0.07817865163087845\n",
      "step :  451 loss :  0.07817823439836502\n",
      "step :  452 loss :  0.07817837595939636\n",
      "step :  453 loss :  0.07817866653203964\n",
      "step :  454 loss :  0.07817832380533218\n",
      "step :  455 loss :  0.07817848771810532\n",
      "step :  456 loss :  0.07817826420068741\n",
      "step :  457 loss :  0.07817846536636353\n",
      "step :  458 loss :  0.07817858457565308\n",
      "step :  459 loss :  0.07817842066287994\n",
      "step :  460 loss :  0.07817837595939636\n",
      "step :  461 loss :  0.07817856222391129\n",
      "step :  462 loss :  0.0781785398721695\n",
      "step :  463 loss :  0.0781782865524292\n",
      "step :  464 loss :  0.07817810028791428\n",
      "step :  465 loss :  0.07817845046520233\n",
      "step :  466 loss :  0.07817848771810532\n",
      "step :  467 loss :  0.0781782865524292\n",
      "step :  468 loss :  0.07817824929952621\n",
      "step :  469 loss :  0.07817815989255905\n",
      "step :  470 loss :  0.07817819714546204\n",
      "step :  471 loss :  0.0781782865524292\n",
      "step :  472 loss :  0.0781780332326889\n",
      "step :  473 loss :  0.07817821949720383\n",
      "step :  474 loss :  0.07817841321229935\n",
      "step :  475 loss :  0.07817809283733368\n",
      "step :  476 loss :  0.07817795127630234\n",
      "step :  477 loss :  0.07817818969488144\n",
      "step :  478 loss :  0.07817848026752472\n",
      "step :  479 loss :  0.0781782940030098\n",
      "step :  480 loss :  0.0781780555844307\n",
      "step :  481 loss :  0.07817813754081726\n",
      "step :  482 loss :  0.07817821204662323\n",
      "step :  483 loss :  0.07817820459604263\n",
      "step :  484 loss :  0.07817824184894562\n",
      "step :  485 loss :  0.07817806303501129\n",
      "step :  486 loss :  0.07817806303501129\n",
      "step :  487 loss :  0.0781782940030098\n",
      "step :  488 loss :  0.07817824929952621\n",
      "step :  489 loss :  0.07817811518907547\n",
      "step :  490 loss :  0.0781780332326889\n",
      "step :  491 loss :  0.07817815244197845\n",
      "step :  492 loss :  0.07817838340997696\n",
      "step :  493 loss :  0.07817824184894562\n",
      "step :  494 loss :  0.07817806303501129\n",
      "step :  495 loss :  0.0781780257821083\n",
      "step :  496 loss :  0.07817815989255905\n",
      "step :  497 loss :  0.07817819714546204\n",
      "step :  498 loss :  0.07817810028791428\n",
      "step :  499 loss :  0.07817807048559189\n",
      "Training model 10\n",
      "step :  0 loss :  0.2775394022464752\n",
      "step :  1 loss :  0.25668391585350037\n",
      "step :  2 loss :  0.2134181410074234\n",
      "step :  3 loss :  0.22464017570018768\n",
      "step :  4 loss :  0.18823476135730743\n",
      "step :  5 loss :  0.21895571053028107\n",
      "step :  6 loss :  0.21760068833827972\n",
      "step :  7 loss :  0.22846977412700653\n",
      "step :  8 loss :  0.20299381017684937\n",
      "step :  9 loss :  0.20804156363010406\n",
      "step :  10 loss :  0.17744062840938568\n",
      "step :  11 loss :  0.17317235469818115\n",
      "step :  12 loss :  0.1728324294090271\n",
      "step :  13 loss :  0.1656070351600647\n",
      "step :  14 loss :  0.14956128597259521\n",
      "step :  15 loss :  0.1448570191860199\n",
      "step :  16 loss :  0.1250743865966797\n",
      "step :  17 loss :  0.22241277992725372\n",
      "step :  18 loss :  0.16304928064346313\n",
      "step :  19 loss :  0.2269570529460907\n",
      "step :  20 loss :  0.1340564340353012\n",
      "step :  21 loss :  0.11359940469264984\n",
      "step :  22 loss :  0.1564186066389084\n",
      "step :  23 loss :  0.10933344811201096\n",
      "step :  24 loss :  0.1263727992773056\n",
      "step :  25 loss :  0.10871442407369614\n",
      "step :  26 loss :  0.12676934897899628\n",
      "step :  27 loss :  0.16356725990772247\n",
      "step :  28 loss :  0.11072968691587448\n",
      "step :  29 loss :  0.10701582580804825\n",
      "step :  30 loss :  0.10552600771188736\n",
      "step :  31 loss :  0.11732980608940125\n",
      "step :  32 loss :  0.14689025282859802\n",
      "step :  33 loss :  0.10535120218992233\n",
      "step :  34 loss :  0.09550203382968903\n",
      "step :  35 loss :  0.0925716757774353\n",
      "step :  36 loss :  0.12158384174108505\n",
      "step :  37 loss :  0.10241121053695679\n",
      "step :  38 loss :  0.10038615018129349\n",
      "step :  39 loss :  0.11523249000310898\n",
      "step :  40 loss :  0.10351086407899857\n",
      "step :  41 loss :  0.1017078384757042\n",
      "step :  42 loss :  0.09969912469387054\n",
      "step :  43 loss :  0.0892014130949974\n",
      "step :  44 loss :  0.10583161562681198\n",
      "step :  45 loss :  0.0867948904633522\n",
      "step :  46 loss :  0.09814978390932083\n",
      "step :  47 loss :  0.10319440811872482\n",
      "step :  48 loss :  0.09366556257009506\n",
      "step :  49 loss :  0.0884983241558075\n",
      "step :  50 loss :  0.08825386315584183\n",
      "step :  51 loss :  0.0913466066122055\n",
      "step :  52 loss :  0.08763811737298965\n",
      "step :  53 loss :  0.08368847519159317\n",
      "step :  54 loss :  0.08521163463592529\n",
      "step :  55 loss :  0.0882294774055481\n",
      "step :  56 loss :  0.0868958830833435\n",
      "step :  57 loss :  0.10291831940412521\n",
      "step :  58 loss :  0.08626855909824371\n",
      "step :  59 loss :  0.08669588714838028\n",
      "step :  60 loss :  0.08378475904464722\n",
      "step :  61 loss :  0.08586013317108154\n",
      "step :  62 loss :  0.08572878688573837\n",
      "step :  63 loss :  0.0866619348526001\n",
      "step :  64 loss :  0.08740832656621933\n",
      "step :  65 loss :  0.08847551792860031\n",
      "step :  66 loss :  0.08423635363578796\n",
      "step :  67 loss :  0.08306752145290375\n",
      "step :  68 loss :  0.0829210951924324\n",
      "step :  69 loss :  0.08449208736419678\n",
      "step :  70 loss :  0.08281558007001877\n",
      "step :  71 loss :  0.08995678275823593\n",
      "step :  72 loss :  0.0845215916633606\n",
      "step :  73 loss :  0.08669047057628632\n",
      "step :  74 loss :  0.08220871537923813\n",
      "step :  75 loss :  0.08265642821788788\n",
      "step :  76 loss :  0.08233250677585602\n",
      "step :  77 loss :  0.08182069659233093\n",
      "step :  78 loss :  0.08509749919176102\n",
      "step :  79 loss :  0.08183791488409042\n",
      "step :  80 loss :  0.08834760636091232\n",
      "step :  81 loss :  0.08227931708097458\n",
      "step :  82 loss :  0.081853948533535\n",
      "step :  83 loss :  0.08379785716533661\n",
      "step :  84 loss :  0.0815785825252533\n",
      "step :  85 loss :  0.08172047138214111\n",
      "step :  86 loss :  0.0836382508277893\n",
      "step :  87 loss :  0.08130547404289246\n",
      "step :  88 loss :  0.08117085695266724\n",
      "step :  89 loss :  0.08131501823663712\n",
      "step :  90 loss :  0.08163038641214371\n",
      "step :  91 loss :  0.08101700991392136\n",
      "step :  92 loss :  0.08193639665842056\n",
      "step :  93 loss :  0.08229749649763107\n",
      "step :  94 loss :  0.08253105729818344\n",
      "step :  95 loss :  0.08165328949689865\n",
      "step :  96 loss :  0.08168961107730865\n",
      "step :  97 loss :  0.08127535134553909\n",
      "step :  98 loss :  0.08286966383457184\n",
      "step :  99 loss :  0.0806647390127182\n",
      "step :  100 loss :  0.08180160075426102\n",
      "step :  101 loss :  0.08151962608098984\n",
      "step :  102 loss :  0.08109825104475021\n",
      "step :  103 loss :  0.0808546394109726\n",
      "step :  104 loss :  0.0813363566994667\n",
      "step :  105 loss :  0.080609031021595\n",
      "step :  106 loss :  0.08110805600881577\n",
      "step :  107 loss :  0.08103097975254059\n",
      "step :  108 loss :  0.08194097131490707\n",
      "step :  109 loss :  0.0810808464884758\n",
      "step :  110 loss :  0.08122076094150543\n",
      "step :  111 loss :  0.08130895346403122\n",
      "step :  112 loss :  0.08080415427684784\n",
      "step :  113 loss :  0.08066466450691223\n",
      "step :  114 loss :  0.08188479393720627\n",
      "step :  115 loss :  0.08123781532049179\n",
      "step :  116 loss :  0.08092275261878967\n",
      "step :  117 loss :  0.08088438957929611\n",
      "step :  118 loss :  0.08042963594198227\n",
      "step :  119 loss :  0.08157238364219666\n",
      "step :  120 loss :  0.08030858635902405\n",
      "step :  121 loss :  0.0803326815366745\n",
      "step :  122 loss :  0.08035346865653992\n",
      "step :  123 loss :  0.08014646917581558\n",
      "step :  124 loss :  0.08024454861879349\n",
      "step :  125 loss :  0.08060506731271744\n",
      "step :  126 loss :  0.08027515560388565\n",
      "step :  127 loss :  0.08011572062969208\n",
      "step :  128 loss :  0.08024930953979492\n",
      "step :  129 loss :  0.08066040277481079\n",
      "step :  130 loss :  0.08001914620399475\n",
      "step :  131 loss :  0.08020254224538803\n",
      "step :  132 loss :  0.08025866001844406\n",
      "step :  133 loss :  0.08007576316595078\n",
      "step :  134 loss :  0.08047739416360855\n",
      "step :  135 loss :  0.08022327721118927\n",
      "step :  136 loss :  0.08020829409360886\n",
      "step :  137 loss :  0.08008703589439392\n",
      "step :  138 loss :  0.08004831522703171\n",
      "step :  139 loss :  0.08005639910697937\n",
      "step :  140 loss :  0.08039503544569016\n",
      "step :  141 loss :  0.07995861768722534\n",
      "step :  142 loss :  0.08012335002422333\n",
      "step :  143 loss :  0.07987454533576965\n",
      "step :  144 loss :  0.0801352709531784\n",
      "step :  145 loss :  0.08062269538640976\n",
      "step :  146 loss :  0.08001856505870819\n",
      "step :  147 loss :  0.08045129477977753\n",
      "step :  148 loss :  0.07990831136703491\n",
      "step :  149 loss :  0.08029376715421677\n",
      "step :  150 loss :  0.08005926758050919\n",
      "step :  151 loss :  0.0799034982919693\n",
      "step :  152 loss :  0.08004093915224075\n",
      "step :  153 loss :  0.07976048439741135\n",
      "step :  154 loss :  0.08012647926807404\n",
      "step :  155 loss :  0.08016250282526016\n",
      "step :  156 loss :  0.079734668135643\n",
      "step :  157 loss :  0.0800057202577591\n",
      "step :  158 loss :  0.07982870936393738\n",
      "step :  159 loss :  0.0797603651881218\n",
      "step :  160 loss :  0.07984976470470428\n",
      "step :  161 loss :  0.07979482412338257\n",
      "step :  162 loss :  0.08002938330173492\n",
      "step :  163 loss :  0.07975194603204727\n",
      "step :  164 loss :  0.07992038875818253\n",
      "step :  165 loss :  0.07978194206953049\n",
      "step :  166 loss :  0.07973537594079971\n",
      "step :  167 loss :  0.07987005263566971\n",
      "step :  168 loss :  0.07965991646051407\n",
      "step :  169 loss :  0.07980743050575256\n",
      "step :  170 loss :  0.07970500737428665\n",
      "step :  171 loss :  0.07980901747941971\n",
      "step :  172 loss :  0.07964558899402618\n",
      "step :  173 loss :  0.0797392874956131\n",
      "step :  174 loss :  0.07969149202108383\n",
      "step :  175 loss :  0.07965725660324097\n",
      "step :  176 loss :  0.07978060096502304\n",
      "step :  177 loss :  0.0797026976943016\n",
      "step :  178 loss :  0.07981941103935242\n",
      "step :  179 loss :  0.07961588352918625\n",
      "step :  180 loss :  0.0796964094042778\n",
      "step :  181 loss :  0.07966401427984238\n",
      "step :  182 loss :  0.07964014261960983\n",
      "step :  183 loss :  0.07964546233415604\n",
      "step :  184 loss :  0.07959989458322525\n",
      "step :  185 loss :  0.0797131210565567\n",
      "step :  186 loss :  0.0795988067984581\n",
      "step :  187 loss :  0.07966779917478561\n",
      "step :  188 loss :  0.07959605008363724\n",
      "step :  189 loss :  0.07964295148849487\n",
      "step :  190 loss :  0.0796118900179863\n",
      "step :  191 loss :  0.07969635725021362\n",
      "step :  192 loss :  0.07961449027061462\n",
      "step :  193 loss :  0.07962139695882797\n",
      "step :  194 loss :  0.07960660010576248\n",
      "step :  195 loss :  0.07961027324199677\n",
      "step :  196 loss :  0.07956337183713913\n",
      "step :  197 loss :  0.07962443679571152\n",
      "step :  198 loss :  0.07960030436515808\n",
      "step :  199 loss :  0.07958885282278061\n",
      "step :  200 loss :  0.0795404464006424\n",
      "step :  201 loss :  0.07960285991430283\n",
      "step :  202 loss :  0.07950885593891144\n",
      "step :  203 loss :  0.0795733705163002\n",
      "step :  204 loss :  0.07961217314004898\n",
      "step :  205 loss :  0.07952287048101425\n",
      "step :  206 loss :  0.07959006726741791\n",
      "step :  207 loss :  0.07955816388130188\n",
      "step :  208 loss :  0.07950756698846817\n",
      "step :  209 loss :  0.07960189878940582\n",
      "step :  210 loss :  0.07949472218751907\n",
      "step :  211 loss :  0.07962550967931747\n",
      "step :  212 loss :  0.0794699490070343\n",
      "step :  213 loss :  0.07960806041955948\n",
      "step :  214 loss :  0.07951504737138748\n",
      "step :  215 loss :  0.07957584410905838\n",
      "step :  216 loss :  0.07951309531927109\n",
      "step :  217 loss :  0.07954832911491394\n",
      "step :  218 loss :  0.07953613996505737\n",
      "step :  219 loss :  0.07953918725252151\n",
      "step :  220 loss :  0.07953105121850967\n",
      "step :  221 loss :  0.07955513894557953\n",
      "step :  222 loss :  0.07951526343822479\n",
      "step :  223 loss :  0.07954151183366776\n",
      "step :  224 loss :  0.07952606678009033\n",
      "step :  225 loss :  0.07952913641929626\n",
      "step :  226 loss :  0.07952404767274857\n",
      "step :  227 loss :  0.07948973029851913\n",
      "step :  228 loss :  0.07951497286558151\n",
      "step :  229 loss :  0.07948318123817444\n",
      "step :  230 loss :  0.07950060069561005\n",
      "step :  231 loss :  0.07951812446117401\n",
      "step :  232 loss :  0.07948629558086395\n",
      "step :  233 loss :  0.0795336589217186\n",
      "step :  234 loss :  0.07946428656578064\n",
      "step :  235 loss :  0.07948482781648636\n",
      "step :  236 loss :  0.07952899485826492\n",
      "step :  237 loss :  0.07945065200328827\n",
      "step :  238 loss :  0.07952510565519333\n",
      "step :  239 loss :  0.07948724925518036\n",
      "step :  240 loss :  0.07950460910797119\n",
      "step :  241 loss :  0.07947277277708054\n",
      "step :  242 loss :  0.07949592918157578\n",
      "step :  243 loss :  0.07946327328681946\n",
      "step :  244 loss :  0.07948152720928192\n",
      "step :  245 loss :  0.07949477434158325\n",
      "step :  246 loss :  0.07947488874197006\n",
      "step :  247 loss :  0.07951813191175461\n",
      "step :  248 loss :  0.07944580167531967\n",
      "step :  249 loss :  0.07950105518102646\n",
      "step :  250 loss :  0.07948677241802216\n",
      "step :  251 loss :  0.07945599406957626\n",
      "step :  252 loss :  0.07945762574672699\n",
      "step :  253 loss :  0.07951007038354874\n",
      "step :  254 loss :  0.07944357395172119\n",
      "step :  255 loss :  0.07947557419538498\n",
      "step :  256 loss :  0.07947728037834167\n",
      "step :  257 loss :  0.07946042716503143\n",
      "step :  258 loss :  0.07948484271764755\n",
      "step :  259 loss :  0.07945090532302856\n",
      "step :  260 loss :  0.07947003841400146\n",
      "step :  261 loss :  0.0794740691781044\n",
      "step :  262 loss :  0.07944411784410477\n",
      "step :  263 loss :  0.07947466522455215\n",
      "step :  264 loss :  0.07945490628480911\n",
      "step :  265 loss :  0.07945120334625244\n",
      "step :  266 loss :  0.07946120947599411\n",
      "step :  267 loss :  0.07946760207414627\n",
      "step :  268 loss :  0.0794496163725853\n",
      "step :  269 loss :  0.0794413611292839\n",
      "step :  270 loss :  0.07947555184364319\n",
      "step :  271 loss :  0.07943181693553925\n",
      "step :  272 loss :  0.07945515960454941\n",
      "step :  273 loss :  0.07946315407752991\n",
      "step :  274 loss :  0.0794408917427063\n",
      "step :  275 loss :  0.0794425755739212\n",
      "step :  276 loss :  0.07948288321495056\n",
      "step :  277 loss :  0.07943600416183472\n",
      "step :  278 loss :  0.07945241779088974\n",
      "step :  279 loss :  0.0794922262430191\n",
      "step :  280 loss :  0.07943101227283478\n",
      "step :  281 loss :  0.07944878190755844\n",
      "step :  282 loss :  0.07947613298892975\n",
      "step :  283 loss :  0.0794292613863945\n",
      "step :  284 loss :  0.07944101095199585\n",
      "step :  285 loss :  0.07947451621294022\n",
      "step :  286 loss :  0.07942865043878555\n",
      "step :  287 loss :  0.07943128794431686\n",
      "step :  288 loss :  0.079460509121418\n",
      "step :  289 loss :  0.07943334430456161\n",
      "step :  290 loss :  0.0794256404042244\n",
      "step :  291 loss :  0.07944073528051376\n",
      "step :  292 loss :  0.0794474259018898\n",
      "step :  293 loss :  0.07943153381347656\n",
      "step :  294 loss :  0.07943259924650192\n",
      "step :  295 loss :  0.07943557947874069\n",
      "step :  296 loss :  0.07944874465465546\n",
      "step :  297 loss :  0.0794362798333168\n",
      "step :  298 loss :  0.0794372484087944\n",
      "step :  299 loss :  0.0794391855597496\n",
      "step :  300 loss :  0.0794428214430809\n",
      "step :  301 loss :  0.07943741232156754\n",
      "step :  302 loss :  0.07945224642753601\n",
      "step :  303 loss :  0.079438216984272\n",
      "step :  304 loss :  0.07943235337734222\n",
      "step :  305 loss :  0.07943659275770187\n",
      "step :  306 loss :  0.07944092154502869\n",
      "step :  307 loss :  0.07943124324083328\n",
      "step :  308 loss :  0.07943654805421829\n",
      "step :  309 loss :  0.07943688333034515\n",
      "step :  310 loss :  0.07943782955408096\n",
      "step :  311 loss :  0.07942718267440796\n",
      "step :  312 loss :  0.07943496853113174\n",
      "step :  313 loss :  0.07945091277360916\n",
      "step :  314 loss :  0.07943198829889297\n",
      "step :  315 loss :  0.07943165302276611\n",
      "step :  316 loss :  0.07943329960107803\n",
      "step :  317 loss :  0.07943759113550186\n",
      "step :  318 loss :  0.07943183928728104\n",
      "step :  319 loss :  0.0794311985373497\n",
      "step :  320 loss :  0.07944075018167496\n",
      "step :  321 loss :  0.07943269610404968\n",
      "step :  322 loss :  0.07942846417427063\n",
      "step :  323 loss :  0.07943161576986313\n",
      "step :  324 loss :  0.07943564653396606\n",
      "step :  325 loss :  0.07942734658718109\n",
      "step :  326 loss :  0.07943438738584518\n",
      "step :  327 loss :  0.07942787557840347\n",
      "step :  328 loss :  0.07942720502614975\n",
      "step :  329 loss :  0.0794295072555542\n",
      "step :  330 loss :  0.07942939549684525\n",
      "step :  331 loss :  0.07943879812955856\n",
      "step :  332 loss :  0.07942796498537064\n",
      "step :  333 loss :  0.07943113893270493\n",
      "step :  334 loss :  0.07942932844161987\n",
      "step :  335 loss :  0.07943465560674667\n",
      "step :  336 loss :  0.07942835241556168\n",
      "step :  337 loss :  0.07942257821559906\n",
      "step :  338 loss :  0.07942910492420197\n",
      "step :  339 loss :  0.07943630963563919\n",
      "step :  340 loss :  0.07943986356258392\n",
      "step :  341 loss :  0.07942603528499603\n",
      "step :  342 loss :  0.07942566275596619\n",
      "step :  343 loss :  0.07943449914455414\n",
      "step :  344 loss :  0.07942783087491989\n",
      "step :  345 loss :  0.07942988723516464\n",
      "step :  346 loss :  0.07943467795848846\n",
      "step :  347 loss :  0.07942470163106918\n",
      "step :  348 loss :  0.07941984385251999\n",
      "step :  349 loss :  0.07942387461662292\n",
      "step :  350 loss :  0.0794268250465393\n",
      "step :  351 loss :  0.0794261023402214\n",
      "step :  352 loss :  0.0794239267706871\n",
      "step :  353 loss :  0.07942598313093185\n",
      "step :  354 loss :  0.07943101972341537\n",
      "step :  355 loss :  0.07942494004964828\n",
      "step :  356 loss :  0.07942365109920502\n",
      "step :  357 loss :  0.07942590862512589\n",
      "step :  358 loss :  0.07942643761634827\n",
      "step :  359 loss :  0.07943043857812881\n",
      "step :  360 loss :  0.07942629605531693\n",
      "step :  361 loss :  0.07942231744527817\n",
      "step :  362 loss :  0.07942712306976318\n",
      "step :  363 loss :  0.0794307142496109\n",
      "step :  364 loss :  0.07942324131727219\n",
      "step :  365 loss :  0.0794239267706871\n",
      "step :  366 loss :  0.07942358404397964\n",
      "step :  367 loss :  0.07942201942205429\n",
      "step :  368 loss :  0.07942365109920502\n",
      "step :  369 loss :  0.07942313700914383\n",
      "step :  370 loss :  0.07942406088113785\n",
      "step :  371 loss :  0.07942266017198563\n",
      "step :  372 loss :  0.07942233979701996\n",
      "step :  373 loss :  0.07942519336938858\n",
      "step :  374 loss :  0.07942288368940353\n",
      "step :  375 loss :  0.07942453771829605\n",
      "step :  376 loss :  0.0794241726398468\n",
      "step :  377 loss :  0.07942239195108414\n",
      "step :  378 loss :  0.07942287623882294\n",
      "step :  379 loss :  0.07942277193069458\n",
      "step :  380 loss :  0.07942305505275726\n",
      "step :  381 loss :  0.0794232040643692\n",
      "step :  382 loss :  0.07942137867212296\n",
      "step :  383 loss :  0.07942226529121399\n",
      "step :  384 loss :  0.07942488044500351\n",
      "step :  385 loss :  0.07942136377096176\n",
      "step :  386 loss :  0.07942281663417816\n",
      "step :  387 loss :  0.07942506670951843\n",
      "step :  388 loss :  0.07942161709070206\n",
      "step :  389 loss :  0.07942142337560654\n",
      "step :  390 loss :  0.07942180335521698\n",
      "step :  391 loss :  0.0794229730963707\n",
      "step :  392 loss :  0.07942173629999161\n",
      "step :  393 loss :  0.07942090183496475\n",
      "step :  394 loss :  0.0794210433959961\n",
      "step :  395 loss :  0.07942365109920502\n",
      "step :  396 loss :  0.07942216843366623\n",
      "step :  397 loss :  0.0794198289513588\n",
      "step :  398 loss :  0.07942071557044983\n",
      "step :  399 loss :  0.07942245155572891\n",
      "step :  400 loss :  0.07942012697458267\n",
      "step :  401 loss :  0.07942115515470505\n",
      "step :  402 loss :  0.07942166179418564\n",
      "step :  403 loss :  0.07941891252994537\n",
      "step :  404 loss :  0.07942009717226028\n",
      "step :  405 loss :  0.07942403852939606\n",
      "step :  406 loss :  0.07942180335521698\n",
      "step :  407 loss :  0.07941965013742447\n",
      "step :  408 loss :  0.0794198215007782\n",
      "step :  409 loss :  0.0794232115149498\n",
      "step :  410 loss :  0.07942074537277222\n",
      "step :  411 loss :  0.07941872626543045\n",
      "step :  412 loss :  0.07941915839910507\n",
      "step :  413 loss :  0.07941941916942596\n",
      "step :  414 loss :  0.07942041754722595\n",
      "step :  415 loss :  0.07942002266645432\n",
      "step :  416 loss :  0.07941916584968567\n",
      "step :  417 loss :  0.07941946387290955\n",
      "step :  418 loss :  0.07941986620426178\n",
      "step :  419 loss :  0.07941970974206924\n",
      "step :  420 loss :  0.07941925525665283\n",
      "step :  421 loss :  0.07941965758800507\n",
      "step :  422 loss :  0.07941999286413193\n",
      "step :  423 loss :  0.07941975444555283\n",
      "step :  424 loss :  0.07941903918981552\n",
      "step :  425 loss :  0.07941998541355133\n",
      "step :  426 loss :  0.07941965758800507\n",
      "step :  427 loss :  0.07941934466362\n",
      "step :  428 loss :  0.07941988110542297\n",
      "step :  429 loss :  0.07941852509975433\n",
      "step :  430 loss :  0.07941918075084686\n",
      "step :  431 loss :  0.07942073792219162\n",
      "step :  432 loss :  0.07941916584968567\n",
      "step :  433 loss :  0.07941865921020508\n",
      "step :  434 loss :  0.07941941916942596\n",
      "step :  435 loss :  0.07941924780607224\n",
      "step :  436 loss :  0.07941979914903641\n",
      "step :  437 loss :  0.07941895723342896\n",
      "step :  438 loss :  0.07941934466362\n",
      "step :  439 loss :  0.07941941916942596\n",
      "step :  440 loss :  0.07941903918981552\n",
      "step :  441 loss :  0.0794195681810379\n",
      "step :  442 loss :  0.07941911369562149\n",
      "step :  443 loss :  0.0794195681810379\n",
      "step :  444 loss :  0.07941883057355881\n",
      "step :  445 loss :  0.07941911369562149\n",
      "step :  446 loss :  0.07941865921020508\n",
      "step :  447 loss :  0.0794188529253006\n",
      "step :  448 loss :  0.0794195905327797\n",
      "step :  449 loss :  0.0794188529253006\n",
      "step :  450 loss :  0.07941903173923492\n",
      "step :  451 loss :  0.07941876351833344\n",
      "step :  452 loss :  0.07941845059394836\n",
      "step :  453 loss :  0.07941852509975433\n",
      "step :  454 loss :  0.07941960543394089\n",
      "step :  455 loss :  0.07941880822181702\n",
      "step :  456 loss :  0.07941828668117523\n",
      "step :  457 loss :  0.07941876351833344\n",
      "step :  458 loss :  0.07941906154155731\n",
      "step :  459 loss :  0.07941855490207672\n",
      "step :  460 loss :  0.07941850274801254\n",
      "step :  461 loss :  0.07941854000091553\n",
      "step :  462 loss :  0.0794193223118782\n",
      "step :  463 loss :  0.07941880822181702\n",
      "step :  464 loss :  0.07941782474517822\n",
      "step :  465 loss :  0.07941824197769165\n",
      "step :  466 loss :  0.07941931486129761\n",
      "step :  467 loss :  0.07941883057355881\n",
      "step :  468 loss :  0.0794183686375618\n",
      "step :  469 loss :  0.07941831648349762\n",
      "step :  470 loss :  0.0794186219573021\n",
      "step :  471 loss :  0.0794183611869812\n",
      "step :  472 loss :  0.07941810041666031\n",
      "step :  473 loss :  0.07941833138465881\n",
      "step :  474 loss :  0.07941827923059464\n",
      "step :  475 loss :  0.07941856235265732\n",
      "step :  476 loss :  0.07941828668117523\n",
      "step :  477 loss :  0.07941833883523941\n",
      "step :  478 loss :  0.07941889017820358\n",
      "step :  479 loss :  0.07941817492246628\n",
      "step :  480 loss :  0.07941820472478867\n",
      "step :  481 loss :  0.07941831648349762\n",
      "step :  482 loss :  0.07941818982362747\n",
      "step :  483 loss :  0.0794183611869812\n",
      "step :  484 loss :  0.07941792905330658\n",
      "step :  485 loss :  0.07941808551549911\n",
      "step :  486 loss :  0.07941846549510956\n",
      "step :  487 loss :  0.07941801100969315\n",
      "step :  488 loss :  0.07941817492246628\n",
      "step :  489 loss :  0.07941818237304688\n",
      "step :  490 loss :  0.07941803336143494\n",
      "step :  491 loss :  0.07941817492246628\n",
      "step :  492 loss :  0.07941804081201553\n",
      "step :  493 loss :  0.07941804826259613\n",
      "step :  494 loss :  0.07941803336143494\n",
      "step :  495 loss :  0.07941791415214539\n",
      "step :  496 loss :  0.07941802591085434\n",
      "step :  497 loss :  0.07941820472478867\n",
      "step :  498 loss :  0.0794181153178215\n",
      "step :  499 loss :  0.07941780984401703\n"
     ]
    }
   ],
   "source": [
    "INTERVALS = get_intervals()\n",
    "\n",
    "for year in [2010, 2019, 2022]:\n",
    "    \n",
    "    # Load year-specific parameters\n",
    "    dataset_end_date = INTERVALS[year][\"dataset_end_date\"]\n",
    "    train_end_date = INTERVALS[year][\"train_end_date\"]\n",
    "    test_start_date = INTERVALS[year][\"test_start_date\"]\n",
    "    structural_breakpoints = INTERVALS[year][\"break_points\"]\n",
    "    \n",
    "    original_data = load_data(dataset_end_date)\n",
    "    \n",
    "    # Add dummy variables\n",
    "    dummy_variables = []\n",
    "    for i in range(1, len(structural_breakpoints)+1):\n",
    "        col_name = f\"Break_{i}\"\n",
    "        original_data[col_name] = 0\n",
    "        original_data.loc[structural_breakpoints[i-1]:,col_name] = 1\n",
    "        dummy_variables.append(col_name)\n",
    "    \n",
    "    # Keep Date and GDP(t+1) variables\n",
    "    \n",
    "    # Set no. of lags\n",
    "    lags = 2\n",
    "    \n",
    "    # Lag explanatory variables\n",
    "    data = lag_data(original_data, lags, dummy_variables)\n",
    "    \n",
    "    # Partition data into training and test\n",
    "    train_set = data[\n",
    "            (data['Date'] <= pd.to_datetime(train_end_date))\n",
    "            ]\n",
    "    test_set = data[\n",
    "            (data['Date'] >= pd.to_datetime(test_start_date))\n",
    "            ]\n",
    "    \n",
    "    # Load hyperparameters\n",
    "    best_hyperparameters, _ = load_selection_results()\n",
    "    \n",
    "    # Initiate LSTM object with hyperparameters\n",
    "    model = LSTM(\n",
    "        train_set,\n",
    "        'GDP',\n",
    "        best_hyperparameters[\"n_timesteps\"],\n",
    "        n_models=best_hyperparameters[\"n_models\"],\n",
    "        train_episodes=best_hyperparameters[\"train_episodes\"],\n",
    "        batch_size=best_hyperparameters[\"batch_size\"],\n",
    "        decay=best_hyperparameters[\"decay\"],\n",
    "        n_hidden=best_hyperparameters[\"n_hidden\"],\n",
    "        n_layers=best_hyperparameters[\"n_layers\"],\n",
    "        dropout=best_hyperparameters[\"dropout\"],\n",
    "    )\n",
    "    \n",
    "    # Train LSTM object (the network)\n",
    "    model.train()\n",
    "    \n",
    "    # This makes expanding window predictions,\n",
    "    # we pass full dataset but it only forecasts OOS\n",
    "    # (it \"remembers\" what dates were training dates)\n",
    "    predictions = model.predict(data, only_actuals_obs=False).loc[\n",
    "        lambda x: x.date >= train_end_date\n",
    "    ]\n",
    "    \n",
    "    # Rename columns\n",
    "    predictions = predictions.rename(\n",
    "        columns={\n",
    "            'date': 'Date',\n",
    "            'actuals': 'GDP',\n",
    "            'predictions': 'LSTM Predictions'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Store all predictions\n",
    "    predictions.to_csv(f'../output/LSTM_{year}_{\"w_str_b\" if STR_BREAKS else \"wout_str_b\"}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8db13ca14d0d390cea81bc021e13bb595ce0aa70dd0e0a9e6eb726cccac2cd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
